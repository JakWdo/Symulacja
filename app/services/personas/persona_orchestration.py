"""Serwis orkiestracji generowania person uÅ¼ywajÄ…cy Gemini 2.5 Pro.

Ten moduÅ‚ zawiera `PersonaOrchestrationService`, ktÃ³ry wykorzystuje Gemini 2.5 Pro
do przeprowadzenia gÅ‚Ä™bokiej analizy Graph RAG i tworzenia szczegÃ³Å‚owych briefÃ³w
(900-1200 znakÃ³w) dla kaÅ¼dej grupy demograficznej person.

Filozofia:
- Orchestration Agent (Gemini 2.5 Pro) = complex reasoning, dÅ‚ugie analizy
- Individual Generators (Gemini 2.5 Flash) = szybkie generowanie konkretnych person
- Output style: Edukacyjny - wyjaÅ›nia "dlaczego", konwersacyjny ton
"""

from __future__ import annotations

import asyncio
import json
import logging
from typing import Any

from pydantic import BaseModel, Field

from app.core.config import get_settings
from app.services.shared import build_chat_model, get_polish_society_rag

settings = get_settings()
logger = logging.getLogger(__name__)


def _map_graph_node_to_insight(node: dict[str, Any]) -> "GraphInsight" | None:
    """Konwertuje graph node z polskimi property names na GraphInsight z angielskimi.

    Mapowanie:
    - streszczenie â†’ summary
    - skala â†’ magnitude
    - pewnosc â†’ confidence ("wysoka"â†’"high", "srednia"â†’"medium", "niska"â†’"low")
    - okres_czasu â†’ time_period
    - kluczowe_fakty â†’ why_matters (z dodatkowym kontekstem)

    Args:
        node: Dict z grafu Neo4j (polskie property names)

    Returns:
        GraphInsight object lub None jeÅ›li dane niepeÅ‚ne
    """
    if not node:
        return None

    # Graf uÅ¼ywa polskich property names
    node_type = node.get('type', 'Unknown')
    summary = node.get('streszczenie')

    if not summary:
        logger.warning(f"Graph node bez streszczenia: {node}")
        return None

    # Mapowanie pewnoÅ›ci PLâ†’EN
    pewnosc_pl = node.get('pewnosc', '').lower()
    confidence_map = {'wysoka': 'high', 'srednia': 'medium', 'niska': 'low'}
    confidence = confidence_map.get(pewnosc_pl, 'medium')

    # Dane wÄ™zÅ‚a (polskie property names)
    magnitude = node.get('skala')
    time_period = node.get('okres_czasu')
    source = node.get('source', node.get('document_title'))

    # why_matters - uÅ¼yj kluczowych faktÃ³w lub summary jako fallback
    kluczowe_fakty = node.get('kluczowe_fakty', '')
    why_matters = f"Ten wskaÅºnik pokazuje: {kluczowe_fakty}" if kluczowe_fakty else summary

    try:
        return GraphInsight(
            type=node_type,
            summary=summary,
            magnitude=magnitude,
            confidence=confidence,
            time_period=time_period,
            source=source,
            why_matters=why_matters
        )
    except Exception as e:
        logger.error(f"Nie moÅ¼na utworzyÄ‡ GraphInsight z node: {node}, error: {e}")
        return None


class GraphInsight(BaseModel):
    """Pojedynczy insight z grafu wiedzy (Wskaznik, Obserwacja, Trend).

    UWAGA: Ten schema uÅ¼ywa ANGIELSKICH property names dla API consistency.
    Dane w grafie Neo4j uÅ¼ywajÄ… POLSKICH nazw (streszczenie, skala, pewnosc, etc.).

    Konwersja wykonywana przez funkcjÄ™ _map_graph_node_to_insight():
    - streszczenie â†’ summary
    - skala â†’ magnitude
    - pewnosc â†’ confidence ("wysoka"â†’"high", "srednia"â†’"medium", "niska"â†’"low")
    - okres_czasu â†’ time_period
    - kluczowe_fakty â†’ why_matters (z dodatkowym edukacyjnym kontekstem)
    """

    type: str = Field(description="Typ wÄ™zÅ‚a (Wskaznik, Obserwacja, Trend, etc.)")
    summary: str = Field(description="Jednozdaniowe podsumowanie")
    magnitude: str | None = Field(default=None, description="WartoÅ›Ä‡ liczbowa jeÅ›li istnieje (np. '78.4%')")
    confidence: str = Field(default="medium", description="Poziom pewnoÅ›ci: high, medium, low")
    time_period: str | None = Field(default=None, description="Okres czasu (np. '2022')")
    source: str | None = Field(default=None, description="Å¹rÃ³dÅ‚o danych (np. 'GUS', 'CBOS')")
    why_matters: str = Field(description="Edukacyjne wyjaÅ›nienie dlaczego to waÅ¼ne dla person")


class DemographicGroup(BaseModel):
    """Grupa demograficzna z briefem i insightami."""

    count: int = Field(description="Liczba person do wygenerowania w tej grupie")
    demographics: dict[str, Any] = Field(description="Cechy demograficzne (age, gender, education, etc.)")
    brief: str = Field(description="DÅ‚ugi (900-1200 znakÃ³w) edukacyjny brief dla generatorÃ³w")
    graph_insights: list[GraphInsight] = Field(default_factory=list, description="Insighty z Graph RAG")
    allocation_reasoning: str = Field(description="Dlaczego tyle person w tej grupie")
    segment_characteristics: list[str] = Field(default_factory=list, description="4-6 kluczowych cech tego segmentu (np. 'ProfesjonaliÅ›ci z wielkich miast')")


class PersonaAllocationPlan(BaseModel):
    """Plan alokacji person z szczegÃ³Å‚owymi briefami dla kaÅ¼dej grupy."""

    total_personas: int = Field(description="CaÅ‚kowita liczba person do wygenerowania")
    groups: list[DemographicGroup] = Field(description="Grupy demograficzne z briefami")
    overall_context: str = Field(description="OgÃ³lny kontekst spoÅ‚eczny Polski z Graph RAG")


class PersonaOrchestrationService:
    """Serwis orkiestracji uÅ¼ywajÄ…cy Gemini 2.5 Pro do tworzenia briefÃ³w.

    Ten serwis:
    1. Pobiera comprehensive Graph RAG context (Wskazniki, Grupy_Demograficzne, Trendy)
    2. Przeprowadza gÅ‚Ä™bokÄ… socjologicznÄ… analizÄ™ uÅ¼ywajÄ…c Gemini 2.5 Pro
    3. Tworzy szczegÃ³Å‚owe briefe (900-1200 znakÃ³w) dla kaÅ¼dej grupy person
    4. WyjaÅ›nia "dlaczego" (edukacyjny output style) dla wszystkich decyzji

    Output style: Konwersacyjny, edukacyjny, wyjaÅ›niajÄ…cy, production-ready.
    """

    def __init__(self) -> None:
        """Inicjalizuje orchestration agent (Gemini 2.5 Pro) i RAG service."""
        from config import models

        # Model config z centralnego registry
        model_config = models.get("personas", "orchestration")
        self.llm = build_chat_model(**model_config.params)

        # RAG service dla hybrid search kontekstu (singleton)
        self.rag_service = get_polish_society_rag()

        logger.info(
            "PersonaOrchestrationService zainicjalizowany (%s)",
            model_config.model
        )

    async def create_persona_allocation_plan(
        self,
        target_demographics: dict[str, Any],
        num_personas: int,
        project_description: str | None = None,
        additional_context: str | None = None,
    ) -> PersonaAllocationPlan:
        """Tworzy szczegÃ³Å‚owy plan alokacji person z dÅ‚ugimi briefami.

        Gemini 2.5 Pro przeprowadza gÅ‚Ä™bokÄ… analizÄ™:
        1. Pobiera Graph RAG context (hybrid search dla rozkÅ‚adÃ³w demograficznych)
        2. Analizuje trendy spoÅ‚eczne i wskaÅºniki statystyczne
        3. Tworzy spÃ³jne (900-1200 znakÃ³w) edukacyjne briefe
        4. WyjaÅ›nia "dlaczego" dla kaÅ¼dej decyzji alokacyjnej

        Args:
            target_demographics: RozkÅ‚ad demograficzny projektu (age_group, gender, etc.)
            num_personas: CaÅ‚kowita liczba person do wygenerowania
            project_description: Opis projektu badawczego
            additional_context: Dodatkowy kontekst od uÅ¼ytkownika (z AI Wizard)

        Returns:
            PersonaAllocationPlan z grupami demograficznymi i szczegÃ³Å‚owymi briefami

        Raises:
            Exception: JeÅ›li LLM nie moÅ¼e wygenerowaÄ‡ planu lub JSON parsing fails
        """
        import time

        # === TIMING & MONITORING ===
        start_time = time.time()
        logger.info(f"ğŸ¯ Orchestration START: Creating allocation plan for {num_personas} personas")

        try:
            # Krok 1: Pobierz comprehensive Graph RAG context
            graph_start = time.time()
            graph_context = await self._get_comprehensive_graph_context(target_demographics)
            graph_duration = time.time() - graph_start

            logger.info(
                f"ğŸ“Š Graph RAG completed: {len(graph_context)} chars in {graph_duration:.2f}s"
            )

            # Krok 2: Zbuduj prompt w stylu edukacyjnym
            prompt = self._build_orchestration_prompt(
                target_demographics=target_demographics,
                num_personas=num_personas,
                graph_context=graph_context,
                project_description=project_description,
                additional_context=additional_context,
            )

            # Krok 3: Gemini 2.5 Pro generuje plan (dÅ‚uga analiza)
            llm_start = time.time()
            logger.info("ğŸ¤– Invoking Gemini 2.5 Pro for orchestration (max_tokens=8000)...")
            response = await self.llm.ainvoke(prompt)
            llm_duration = time.time() - llm_start

            # DEBUG: Log surowej odpowiedzi od Gemini
            response_text = response.content if hasattr(response, 'content') else str(response)
            logger.info(
                f"ğŸ“ Gemini response: {len(response_text)} chars in {llm_duration:.2f}s"
            )
            logger.debug(f"ğŸ“ Response preview (first 500 chars): {response_text[:500]}")

            # Parse JSON response
            parsing_start = time.time()
            plan_json = self._extract_json_from_response(response_text)
            parsing_duration = time.time() - parsing_start

            logger.debug(f"âœ… JSON parsed in {parsing_duration:.2f}s: {len(plan_json)} top-level keys")

            # Parse do Pydantic model (walidacja)
            plan = PersonaAllocationPlan(**plan_json)

            # === TIMING METRICS (SUCCESS) ===
            total_duration = time.time() - start_time

            logger.info(
                f"âœ… Orchestration COMPLETED in {total_duration:.2f}s "
                f"(graph={graph_duration:.2f}s, llm={llm_duration:.2f}s, parsing={parsing_duration:.2f}s)"
            )

            # Structured metrics for monitoring
            logger.info(
                "METRICS_ORCHESTRATION",
                extra={
                    "metric_type": "orchestration_performance",
                    "total_duration_seconds": total_duration,
                    "graph_rag_duration_seconds": graph_duration,
                    "llm_duration_seconds": llm_duration,
                    "parsing_duration_seconds": parsing_duration,
                    "num_personas": num_personas,
                    "num_groups": len(plan.groups),
                    "graph_context_chars": len(graph_context),
                    "response_chars": len(response_text),
                    "success": True,
                }
            )

            return plan

        except Exception as e:
            # === TIMING METRICS (FAILURE) ===
            total_duration = time.time() - start_time

            logger.error(
                f"âŒ Orchestration FAILED after {total_duration:.2f}s: {e}",
                exc_info=True
            )

            # Structured metrics for monitoring
            logger.error(
                "METRICS_ORCHESTRATION",
                extra={
                    "metric_type": "orchestration_performance",
                    "total_duration_seconds": total_duration,
                    "num_personas": num_personas,
                    "success": False,
                    "error_type": type(e).__name__,
                    "error_message": str(e)[:500],
                }
            )
            raise

    async def _get_comprehensive_graph_context(
        self,
        target_demographics: dict[str, Any]
    ) -> str:
        """Pobiera comprehensive Graph RAG context dla rozkÅ‚adÃ³w demograficznych.

        Hybrid search (vector + keyword + RRF) dla kaÅ¼dej grupy demograficznej:
        - Age groups (18-24, 25-34, etc.)
        - Gender
        - Education levels
        - Locations

        Args:
            target_demographics: RozkÅ‚ad demograficzny (age_group, gender, etc.)

        Returns:
            Sformatowany string z Graph RAG context (Wskazniki, Obserwacje, Trendy)
        """
        # === OPTIMIZED QUERY GENERATION (8+ queries â†’ 4 consolidated) ===
        # Performance: 50% reduction in Graph RAG queries while maintaining context quality
        # Each query consolidates multiple demographic dimensions for efficiency
        queries = []

        # 1. CONSOLIDATED DEMOGRAPHICS QUERY (age + gender + education)
        # Pick top 2 values from each dimension by weight
        demo_parts = []

        if "age_group" in target_demographics:
            top_ages = sorted(
                target_demographics["age_group"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:2]
            demo_parts.extend([age for age, _ in top_ages])

        if "gender" in target_demographics:
            top_genders = sorted(
                target_demographics["gender"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:1]
            demo_parts.extend([gender for gender, _ in top_genders])

        if "education_level" in target_demographics:
            top_edu = sorted(
                target_demographics["education_level"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:1]
            demo_parts.extend([edu for edu, _ in top_edu])

        if demo_parts:
            queries.append(
                f"Polish demographics statistics {' '.join(demo_parts)} "
                f"workforce employment trends"
            )

        # 2. LOCATION-SPECIFIC QUERY (if specified)
        if "location" in target_demographics:
            top_locations = sorted(
                target_demographics["location"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:2]
            location_str = " ".join([loc for loc, _ in top_locations])
            queries.append(
                f"demographics income housing {location_str} Poland urban areas"
            )

        # 3. GENERAL TRENDS QUERY (always relevant)
        queries.append("Polish society trends 2024 demographics employment economic")

        # 4. WORK-LIFE BALANCE QUERY (always relevant for personas)
        queries.append("work-life balance Poland young professionals income housing")

        logger.info(
            f"ğŸ” Optimized queries: reduced from 8+ to {len(queries)} consolidated queries"
        )

        # === PARALLEL HYBRID SEARCHES WITH OPTIMIZED TIMEOUT ===
        # OPTIMIZATION: 4 consolidated queries (down from 8+) with Redis caching
        # Expected latency:
        #   - Cache HIT: 4 Ã— 50ms = 200ms total (300-400x speedup!)
        #   - Cache MISS: 4 Ã— 2-3s = 8-12s total (50% reduction from 8 queries)
        # Timeout reduced from 150s to 60s (caching makes 150s unnecessary)
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*[
                    self.rag_service.hybrid_search(query=q, top_k=3)
                    for q in queries  # All queries (max 4, optimized)
                ]),
                timeout=60.0  # 60 seconds (with caching, should complete in 5-15s)
            )
        except asyncio.TimeoutError:
            logger.error(
                "âš ï¸ Graph RAG queries timed out (60s) - zwracam pusty kontekst. "
                "To nie powinno siÄ™ zdarzyÄ‡ z cachingiem!"
            )
            return "Brak dostÄ™pnego kontekstu z Graph RAG (timeout)."

        # Deduplikuj i formatuj
        all_docs = []
        seen_texts = set()
        for docs_list in results:
            for doc in docs_list:
                if doc.page_content not in seen_texts:
                    all_docs.append(doc)
                    seen_texts.add(doc.page_content)

        # Formatuj jako czytelny context
        formatted_context = self._format_graph_context(all_docs[:15])  # Top 15 unique
        return formatted_context

    def _format_graph_context(self, documents: list[Any]) -> str:
        """Formatuje Graph RAG documents jako czytelny context dla LLM.

        Args:
            documents: Lista Document objects z Graph RAG

        Returns:
            Sformatowany string z numbered entries
        """
        if not documents:
            return "Brak dostÄ™pnego kontekstu z Graph RAG."

        formatted = "=== KONTEKST Z GRAPH RAG (Raporty o polskim spoÅ‚eczeÅ„stwie) ===\n\n"

        for idx, doc in enumerate(documents, 1):
            formatted += f"[{idx}] {doc.page_content}\n"

            # Dodaj metadata jeÅ›li istnieje
            if hasattr(doc, 'metadata') and doc.metadata:
                meta = doc.metadata
                if 'source' in meta:
                    formatted += f"    Å¹rÃ³dÅ‚o: {meta['source']}\n"
                if 'document_title' in meta:
                    formatted += f"    TytuÅ‚: {meta['document_title']}\n"

            formatted += "\n"

        return formatted

    def _build_orchestration_prompt(
        self,
        target_demographics: dict[str, Any],
        num_personas: int,
        graph_context: str,
        project_description: str | None,
        additional_context: str | None,
    ) -> str:
        """Buduje prompt w stylu edukacyjnym dla Gemini 2.5 Pro.

        Prompt instruuje LLM aby:
        1. PrzeanalizowaÄ‡ Graph RAG context (Wskazniki, Obserwacje)
        2. WyjaÅ›niÄ‡ "dlaczego" dla kaÅ¼dej decyzji (edukacyjny styl)
        3. UtworzyÄ‡ spÃ³jne (900-1200 znakÃ³w) briefe dla kaÅ¼dej grupy
        4. UÅ¼yÄ‡ konwersacyjnego tonu (jak kolega z zespoÅ‚u)

        Args:
            target_demographics: RozkÅ‚ad demograficzny
            num_personas: Liczba person
            graph_context: Kontekst z Graph RAG
            project_description: Opis projektu
            additional_context: Dodatkowy kontekst od uÅ¼ytkownika

        Returns:
            DÅ‚ugi prompt string (production-ready instrukcje)
        """
        prompt = f"""
JesteÅ› ekspertem od socjologii i badaÅ„ spoÅ‚ecznych w Polsce. Twoim zadaniem jest
przeanalizowanie danych demograficznych i Graph RAG context, a nastÄ™pnie stworzenie
szczegÃ³Å‚owego, EDUKACYJNEGO planu alokacji {num_personas} syntetycznych person.

=== STYL KOMUNIKACJI (KRYTYCZNY!) ===

WAÅ»NE: Twoim outputem bÄ™dzie uÅ¼ywany bezpoÅ›rednio przez innych agentÃ³w AI oraz
pokazywany uÅ¼ytkownikom w interfejsie. Dlatego MUSISZ:

âœ… **Konwersacyjny ton** - MÃ³wisz jak kolega z zespoÅ‚u, nie jak suchy raport
âœ… **WyjaÅ›niaj "dlaczego"** - Nie podawaj tylko faktÃ³w, ale ich znaczenie i kontekst
âœ… **UÅ¼ywaj przykÅ‚adÃ³w z Å¼ycia** - "WyobraÅº sobie AnnÄ™ z Warszawy, ktÃ³ra..."
âœ… **Production-ready** - TreÅ›Ä‡ moÅ¼e iÅ›Ä‡ bezpoÅ›rednio do uÅ¼ytkownika bez edycji
âœ… **Edukacyjny** - User ma siÄ™ UCZYÄ† o polskim spoÅ‚eczeÅ„stwie, nie tylko dostaÄ‡ dane
âœ… **PO POLSKU** - Naturalnie, bez anglicyzmÃ³w gdzie niepotrzebne

    DÅUGOÅšÄ† BRIEFÃ“W: KaÅ¼dy brief dla grupy demograficznej ma mieÄ‡ 900-1200 znakÃ³w.
    To ma byÄ‡ edukacyjny mini-esej, ktÃ³ry wyjaÅ›nia kontekst spoÅ‚eczny bez lania wody.

=== DANE WEJÅšCIOWE ===

**Projekt badawczy:**
{project_description or "Badanie person syntetycznych"}

**Dodatkowy kontekst od uÅ¼ytkownika:**
{additional_context or "Brak dodatkowego kontekstu"}

**RozkÅ‚ad demograficzny docelowy:**
```json
{json.dumps(target_demographics, indent=2, ensure_ascii=False)}
```

**Liczba person do wygenerowania:** {num_personas}

{graph_context}

=== TWOJE ZADANIE ===

PrzeprowadÅº gÅ‚Ä™bokÄ… socjologicznÄ… analizÄ™ i stwÃ³rz plan alokacji person ktÃ³ry zawiera:

### 1. OGÃ“LNY KONTEKST SPOÅECZNY (500-800 znakÃ³w)

ZrÃ³b overview polskiego spoÅ‚eczeÅ„stwa bazujÄ…c na Graph RAG context:
- Jakie sÄ… kluczowe trendy demograficzne w Polsce?
- Co pokazujÄ… wskaÅºniki ekonomiczne (zatrudnienie, dochody, housing)?
- Jakie wartoÅ›ci i wyzwania ma polskie spoÅ‚eczeÅ„stwo 2025?
- Dlaczego to ma znaczenie dla generowania person?
- Dla kazdej osoby twÃ³rz opis dlaczego akurat do niej siÄ™ to tyczy.

### 2. GRUPY DEMOGRAFICZNE Z DÅUGIMI BRIEFAMI

Dla kaÅ¼dej znaczÄ…cej grupy demograficznej (na podstawie rozkÅ‚adu docelowego), stwÃ³rz:

**KaÅ¼dy brief MUSI zawieraÄ‡ (900-1200 znakÃ³w):**

a) **Dlaczego ta grupa?** (180-220 znakÃ³w)
   - Jaki % populacji stanowi ta grupa (z Graph RAG)
   - Dlaczego sÄ… waÅ¼ni dla badania
   - Jak rozkÅ‚ad pasuje do realiÃ³w polskiego spoÅ‚eczeÅ„stwa
   - Statystyki z Graph RAG (magnitude, confidence)

b) **Kontekst zawodowy i Å¼yciowy** (260-320 znakÃ³w)
   - Typowe zawody dla tej grupy
   - Zarobki (realne liczby w PLN z Graph RAG jeÅ›li dostÄ™pne)
   - Housing situation (wÅ‚asne/wynajem, ceny mieszkaÅ„)
   - Wyzwania ekonomiczne (kredyty, oszczÄ™dnoÅ›ci, koszty Å¼ycia)
   - Dlaczego tak jest? (spoÅ‚eczno-ekonomiczny kontekst)

c) **WartoÅ›ci i aspiracje** (260-320 znakÃ³w)
   - Jakie wartoÅ›ci sÄ… waÅ¼ne dla tej grupy (z badaÅ„ spoÅ‚ecznych)
   - Aspiracje i life goals
   - Dlaczego te wartoÅ›ci? (kontekst pokoleniowy, historyczny)
   - Jak zmienia siÄ™ to w czasie (trendy)

d) **Typowe wyzwania i zainteresowania** (180-240 znakÃ³w)
   - Realne problemy Å¼yciowe tej grupy
   - Typowe hobby i sposÃ³b spÄ™dzania wolnego czasu
   - Dlaczego te zainteresowania pasujÄ… do profilu

e) **Segment Characteristics** (4-6 kluczowych cech tego segmentu)
   - KrÃ³tkie, mÃ³wiÄ…ce cechy charakterystyczne dla tej grupy
   - Format: Lista stringÃ³w (np. ["ProfesjonaliÅ›ci z wielkich miast", "Wysoko wyksztaÅ‚ceni", "Stabilna kariera"])
   - Cechy powinny byÄ‡ KONKRETNE dla tej grupy (nie ogÃ³lne!)
   - Bazowane na demographics + insights z grafu

f) **Graph Insights** (structured data)
   - Lista 3-5 kluczowych wskaÅºnikÃ³w z Graph RAG
   - KaÅ¼dy z wyjaÅ›nieniem "why_matters"

g) **Allocation Reasoning**
   - Dlaczego tyle person w tej grupie (X z {num_personas})?
   - Jak to odnosi siÄ™ do % populacji vs. relevance dla badania?

### 3. PRZYKÅAD DOBREGO BRIEFU

```
# Grupa: Kobiety 25-34, wyÅ¼sze wyksztaÅ‚cenie, Warszawa (6 person)

## Dlaczego ta grupa?

W polskim spoÅ‚eczeÅ„stwie kobiety 25-34 z wyÅ¼szym wyksztaÅ‚ceniem stanowiÄ…
okoÅ‚o 17.3% populacji miejskiej wedÅ‚ug danych GUS z 2022 roku. To fascynujÄ…ca
grupa spoÅ‚eczna ktÃ³ra znajduje siÄ™ w momencie Å¼ycia peÅ‚nym zmian - balansujÄ…
miÄ™dzy budowaniem kariery a decyzjami o rodzinie, miÄ™dzy niezaleÅ¼noÅ›ciÄ… finansowÄ…
a realiami rynku nieruchomoÅ›ci.

Dla tego badania ta grupa jest kluczowa bo to oni sÄ… early adopters nowych
produktÃ³w i usÅ‚ug. WskaÅºniki pokazujÄ… Å¼e 78.4% tej grupy jest zatrudnionych
(najwyÅ¼sza stopa w Polsce!), co oznacza Å¼e majÄ… purchasing power. JednoczeÅ›nie
63% wykazuje wysokÄ… mobilnoÅ›Ä‡ zawodowÄ… - czÄ™sto zmieniajÄ… pracÄ™, co czyni ich
otwartymi na nowe rozwiÄ…zania.

## Kontekst zawodowy i Å¼yciowy

Warszawa koncentruje 35% polskiego rynku tech, fintech i consulting. Dla mÅ‚odych
kobiet z wyÅ¼szym wyksztaÅ‚ceniem to oznacza szeroki wybÃ³r moÅ¼liwoÅ›ci kariery - od
project managerÃ³w przez UX designerÃ³w po analitykÃ³w danych. Typowe zarobki w tej
grupie to 7000-12000 zÅ‚ netto, co brzmi nieÅºle, ale...

...ale tu zaczyna siÄ™ problem. Cena m2 w Warszawie to ~15000 zÅ‚. Dla osoby
zarabiajÄ…cej 9000 zÅ‚ netto (mediana), zakup 50m2 mieszkania wymaga odÅ‚oÅ¼enia
~750000 zÅ‚, co przy oszczÄ™dzaniu 2000 zÅ‚ miesiÄ™cznie daje... 31 lat. Nie dziwi
wiÄ™c Å¼e 45% tej grupy wynajmuje mieszkania. To nie wybÃ³r stylu Å¼ycia - to
koniecznoÅ›Ä‡ ekonomiczna.

[... dalszy tekst 1000+ znakÃ³w ...]
```

=== OUTPUT FORMAT ===

Generuj JSON zgodny z tym schematem:

```json
{{
  "total_personas": {num_personas},
  "overall_context": "DÅUGI (500-800 znakÃ³w) overview polskiego spoÅ‚eczeÅ„stwa...",
  "groups": [
    {{
      "count": 6,
      "demographics": {{
        "age": "25-34",
        "gender": "kobieta",
        "education": "wyÅ¼sze",
        "location": "Warszawa"
      }},
      "brief": "Edukacyjny brief (900-1200 znakÃ³w) jak w przykÅ‚adzie...",
      "segment_characteristics": [
        "ProfesjonaliÅ›ci z wielkich miast",
        "Wysoko wyksztaÅ‚ceni",
        "Stabilna kariera",
        "Wysokie zaangaÅ¼owanie spoÅ‚eczne"
      ],
      "graph_insights": [
        {{
          "type": "Wskaznik",
          "summary": "Stopa zatrudnienia kobiet 25-34 z wyÅ¼szym",
          "magnitude": "78.4%",
          "confidence": "high",
          "time_period": "2022",
          "source": "GUS",
          "why_matters": "Wysoka stopa zatrudnienia oznacza Å¼e ta grupa ma..."
        }}
      ],
      "allocation_reasoning": "Dlaczego 6 z {num_personas}? Bo ta grupa stanowi..."
    }}
  ]
}}
```

KLUCZOWE ZASADY:

1. **Briefe majÄ… byÄ‡ KONKRETNE** (900-1200 znakÃ³w kaÅ¼dy) - mini-eseje, nie listy
2. **WyjaÅ›niaj "dlaczego"** dla WSZYSTKIEGO - user ma siÄ™ uczyÄ‡
3. **Konwersacyjny ton** - jak kolega tÅ‚umaczy przy kawie, nie jak raport naukowy
4. **UÅ¼ywaj danych z Graph RAG** - magnitude, confidence, time_period, sources
5. **Production-ready** - ten output idzie bezpoÅ›rednio do uÅ¼ytkownikÃ³w
6. **Realne liczby** - PLN, %, lata, konkretne wskaÅºniki (nie "wysoki" ale "78.4%")
7. **Kontekst spoÅ‚eczny** - wyjaÅ›niaj TÅO (historia, ekonomia, trendy)

Generuj plan alokacji:
"""
        return prompt

    def _extract_json_from_response(self, response_text: str) -> dict[str, Any]:
        """Ekstraktuje JSON z odpowiedzi LLM (moÅ¼e byÄ‡ otoczony markdown lub preambuÅ‚Ä…).

        Args:
            response_text: Surowa odpowiedÅº od LLM

        Returns:
            Parsed JSON jako dict

        Raises:
            ValueError: JeÅ›li nie moÅ¼na sparsowaÄ‡ JSON
        """
        text = response_text.strip()

        # Strategia 1: ZnajdÅº blok ```json ... ``` (moÅ¼e byÄ‡ w Å›rodku tekstu)
        import re
        json_block_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_block_match:
            json_text = json_block_match.group(1).strip()
            try:
                return json.loads(json_text)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Nie moÅ¼na sparsowaÄ‡ JSON z bloku markdown: {e}")
                logger.error(f"JSON block text: {json_text[:500]}...")
                # Kontynuuj do nastÄ™pnej strategii

        # Strategia 2: ZnajdÅº blok ``` ... ``` (bez json)
        code_block_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if code_block_match:
            json_text = code_block_match.group(1).strip()
            try:
                return json.loads(json_text)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Nie moÅ¼na sparsowaÄ‡ JSON z bloku kodu: {e}")
                # Kontynuuj do nastÄ™pnej strategii

        # Strategia 3: ZnajdÅº pierwszy { ... } (moÅ¼e byÄ‡ po preambule)
        brace_match = re.search(r'\{.*\}', text, re.DOTALL)
        if brace_match:
            json_text = brace_match.group(0).strip()
            try:
                return json.loads(json_text)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Nie moÅ¼na sparsowaÄ‡ JSON z braces: {e}")
                logger.error(f"Braces text: {json_text[:500]}...")

        # Strategia 4: SprÃ³buj sparsowaÄ‡ caÅ‚y tekst (fallback)
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Nie moÅ¼na sparsowaÄ‡ JSON (all strategies failed): {e}")
            logger.error(f"âŒ Response text length: {len(text)} chars")
            logger.error(f"âŒ Response text (first 1000 chars): {text[:1000]}")
            logger.error(f"âŒ Response text (last 1000 chars): {text[-1000:]}")
            raise ValueError(f"LLM nie zwrÃ³ciÅ‚ poprawnego JSON: {e}")

    # === NEW METHODS FOR SEGMENT-BASED ARCHITECTURE ===

    async def _generate_segment_name(
        self,
        demographics: dict[str, Any],
        graph_insights: list[GraphInsight],
        rag_citations: list[Any]
    ) -> str:
        """Generuje mÃ³wiÄ…cÄ… nazwÄ™ segmentu uÅ¼ywajÄ…c Gemini 2.5 Flash.

        Nazwa powinna byÄ‡ krÃ³tka (2-4 sÅ‚owa), mÃ³wiÄ…ca i odzwierciedlaÄ‡
        kluczowe cechy grupy demograficznej bazujÄ…c na insightach.

        Args:
            demographics: Cechy demograficzne (age, gender, education, income)
            graph_insights: Graph insights dla tej grupy
            rag_citations: RAG citations dla kontekstu

        Returns:
            Nazwa segmentu (np. "MÅ‚odzi Prekariusze", "AspirujÄ…ce Profesjonalistki 35-44")

        Raises:
            ValueError: JeÅ›li LLM nie zwrÃ³ci poprawnej nazwy
        """
        # Extract key demographics
        age_range = demographics.get('age', demographics.get('age_group', 'nieznany'))
        gender = demographics.get('gender', 'nieznana')
        education = demographics.get('education', demographics.get('education_level', 'nieznane'))
        income = demographics.get('income', demographics.get('income_bracket', 'nieznany'))

        # Format insights
        insights_text = "\n".join([
            f"- {ins.summary} ({ins.confidence})"
            for ins in graph_insights[:3]  # Top 3
        ]) if graph_insights else "Brak insights"

        # Format citations (first 2 max)
        citations_text = "\n".join([
            f"- {cit.page_content[:100]}..."
            for cit in rag_citations[:2]
        ]) if hasattr(rag_citations[0] if rag_citations else None, 'page_content') else "Brak cytatÃ³w"

        prompt = f"""StwÃ³rz trafnÄ…, MÃ“WIÄ„CÄ„ nazwÄ™ dla poniÅ¼szego segmentu demograficznego.

DANE SEGMENTU:
- Wiek: {age_range}
- PÅ‚eÄ‡: {gender}
- WyksztaÅ‚cenie: {education}
- DochÃ³d: {income}

INSIGHTS Z GRAFU:
{insights_text}

CYTATY Z RAG:
{citations_text}

ZASADY:
1. Nazwa powinna byÄ‡ 2-4 sÅ‚owa (np. "MÅ‚odzi Prekariusze", "AspirujÄ…ce Profesjonalistki 35-44")
2. Oddaje kluczowÄ… charakterystykÄ™ grupy (wiek + status spoÅ‚eczno-ekonomiczny)
3. UÅ¼ywa polskiego jÄ™zyka, brzmi naturalnie
4. Bazuje na insightach (np. jeÅ›li grupa ma niskie dochody + mÅ‚ody wiek â†’ "MÅ‚odzi Prekariusze")
5. Unikaj ogÃ³lnikÃ³w ("Grupa A", "Segment 1")
6. JeÅ›li wiek jest istotny, wÅ‚Ä…cz go (np. "35-44")

PRZYKÅADY DOBRYCH NAZW:
- "MÅ‚odzi Prekariusze" (18-24, niskie dochody)
- "AspirujÄ…ce Profesjonalistki 35-44" (kobiety, wyÅ¼sze wyksztaÅ‚cenie, Å›rednie dochody)
- "Dojrzali Eksperci" (45-54, wysokie dochody, stabilna kariera)
- "PoczÄ…tkujÄ…cy ProfesjonaliÅ›ci" (25-34, pierwsze kroki w karierze)

ZWRÃ“Ä† TYLKO NAZWÄ˜ (bez cudzysÅ‚owÃ³w, bez dodatkowych wyjaÅ›nieÅ„):"""

        try:
            # Use Gemini Flash for quick naming (cheap, fast)
            llm_flash = build_chat_model(
                model=settings.DEFAULT_MODEL,
                temperature=0.7,
                max_tokens=50,
                timeout=10,
            )

            response = await llm_flash.ainvoke(prompt)
            segment_name = response.content.strip() if hasattr(response, 'content') else str(response).strip()

            # Clean up (remove quotes if present)
            segment_name = segment_name.strip('"\'')

            # Validation: nazwa powinna mieÄ‡ 5-60 znakÃ³w
            if len(segment_name) < 5 or len(segment_name) > 60:
                logger.warning(f"Generated segment name too short/long: '{segment_name}', using fallback")
                # Fallback: template name
                segment_name = f"Segment {age_range}, {gender}"

            logger.info(f"âœ… Generated segment name: '{segment_name}'")
            return segment_name

        except Exception as e:
            logger.error(f"âŒ Failed to generate segment name: {e}")
            # Fallback: template name
            fallback_name = f"Segment {age_range}, {gender}"
            logger.warning(f"Using fallback segment name: '{fallback_name}'")
            return fallback_name

    async def _generate_segment_context(
        self,
        segment_name: str,
        demographics: dict[str, Any],
        graph_insights: list[GraphInsight],
        rag_citations: list[Any],
        project_goal: str | None = None
    ) -> str:
        """Generuje kontekst spoÅ‚eczny dla segmentu uÅ¼ywajÄ…c Gemini 2.5 Pro.

        Kontekst powinien byÄ‡ 500-800 znakÃ³w, edukacyjny i specyficzny dla TEJ grupy.

        Args:
            segment_name: Nazwa segmentu
            demographics: Cechy demograficzne
            graph_insights: Graph insights dla tej grupy
            rag_citations: RAG citations dla kontekstu
            project_goal: Cel projektu (opcjonalny)

        Returns:
            Kontekst spoÅ‚eczny (500-800 znakÃ³w)

        Raises:
            ValueError: JeÅ›li LLM nie zwrÃ³ci poprawnego kontekstu
        """
        # Extract key demographics
        age_range = demographics.get('age', demographics.get('age_group', 'nieznany'))
        gender = demographics.get('gender', 'nieznana')
        education = demographics.get('education', demographics.get('education_level', 'nieznane'))
        income = demographics.get('income', demographics.get('income_bracket', 'nieznany'))

        # Format insights with details
        insights_text = "\n".join([
            f"- **{ins.summary}**\n  Magnitude: {ins.magnitude or 'N/A'}, Confidence: {ins.confidence}, "
            f"Source: {ins.source or 'N/A'}, Year: {ins.time_period or 'N/A'}\n  "
            f"Why it matters: {ins.why_matters[:150]}..."
            for ins in graph_insights[:5]  # Top 5
        ]) if graph_insights else "Brak insights"

        # Format citations (first 3 max)
        citations_text = "\n".join([
            f"[{idx+1}] {cit.page_content[:200]}..."
            for idx, cit in enumerate(rag_citations[:3])
        ]) if hasattr(rag_citations[0] if rag_citations else None, 'page_content') else "Brak cytatÃ³w"

        prompt = f"""StwÃ³rz kontekst spoÅ‚eczny dla segmentu "{segment_name}".

DEMOGRAFIA SEGMENTU:
- Wiek: {age_range}
- PÅ‚eÄ‡: {gender}
- WyksztaÅ‚cenie: {education}
- DochÃ³d: {income}

INSIGHTS Z GRAFU WIEDZY:
{insights_text}

CYTATY Z RAG:
{citations_text}

CEL PROJEKTU:
{project_goal or "Badanie syntetycznych person"}

WYTYCZNE:
1. DÅ‚ugoÅ›Ä‡: 500-800 znakÃ³w (WAÅ»NE!)
2. Kontekst SPECYFICZNY dla KONKRETNEJ GRUPY (nie ogÃ³lny opis Polski!)
3. Zacznij od opisu charakterystyki grupy (jak w przykÅ‚adzie)
4. Struktura:
   a) Pierwsza czÄ™Å›Ä‡ (2-3 zdania): KIM sÄ… te osoby, co ich charakteryzuje
   b) Druga czÄ™Å›Ä‡ (2-3 zdania): Ich WARTOÅšCI i ASPIRACJE
   c) Trzecia czÄ™Å›Ä‡ (2-3 zdania): WYZWANIA i kontekst ekonomiczny z konkretnymi liczbami
5. Ton: konkretny, praktyczny, opisujÄ…cy TYCH ludzi (nie teoretyczny!)
6. UÅ¼ywaj konkretnych liczb z insights tam gdzie dostÄ™pne
7. Unikaj: ogÃ³lnikÃ³w ("polska spoÅ‚eczeÅ„stwo"), teoretyzowania

PRZYKÅAD DOBREGO KONTEKSTU (na wzÃ³r Figmy):
"Tech-Savvy ProfesjonaliÅ›ci to osoby w wieku 28 lat, pracujÄ…ce jako Marketing Manager w duÅ¼ych miastach jak Warszawa czy KrakÃ³w. CharakteryzujÄ… siÄ™ wysokim wyksztaÅ‚ceniem (licencjat lub wyÅ¼ej), stabilnÄ… karierÄ… w branÅ¼y technologicznej i dochodami 8k-12k PLN netto. SÄ… early adopters nowych technologii i ceniÄ… sobie work-life balance. Ich gÅ‚Ã³wne wartoÅ›ci to innovation, ciÄ…gÅ‚y rozwÃ³j i sustainability. AspirujÄ… do awansu na wyÅ¼sze stanowiska (senior manager, director), wÅ‚asnego mieszkania w atrakcyjnej lokalizacji (co przy cenach 15-20k PLN/m2 wymaga oszczÄ™dzania przez 10+ lat) i rozwoju kompetencji w digital marketing oraz AI tools. Wyzwania: rosnÄ…ca konkurencja na rynku pracy (wedÅ‚ug GUS 78% osÃ³b z tej grupy ma wyÅ¼sze wyksztaÅ‚cenie), wysokie koszty Å¼ycia w duÅ¼ych miastach (Å›redni czynsz ~3500 PLN), presja na ciÄ…gÅ‚y rozwÃ³j i keeping up with tech trends."

WAÅ»NE: Pisz o KONKRETNEJ grupie ludzi, uÅ¼ywaj przykÅ‚adÃ³w zawodÃ³w, konkretnych liczb, opisuj ICH Å¼ycie.

ZWRÃ“Ä† TYLKO KONTEKST (bez nagÅ‚Ã³wkÃ³w, bez komentarzy, 500-800 znakÃ³w):"""

        try:
            response = await self.llm.ainvoke(prompt)  # Use Gemini 2.5 Pro
            segment_context = response.content.strip() if hasattr(response, 'content') else str(response).strip()

            # Validation: kontekst powinien mieÄ‡ 400-1200 znakÃ³w
            if len(segment_context) < 400 or len(segment_context) > 1200:
                logger.warning(
                    f"Generated segment context length ({len(segment_context)}) outside range 400-1200, "
                    "but accepting anyway"
                )

            logger.info(f"âœ… Generated segment context: {len(segment_context)} chars")
            return segment_context

        except Exception as e:
            logger.error(f"âŒ Failed to generate segment context: {e}")
            # Fallback: minimal context
            fallback_context = (
                f"Segment '{segment_name}' obejmuje osoby w wieku {age_range}, {gender}, "
                f"z wyksztaÅ‚ceniem {education} i dochodami {income}. "
                f"Ta grupa stanowi istotnÄ… czÄ™Å›Ä‡ polskiego spoÅ‚eczeÅ„stwa i wymaga szczegÃ³lnej uwagi "
                f"w kontekÅ›cie badaÅ„ rynkowych."
            )
            logger.warning(f"Using fallback segment context: {len(fallback_context)} chars")
            return fallback_context

    def _filter_graph_insights_for_segment(
        self,
        insights: list[GraphInsight],
        demographics: dict[str, Any]
    ) -> list[GraphInsight]:
        """Filtruje graph insights dla konkretnego segmentu demograficznego.

        Zwraca tylko insights relevantne dla tego segmentu (np. insights o mÅ‚odych
        dla segmentu 18-24).

        Args:
            insights: Wszystkie graph insights
            demographics: Demografia segmentu

        Returns:
            Filtrowana lista insights (max 10)
        """
        if not insights:
            return []

        # Extract age range for filtering
        age_str = demographics.get('age', demographics.get('age_group', ''))

        filtered = []
        for insight in insights:
            # Filter by age relevance (check if age range mentioned in summary)
            age_relevant = True
            if age_str:
                # Simple heuristic: if insight mentions age, check if it's relevant
                summary_lower = insight.summary.lower()
                if any(age_term in summary_lower for age_term in ['wiek', 'lat', 'young', 'old', 'mÅ‚od', 'star']):
                    # Check if age range overlaps (simplified)
                    age_relevant = age_str.lower() in summary_lower or not any(
                        other_age in summary_lower
                        for other_age in ['18-24', '25-34', '35-44', '45-54', '55+']
                        if other_age != age_str
                    )

            if age_relevant:
                filtered.append(insight)

        # Sort by confidence (high first) and return top 10
        confidence_order = {'high': 3, 'medium': 2, 'low': 1}
        filtered.sort(key=lambda x: confidence_order.get(x.confidence, 0), reverse=True)

        return filtered[:10]

    def _filter_rag_citations(
        self,
        citations: list[Any],
        min_confidence: float = 0.7
    ) -> list[Any]:
        """Filtruje RAG citations - tylko high-quality (confidence > threshold).

        Args:
            citations: Lista RAG citations (Document objects)
            min_confidence: Minimalny confidence score (default 0.7)

        Returns:
            Filtrowana lista citations (max 10)
        """
        if not citations:
            return []

        filtered = []
        for cit in citations:
            # Check if citation has confidence score in metadata
            confidence = 1.0  # Default if not available
            if hasattr(cit, 'metadata') and cit.metadata:
                confidence = cit.metadata.get('confidence', cit.metadata.get('score', 1.0))

            # Filter by confidence
            if confidence >= min_confidence:
                filtered.append(cit)

        # Return top 10
        return filtered[:10]
