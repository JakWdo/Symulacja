"""
Generator Person oparty na LangChain i Google Gemini

Ten moduÅ‚ generuje realistyczne, statystycznie reprezentatywne persony
dla badaÅ„ rynkowych przy uÅ¼yciu Google Gemini przez framework LangChain.

Kluczowe funkcjonalnoÅ›ci:
- Generowanie person zgodnie z zadanymi rozkÅ‚adami demograficznymi
- Walidacja statystyczna przy uÅ¼yciu testu chi-kwadrat
- Sampling cech osobowoÅ›ci (Big Five) i wymiarÃ³w kulturowych (Hofstede)
- Integracja z LangChain dla Å‚atwej zmiany modelu LLM
"""

import re
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from scipy import stats
from dataclasses import dataclass

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough

from app.core.config import get_settings
from app.core.constants import (
    DEFAULT_AGE_GROUPS,
    DEFAULT_GENDERS,
    DEFAULT_EDUCATION_LEVELS,
    DEFAULT_INCOME_BRACKETS,
    DEFAULT_LOCATIONS,
    POLISH_LOCATIONS,
    POLISH_VALUES,
    POLISH_INTERESTS,
    POLISH_COMMUNICATION_STYLES,
    POLISH_DECISION_STYLES,
    POLISH_INCOME_BRACKETS,
    POLISH_EDUCATION_LEVELS,
    POLISH_MALE_NAMES,
    POLISH_FEMALE_NAMES,
    POLISH_SURNAMES,
)
from app.core.prompts.personas import (
    PERSONA_GENERATION_SYSTEM_PROMPT,
    PERSONA_GENERATION_CHAT_PROMPT,
)
from app.models import Persona
from app.services.clients import build_chat_model

settings = get_settings()


# ============================================================================
# PROMPT BUILDERS - Funkcje pomocnicze dla generacji promptÃ³w
# ============================================================================

def create_persona_prompt(
    demographic: Dict[str, Any],
    psychological: Dict[str, Any],
    persona_seed: int,
    suggested_first_name: str,
    suggested_surname: str,
    rag_context: Optional[str] = None,
    target_audience_description: Optional[str] = None,
    orchestration_brief: Optional[str] = None
) -> str:
    """
    Tworzy szczegÃ³Å‚owy prompt dla LLM do generowania persony - WERSJA POLSKA

    Args:
        demographic: Profil demograficzny (wiek, pÅ‚eÄ‡, edukacja, etc.)
        psychological: Profil psychologiczny (Big Five + Hofstede)
        persona_seed: Unikalny seed dla rÃ³Å¼nicowania (1000-9999)
        suggested_first_name: Sugerowane polskie imiÄ™
        suggested_surname: Sugerowane polskie nazwisko
        rag_context: Opcjonalny kontekst z RAG (fragmenty z dokumentÃ³w)
        target_audience_description: Opcjonalny dodatkowy opis grupy docelowej
        orchestration_brief: Opcjonalny DÅUGI brief od orchestration agent (Gemini 2.5 Pro)

    Returns:
        PeÅ‚ny tekst prompta gotowy do wysÅ‚ania do LLM (po polsku)
    """

    # Determine headline age rule based on available data
    if demographic.get('age'):
        headline_age_rule = f"â€¢ HEADLINE: Musi zawieraÄ‡ liczbÄ™ {demographic['age']} lat i realnÄ… motywacjÄ™ tej osoby.\n"
    elif demographic.get('age_group'):
        headline_age_rule = (
            f"â€¢ HEADLINE: Podaj konkretnÄ… liczbÄ™ lat zgodnÄ… z przedziaÅ‚em {demographic['age_group']} "
            "i pokaÅ¼ realnÄ… motywacjÄ™ tej osoby.\n"
        )
    else:
        headline_age_rule = "â€¢ HEADLINE: Podaj konkretny wiek w latach i realnÄ… motywacjÄ™ tej osoby.\n"

    # Get Big Five values
    openness_val = psychological.get('openness', 0.5)
    conscientiousness_val = psychological.get('conscientiousness', 0.5)
    extraversion_val = psychological.get('extraversion', 0.5)
    agreeableness_val = psychological.get('agreeableness', 0.5)
    neuroticism_val = psychological.get('neuroticism', 0.5)

    # Unified context section (merge RAG + Target Audience + Orchestration Brief)
    unified_context = ""
    if rag_context or target_audience_description or orchestration_brief:
        context_parts = []

        if rag_context:
            context_parts.append(f"ğŸ“Š KONTEKST RAG:\n{rag_context}")
        if orchestration_brief and orchestration_brief.strip():
            context_parts.append(f"ğŸ“‹ ORCHESTRATION BRIEF:\n{orchestration_brief.strip()}")
        if target_audience_description and target_audience_description.strip():
            context_parts.append(f"ğŸ¯ GRUPA DOCELOWA:\n{target_audience_description.strip()}")

        unified_context = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KONTEKST (RAG + Brief + Audience):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{chr(10).join(context_parts)}

âš ï¸ KLUCZOWE ZASADY:
â€¢ UÅ¼yj kontekstu jako TÅA Å¼ycia persony (nie cytuj statystyk!)
â€¢ StwÃ³rz FASCYNUJÄ„CÄ„ historiÄ™ - kontekst to fundament, nie lista faktÃ³w
â€¢ WskaÅºniki â†’ konkretne detale Å¼ycia (housing crisis â†’ wynajmuje, oszczÄ™dza)
â€¢ Trendy â†’ doÅ›wiadczenia Å¼yciowe (mobilnoÅ›Ä‡ â†’ zmiana 3 prac w 5 lat)
â€¢ NaturalnoÅ›Ä‡: "Jak wielu rÃ³wieÅ›nikÃ³w..." zamiast "67% absolwentÃ³w..."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""

    return f"""Expert: Syntetyczne persony dla polskiego rynku - UNIKALNE, REALISTYCZNE, SPÃ“JNE.

{unified_context}PERSONA #{persona_seed}: {suggested_first_name} {suggested_surname}

PROFIL:
â€¢ Wiek: {demographic.get('age_group')} | PÅ‚eÄ‡: {demographic.get('gender')} | Lokalizacja: {demographic.get('location')}
â€¢ WyksztaÅ‚cenie: {demographic.get('education_level')} | DochÃ³d: {demographic.get('income_bracket')}

OSOBOWOÅšÄ† (Big Five - wartoÅ›ci 0-1):
â€¢ OtwartoÅ›Ä‡ (Openness): {openness_val:.2f}
â€¢ SumiennoÅ›Ä‡ (Conscientiousness): {conscientiousness_val:.2f}
â€¢ Ekstrawersja (Extraversion): {extraversion_val:.2f}
â€¢ UgodowoÅ›Ä‡ (Agreeableness): {agreeableness_val:.2f}
â€¢ Neurotyzm (Neuroticism): {neuroticism_val:.2f}

Interpretacja Big Five: <0.4 = niskie, 0.4-0.6 = Å›rednie, >0.6 = wysokie.
Wykorzystaj te wartoÅ›ci do stworzenia spÃ³jnej osobowoÅ›ci i historii Å¼yciowej.

HOFSTEDE (wartoÅ›ci 0-1): PD={psychological.get('power_distance', 0.5):.2f} | IND={psychological.get('individualism', 0.5):.2f} | UA={psychological.get('uncertainty_avoidance', 0.5):.2f}

ZASADY:
â€¢ ZawÃ³d = wyksztaÅ‚cenie + dochÃ³d
â€¢ OsobowoÅ›Ä‡ â†’ historia (Oâ†’podrÃ³Å¼e, Sâ†’planowanie)
â€¢ Detale: dzielnice, marki, konkretne hobby
â€¢ UNIKALNOÅšÄ†: KaÅ¼da persona MUSI mieÄ‡ RÃ“Å»NÄ„ historiÄ™ Å¼yciowÄ… - nie kopiuj opisÃ³w!
â€¢ Background_story NIE moÅ¼e kopiowaÄ‡ briefu segmentu ani powtarzaÄ‡ caÅ‚ych akapitÃ³w z kontekstu
{headline_age_rule}â€¢ PokaÅ¼ codzienne wybory i motywacje tej osoby - zero ogÃ³lnikÃ³w

âš ï¸ CATCHY SEGMENT NAME (2-4 sÅ‚owa):
Wygeneruj krÃ³tkÄ…, chwytliwÄ… nazwÄ™ marketingowÄ… dla segmentu tej persony.
â€¢ Powinna odzwierciedlaÄ‡ wiek, wartoÅ›ci, styl Å¼ycia, status ekonomiczny
â€¢ PrzykÅ‚ady: "Pasywni LiberaÅ‚owie", "MÅ‚odzi Prekariusze", "Aktywni Seniorzy", "Cyfrowi Nomadzi", "Stabilni TradycjonaliÅ›ci"
â€¢ UNIKAJ dÅ‚ugich opisÃ³w technicznych jak "Kobiety 35-44 wyÅ¼sze wyksztaÅ‚cenie"
â€¢ Polski jÄ™zyk, kulturowo relevantne, konkretne

PRZYKÅAD:
{{"full_name": "Marek Kowalczyk", "catchy_segment_name": "Stabilni TradycjonaliÅ›ci", "persona_title": "GÅ‚Ã³wny KsiÄ™gowy", "headline": "PoznaÅ„ski ksiÄ™gowy (56) planujÄ…cy emeryturÄ™", "background_story": "28 lat w firmie, Å¼onaty, dwoje dorosÅ‚ych dzieci, kupiÅ‚ dziaÅ‚kÄ™ pod Poznaniem, skarbnik parafii", "values": ["StabilnoÅ›Ä‡", "LojalnoÅ›Ä‡", "Rodzina", "OdpowiedzialnoÅ›Ä‡"], "interests": ["WÄ™dkarstwo", "Majsterkowanie", "Grillowanie"], "communication_style": "formalny, face-to-face", "decision_making_style": "metodyczny, unika ryzyka", "typical_concerns": ["Emerytura", "Sukcesja", "Zdrowie"]}}

âš ï¸ KRYTYCZNE: Generuj KOMPLETNIE INNÄ„ personÄ™ z UNIKALNÄ„ historiÄ… Å¼yciowÄ…!
â€¢ NIE kopiuj ogÃ³lnych opisÃ³w segmentu do background_story
â€¢ Fokus na TEJ KONKRETNEJ OSOBY, jej specyficznych doÅ›wiadczeniach
â€¢ UÅ¼yj persona_seed #{persona_seed} jako ÅºrÃ³dÅ‚o rÃ³Å¼norodnoÅ›ci

WYÅÄ„CZNIE JSON (bez markdown):
{{
  "full_name": "<polskie imiÄ™+nazwisko>",
  "catchy_segment_name": "<2-4 sÅ‚owa, krÃ³tka marketingowa nazwa segmentu>",
  "persona_title": "<zawÃ³d/etap Å¼ycia>",
  "headline": "<1 zdanie: wiek, zawÃ³d, UNIKALNE motywacje>",
  "background_story": "<2-3 zdania: KONKRETNA historia TEJ OSOBY - jej Å¼ycie, kariera, sytuacja>",
  "values": ["<5-7 wartoÅ›ci>"],
  "interests": ["<5-7 hobby/aktywnoÅ›ci>"],
  "communication_style": "<jak siÄ™ komunikuje>",
  "decision_making_style": "<jak podejmuje decyzje>",
  "typical_concerns": ["<3-5 SPECYFICZNYCH zmartwieÅ„/priorytetÃ³w>"]
}}"""


def create_segment_persona_prompt(
    demographic: Dict[str, Any],
    psychological: Dict[str, Any],
    segment_name: str,
    segment_context: str,
    graph_insights: List[Any],
    rag_citations: List[Any],
    persona_seed: int,
    suggested_first_name: str,
    suggested_surname: str
) -> str:
    """
    Tworzy prompt dla generacji persony z WYMUSZENIEM demographics z segmentu.

    Args:
        demographic: Enforced demographic profile (age, gender, education, income, location)
        psychological: Psychological profile (Big Five + Hofstede)
        segment_name: Nazwa segmentu (np. "MÅ‚odzi Prekariusze")
        segment_context: Kontekst spoÅ‚eczny segmentu (500-800 znakÃ³w)
        graph_insights: Insights filtrowane dla segmentu
        rag_citations: High-quality RAG citations
        persona_seed: Unique persona seed for diversity
        suggested_first_name: Suggested Polish first name
        suggested_surname: Suggested Polish surname

    Returns:
        Full prompt text for segment-based persona generation
    """
    age = demographic.get('age', 30)

    # Format insights
    insights_text = ""
    if graph_insights:
        insights_text = "\n".join([
            f"- {ins.get('summary', ins.get('streszczenie', 'N/A'))}"
            for ins in graph_insights[:5]
        ])

    return f"""Wygeneruj realistycznÄ… personÄ™ dla segmentu "{segment_name}".

CONSTRAINTS (MUSISZ PRZESTRZEGAÄ†!):
â€¢ Wiek: {age} lat
â€¢ PÅ‚eÄ‡: {demographic.get('gender')}
â€¢ WyksztaÅ‚cenie: {demographic.get('education_level')}
â€¢ DochÃ³d: {demographic.get('income_bracket')}
â€¢ Lokalizacja: {demographic.get('location')}

KONTEKST SEGMENTU:
{segment_context}

INSIGHTS:
{insights_text or "Brak insights"}

OSOBOWOÅšÄ† (Big Five):
â€¢ OtwartoÅ›Ä‡: {psychological.get('openness', 0.5):.2f}
â€¢ SumiennoÅ›Ä‡: {psychological.get('conscientiousness', 0.5):.2f}
â€¢ Ekstrawersja: {psychological.get('extraversion', 0.5):.2f}

ZASADY:
â€¢ Persona MUSI pasowaÄ‡ do constraints
â€¢ ZawÃ³d = wyksztaÅ‚cenie + dochÃ³d
â€¢ UÅ¼ywaj kontekstu jako tÅ‚a (nie cytuj statystyk!)
â€¢ UNIKALNOÅšÄ†: KaÅ¼da persona w segmencie MUSI mieÄ‡ RÃ“Å»NÄ„ historiÄ™ Å¼yciowÄ…!
â€¢ HEADLINE: Musi zawieraÄ‡ liczbÄ™ {age} lat i realnÄ… motywacjÄ™ tej osoby
â€¢ Background_story NIE moÅ¼e kopiowaÄ‡ briefu segmentu ani powtarzaÄ‡ caÅ‚ych akapitÃ³w z kontekstu
â€¢ PokaÅ¼ codzienne wybory i motywacje tej osoby - zero ogÃ³lnikÃ³w

âš ï¸ CATCHY SEGMENT NAME (2-4 sÅ‚owa):
Wygeneruj krÃ³tkÄ…, chwytliwÄ… nazwÄ™ marketingowÄ… dla tego segmentu.
â€¢ Powinna odzwierciedlaÄ‡ wiek, wartoÅ›ci, styl Å¼ycia, status ekonomiczny
â€¢ PrzykÅ‚ady: "Pasywni LiberaÅ‚owie", "MÅ‚odzi Prekariusze", "Aktywni Seniorzy", "Cyfrowi Nomadzi"
â€¢ UNIKAJ dÅ‚ugich opisÃ³w technicznych jak "Kobiety 35-44 wyÅ¼sze wyksztaÅ‚cenie"
â€¢ Polski jÄ™zyk, kulturowo relevantne

âš ï¸ KRYTYCZNE: Generuj UNIKALNÄ„ personÄ™ (Persona #{persona_seed})!
â€¢ NIE kopiuj ogÃ³lnych opisÃ³w segmentu do background_story
â€¢ Fokus na TEJ KONKRETNEJ OSOBY, jej specyficznych doÅ›wiadczeniach
â€¢ KaÅ¼da persona w segmencie ma INNÄ„ historiÄ™ Å¼yciowÄ…, inne detale, rÃ³Å¼ne zainteresowania

ZWRÃ“Ä† JSON:
{{
  "full_name": "{suggested_first_name} {suggested_surname}",
  "catchy_segment_name": "<2-4 sÅ‚owa, krÃ³tka marketingowa nazwa segmentu>",
  "persona_title": "<zawÃ³d>",
  "headline": "<{age} lat, zawÃ³d, UNIKALNE motywacje>",
  "background_story": "<2-3 zdania: KONKRETNA historia TEJ OSOBY - nie ogÃ³lny opis segmentu!>",
  "values": ["<5-7 wartoÅ›ci>"],
  "interests": ["<5-7 hobby>"],
  "communication_style": "<styl>",
  "decision_making_style": "<styl>",
  "typical_concerns": ["<3-5 SPECYFICZNYCH zmartwieÅ„>"]
}}"""


# Import RAG service (opcjonalny - tylko jeÅ›li RAG wÅ‚Ä…czony)
try:
    if settings.RAG_ENABLED:
        from app.services.rag.hybrid_search import PolishSocietyRAG
        _rag_service_available = True
    else:
        _rag_service_available = False
except ImportError:
    _rag_service_available = False


@dataclass
class DemographicDistribution:
    """
    RozkÅ‚ad demograficzny populacji docelowej

    KaÅ¼de pole to sÅ‚ownik mapujÄ…cy kategorie na prawdopodobieÅ„stwa (sumujÄ…ce siÄ™ do 1.0)
    PrzykÅ‚ad: {"18-24": 0.3, "25-34": 0.5, "35-44": 0.2}
    """
    age_groups: Dict[str, float]        # Grupy wiekowe
    genders: Dict[str, float]           # PÅ‚eÄ‡
    education_levels: Dict[str, float]  # Poziomy edukacji
    income_brackets: Dict[str, float]   # PrzedziaÅ‚y dochodowe
    locations: Dict[str, float]         # Lokalizacje geograficzne


class PersonaGeneratorLangChain:
    """
    Generator statystycznie reprezentatywnych person przy uÅ¼yciu LangChain + Gemini

    UÅ¼ywa Google Gemini do generowania realistycznych profili person na podstawie
    zadanych rozkÅ‚adÃ³w demograficznych i psychologicznych.
    """

    def __init__(self):
        """Inicjalizuj generator z konfiguracjÄ… LangChain i Gemini"""
        import logging
        logger = logging.getLogger(__name__)

        self.settings = settings
        self._rng = np.random.default_rng(self.settings.RANDOM_SEED)

        # Inicjalizujemy model Gemini z wyÅ¼szÄ… temperaturÄ… dla wiÄ™kszej rÃ³Å¼norodnoÅ›ci
        persona_model = getattr(settings, "PERSONA_GENERATION_MODEL", settings.DEFAULT_MODEL)

        self.llm = build_chat_model(
            model=persona_model,
            temperature=0.9,  # Podniesiona wartoÅ›Ä‡ dla bardziej kreatywnych, zrÃ³Å¼nicowanych person
            max_tokens=settings.MAX_TOKENS,
            top_p=0.95,
            top_k=40,
        )

        # Konfigurujemy parser JSON, aby wymusiÄ‡ strukturalnÄ… odpowiedÅº
        self.json_parser = JsonOutputParser()

        # Budujemy szablon promptu do generowania person (z importu)
        self.persona_prompt = PERSONA_GENERATION_CHAT_PROMPT

        # SkÅ‚adamy Å‚aÅ„cuch LangChain (prompt -> LLM -> parser)
        self.persona_chain = (
            self.persona_prompt
            | self.llm
            | self.json_parser
        )

        # Inicjalizuj RAG service (opcjonalnie - tylko jeÅ›li wÅ‚Ä…czony)
        self.rag_service = None
        if _rag_service_available and settings.RAG_ENABLED:
            try:
                self.rag_service = PolishSocietyRAG()
                logger.info("RAG service initialized successfully in PersonaGenerator")
            except Exception as e:
                logger.warning(f"RAG service unavailable: {e}")

    def sample_demographic_profile(
        self, distribution: DemographicDistribution, n_samples: int = 1
    ) -> List[Dict[str, Any]]:
        """
        PrÃ³bkuj profile demograficzne zgodnie z zadanym rozkÅ‚adem

        Metoda ta tworzy losowe profile demograficzne na podstawie prawdopodobieÅ„stw
        w obiekcie DemographicDistribution. JeÅ›li jakiÅ› rozkÅ‚ad jest pusty lub niepoprawny,
        uÅ¼ywa domyÅ›lnych wartoÅ›ci z constants.py.

        Args:
            distribution: Obiekt zawierajÄ…cy rozkÅ‚ady prawdopodobieÅ„stw dla kaÅ¼dej kategorii
            n_samples: Liczba profili do wygenerowania (domyÅ›lnie 1)

        Returns:
            Lista sÅ‚ownikÃ³w, kaÅ¼dy zawiera klucze: age_group, gender, education_level,
            income_bracket, location
        """
        profiles = []

        for _ in range(n_samples):
            # Normalizuj kaÅ¼dy rozkÅ‚ad lub uÅ¼yj wartoÅ›ci domyÅ›lnych (polskich)
            age_groups = self._prepare_distribution(
                distribution.age_groups, DEFAULT_AGE_GROUPS
            )
            genders = self._prepare_distribution(distribution.genders, DEFAULT_GENDERS)
            education_levels = self._prepare_distribution(
                distribution.education_levels, POLISH_EDUCATION_LEVELS
            )
            income_brackets = self._prepare_distribution(
                distribution.income_brackets, POLISH_INCOME_BRACKETS
            )
            locations = self._prepare_distribution(
                distribution.locations, POLISH_LOCATIONS
            )

            # Losuj wartoÅ›Ä‡ z kaÅ¼dej kategorii zgodnie z wagami
            profile = {
                "age_group": self._weighted_sample(age_groups),
                "gender": self._weighted_sample(genders),
                "education_level": self._weighted_sample(education_levels),
                "income_bracket": self._weighted_sample(income_brackets),
                "location": self._weighted_sample(locations),
            }
            profiles.append(profile)

        return profiles

    def _weighted_sample(self, distribution: Dict[str, float]) -> str:
        """
        Losuj element z rozkÅ‚adu waÅ¼onego (weighted sampling)

        Args:
            distribution: SÅ‚ownik kategoria -> prawdopodobieÅ„stwo (suma = 1.0)

        Returns:
            Wylosowana kategoria jako string

        Raises:
            ValueError: JeÅ›li rozkÅ‚ad jest pusty
        """
        if not distribution:
            raise ValueError("Distribution cannot be empty")
        categories = list(distribution.keys())
        weights = list(distribution.values())
        return self._rng.choice(categories, p=weights)

    def _sanitize_text(self, text: str, preserve_paragraphs: bool = False) -> str:
        """
        Sanityzuj tekst wygenerowany przez LLM, usuwajÄ…c nadmierne biaÅ‚e znaki

        Metoda ta usuwa:
        - Nadmiarowe znaki nowej linii (\\n\\n -> pojedyncza spacja lub akapit)
        - Nadmiarowe spacje (wiele spacji -> jedna spacja)
        - Leading/trailing whitespace

        Args:
            text: Tekst do sanityzacji
            preserve_paragraphs: Czy zachowaÄ‡ podziaÅ‚ na akapity (dla background_story)
                                JeÅ›li True, zachowuje podziaÅ‚ na paragrafy (\\n\\n)
                                JeÅ›li False, zamienia wszystkie \\n na spacje

        Returns:
            Zsanityzowany tekst bez nadmiarowych biaÅ‚ych znakÃ³w

        PrzykÅ‚ady:
            >>> _sanitize_text("ZawÃ³d\\n\\nJuÅ¼")
            "ZawÃ³d JuÅ¼"
            >>> _sanitize_text("Tekst  z   wieloma    spacjami")
            "Tekst z wieloma spacjami"
            >>> _sanitize_text("Para 1\\n\\nPara 2", preserve_paragraphs=True)
            "Para 1\\n\\nPara 2"
        """
        if not text:
            return text

        if preserve_paragraphs:
            # Dla background_story - zachowaj podziaÅ‚ na akapity ale znormalizuj kaÅ¼dy akapit
            paragraphs = text.split('\n')
            paragraphs = [re.sub(r'\s+', ' ', p).strip() for p in paragraphs if p.strip()]
            return '\n\n'.join(paragraphs)
        else:
            # Dla pÃ³l jednoliniowych - usuÅ„ wszystkie \\n i znormalizuj spacje
            return re.sub(r'\s+', ' ', text).strip()

    def _prepare_distribution(
        self, distribution: Dict[str, float], fallback: Dict[str, float]
    ) -> Dict[str, float]:
        """
        Przygotuj i znormalizuj rozkÅ‚ad prawdopodobieÅ„stw

        Sprawdza czy rozkÅ‚ad jest poprawny, normalizuje go do sumy 1.0,
        lub zwraca fallback jeÅ›li rozkÅ‚ad jest niepoprawny.

        Args:
            distribution: RozkÅ‚ad do znormalizowania
            fallback: RozkÅ‚ad domyÅ›lny uÅ¼ywany gdy distribution jest pusty/bÅ‚Ä™dny

        Returns:
            Znormalizowany rozkÅ‚ad (suma = 1.0) lub fallback
        """
        if not distribution:
            return fallback
        total = sum(distribution.values())
        if total <= 0:
            return fallback
        # Pierwsza normalizacja - dziel przez sumÄ™
        normalized = {key: value / total for key, value in distribution.items()}
        normalized_total = sum(normalized.values())
        # Druga normalizacja jeÅ›li sÄ… bÅ‚Ä™dy zaokrÄ…gleÅ„ numerycznych
        if not np.isclose(normalized_total, 1.0):
            normalized = {
                key: value / normalized_total for key, value in normalized.items()
            }
        return normalized

    def sample_big_five_traits(self, personality_skew: Dict[str, float] = None) -> Dict[str, float]:
        """
        PrÃ³bkuj cechy osobowoÅ›ci Big Five z rozkÅ‚adÃ³w normalnych

        Model Big Five (OCEAN) mierzy piÄ™Ä‡ gÅ‚Ã³wnych wymiarÃ³w osobowoÅ›ci:
        - Openness (otwartoÅ›Ä‡): ciekawoÅ›Ä‡, kreatywnoÅ›Ä‡
        - Conscientiousness (sumiennoÅ›Ä‡): organizacja, dyscyplina
        - Extraversion (ekstrawersja): towarzyskoÅ›Ä‡, energia
        - Agreeableness (ugodowoÅ›Ä‡): empatia, wspÃ³Å‚praca
        - Neuroticism (neurotyzm): emocjonalnoÅ›Ä‡, podatnoÅ›Ä‡ na stres

        Args:
            personality_skew: Opcjonalny sÅ‚ownik do przesuniÄ™cia rozkÅ‚adÃ³w.
                              Klucze: 'openness', 'conscientiousness', etc.
                              WartoÅ›ci: 0.0-1.0 (0=niskie, 0.5=zbalansowane, 1.0=wysokie)

        Returns:
            SÅ‚ownik z wartoÅ›ciami cech w przedziale [0, 1]
        """
        skew = personality_skew or {}

        traits = {}
        for trait in ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            # DomyÅ›lnie: Å›rednia = 0.5, odchylenie standardowe = 0.15
            mean = skew.get(trait, 0.5)
            # Upewnij siÄ™ Å¼e Å›rednia jest w przedziale [0, 1]
            mean = np.clip(mean, 0.0, 1.0)

            # Losuj z rozkÅ‚adu normalnego i przytnij do [0, 1]
            value = np.clip(self._rng.normal(mean, 0.15), 0, 1)
            traits[trait] = value

        return traits

    def sample_cultural_dimensions(self) -> Dict[str, float]:
        """
        PrÃ³bkuj wymiary kulturowe Hofstede

        Model Hofstede opisuje rÃ³Å¼nice kulturowe w 6 wymiarach:
        - power_distance: akceptacja nierÃ³wnoÅ›ci wÅ‚adzy
        - individualism: indywidualizm vs kolektywizm
        - masculinity: asertywnoÅ›Ä‡ vs troska o innych
        - uncertainty_avoidance: unikanie niepewnoÅ›ci
        - long_term_orientation: orientacja dÅ‚ugo- vs krÃ³tkoterminowa
        - indulgence: pobÅ‚aÅ¼liwoÅ›Ä‡ vs powÅ›ciÄ…gliwoÅ›Ä‡

        Returns:
            SÅ‚ownik z wartoÅ›ciami wymiarÃ³w w przedziale [0, 1]
        """
        return {
            "power_distance": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "individualism": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "masculinity": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "uncertainty_avoidance": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "long_term_orientation": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "indulgence": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
        }

    async def _get_rag_context_for_persona(
        self, demographic: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Pobierz kontekst z RAG dla danego profilu demograficznego

        Args:
            demographic: Profil demograficzny persony

        Returns:
            Dict z kluczami: context (str), citations (list), query (str),
            graph_context (str), graph_nodes (list), search_type (str)
            lub None jeÅ›li RAG niedostÄ™pny
        """
        if not self.rag_service:
            return None

        import logging
        logger = logging.getLogger(__name__)

        try:
            context_data = await self.rag_service.get_demographic_insights(
                age_group=demographic.get('age_group', '25-34'),
                education=demographic.get('education_level', 'wyÅ¼sze'),
                location=demographic.get('location', 'Warszawa'),
                gender=demographic.get('gender', 'mÄ™Å¼czyzna')
            )

            # Loguj szczegÃ³Å‚y RAG context
            context_len = len(context_data.get('context', ''))
            graph_nodes_count = len(context_data.get('graph_nodes', []))
            search_type = context_data.get('search_type', 'unknown')
            citations_count = len(context_data.get('citations', []))

            logger.info(
                f"RAG context retrieved: {context_len} chars, "
                f"{graph_nodes_count} graph nodes, "
                f"{citations_count} citations, "
                f"search_type={search_type}"
            )

            # JeÅ›li mamy graph nodes, loguj ich typy
            if graph_nodes_count > 0:
                node_types = [node.get('type', 'Unknown') for node in context_data.get('graph_nodes', [])]
                logger.info(f"Graph node types: {', '.join(node_types)}")

            return context_data
        except Exception as e:
            logger.error(f"RAG context retrieval failed: {e}", exc_info=True)
            return None

    async def generate_persona_personality(
        self,
        demographic_profile: Dict[str, Any],
        psychological_profile: Dict[str, Any],
        use_rag: bool = True,  # NOWY PARAMETR - domyÅ›lnie wÅ‚Ä…czony
        advanced_options: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Generuj osobowoÅ›Ä‡ persony przy uÅ¼yciu LangChain + Gemini

        Tworzy szczegÃ³Å‚owy prompt oparty na profilach demograficznym i psychologicznym,
        opcjonalnie wzbogacony o kontekst RAG z bazy wiedzy o polskim spoÅ‚eczeÅ„stwie.

        Args:
            demographic_profile: SÅ‚ownik z danymi demograficznymi (wiek, pÅ‚eÄ‡, lokalizacja, etc.)
            psychological_profile: SÅ‚ownik z cechami Big Five i wymiarami Hofstede
            use_rag: Czy uÅ¼yÄ‡ kontekstu z bazy wiedzy RAG (default: True)
            advanced_options: Opcjonalne zaawansowane opcje generowania (nieuÅ¼ywane obecnie)

        Returns:
            Krotka (prompt_text, response_dict) gdzie:
            - prompt_text: PeÅ‚ny tekst wysÅ‚any do LLM (do logowania/debugowania)
            - response_dict: Sparsowana odpowiedÅº JSON z polami persony
                            + pole '_rag_citations' jeÅ›li uÅ¼yto RAG

        Raises:
            ValueError: JeÅ›li generowanie siÄ™ nie powiedzie lub odpowiedÅº jest niepoprawna
        """

        import logging
        logger = logging.getLogger(__name__)

        # Pobierz kontekst RAG jeÅ›li wÅ‚Ä…czony
        rag_context = None
        rag_citations = None
        rag_context_details = None
        if use_rag and self.rag_service:
            rag_data = await self._get_rag_context_for_persona(demographic_profile)
            if rag_data:
                rag_context = rag_data.get('context')
                rag_citations = rag_data.get('citations')

                # Przygotuj rag_context_details dla View Details
                rag_context_details = {
                    "search_type": rag_data.get('search_type', 'unknown'),
                    "num_results": rag_data.get('num_results', 0),
                    "graph_nodes_count": rag_data.get('graph_nodes_count', len(rag_data.get('graph_nodes', []))),
                    "graph_nodes": rag_data.get('graph_nodes', []),
                    "graph_context": rag_data.get('graph_context', ''),
                    "enriched_chunks": rag_data.get('enriched_chunks_count', 0),
                }

                if rag_data.get('query'):
                    rag_context_details["query"] = rag_data.get('query')
                if rag_context:
                    rag_context_details["context_preview"] = rag_context[:1500]
                    rag_context_details["context_length"] = len(rag_context)
                if rag_citations is not None:
                    rag_context_details["citations_count"] = len(rag_citations or [])

                logger.info(
                    f"Using RAG context: {len(rag_context or '')} chars, "
                    f"{len(rag_citations or [])} citations, "
                    f"search_type={rag_context_details['search_type']}"
                )

        # Pobierz target_audience_description i orchestration brief z advanced_options
        target_audience_desc = None
        orchestration_brief = None
        graph_insights = None
        allocation_reasoning = None

        if advanced_options:
            target_audience_desc = advanced_options.get('target_audience_description')
            orchestration_brief = advanced_options.get('orchestration_brief')
            graph_insights = advanced_options.get('graph_insights')
            allocation_reasoning = advanced_options.get('allocation_reasoning')

            if target_audience_desc:
                logger.info(f"Using target audience description: {target_audience_desc[:100]}...")
            if orchestration_brief:
                logger.info(f"Using orchestration brief: {orchestration_brief[:150]}... ({len(orchestration_brief)} chars)")

        # Generuj unikalny seed dla tej persony (do rÃ³Å¼nicowania)
        persona_seed = self._rng.integers(1000, 9999)

        # Losuj polskie imiÄ™ i nazwisko dla wiÄ™kszej rÃ³Å¼norodnoÅ›ci
        gender_lower = demographic_profile.get('gender', 'male').lower()
        if 'female' in gender_lower or 'kobieta' in gender_lower:
            suggested_first_name = self._rng.choice(POLISH_FEMALE_NAMES)
        else:
            suggested_first_name = self._rng.choice(POLISH_MALE_NAMES)
        suggested_surname = self._rng.choice(POLISH_SURNAMES)

        # Generuj prompt uÅ¼ywajÄ…c funkcji moduÅ‚owej
        prompt_text = create_persona_prompt(
            demographic_profile,
            psychological_profile,
            persona_seed=persona_seed,
            suggested_first_name=suggested_first_name,
            suggested_surname=suggested_surname,
            rag_context=rag_context,
            target_audience_description=target_audience_desc,
            orchestration_brief=orchestration_brief
        )

        try:
            logger.info(
                f"Generating persona with demographics: {demographic_profile.get('age_group')}, "
                f"{demographic_profile.get('gender')}, {demographic_profile.get('location')}"
                f" | RAG: {'YES' if rag_context else 'NO'}"
            )
            # WywoÅ‚aj Å‚aÅ„cuch LangChain (prompt -> LLM -> parser JSON)
            response = await self.persona_chain.ainvoke({"prompt": prompt_text})

            # Loguj odpowiedÅº do debugowania
            logger.info(f"LLM response type: {type(response)}, keys: {response.keys() if isinstance(response, dict) else 'N/A'}")

            # Waliduj wymagane pola
            required_fields = ["full_name", "persona_title", "headline", "background_story", "values", "interests"]
            missing_fields = [field for field in required_fields if not response.get(field)]
            if missing_fields:
                logger.error(
                    f"LLM response missing required fields: {missing_fields}. "
                    f"Response keys: {list(response.keys()) if isinstance(response, dict) else 'NOT A DICT'}"
                )

            # Sanityzuj wszystkie pola tekstowe (usuÅ„ nadmiarowe \n\n i whitespace)
            # KLUCZOWE: Zapobiega wyÅ›wietlaniu "ZawÃ³d\n\nJuÅ¼" w UI
            text_fields_single = [
                'occupation', 'full_name', 'location', 'headline',
                'persona_title', 'communication_style', 'decision_making_style'
            ]
            for field in text_fields_single:
                if field in response and isinstance(response[field], str):
                    response[field] = self._sanitize_text(response[field], preserve_paragraphs=False)

            # Sanityzuj background_story zachowujÄ…c podziaÅ‚ na akapity
            if 'background_story' in response and isinstance(response['background_story'], str):
                response['background_story'] = self._sanitize_text(response['background_story'], preserve_paragraphs=True)

            # Dodaj RAG citations i details do response (jeÅ›li byÅ‚y uÅ¼ywane)
            if rag_citations:
                response['_rag_citations'] = rag_citations
            if rag_context_details:
                response['_rag_context_details'] = rag_context_details

            return prompt_text, response
        except Exception as e:
            logger.error(f"Failed to generate persona: {str(e)[:500]}", exc_info=True)
            # Fallback dla bÅ‚Ä™dÃ³w parsowania
            raise ValueError(f"Failed to generate persona: {str(e)}")

    def validate_distribution(
        self,
        generated_personas: List[Dict[str, Any]],
        target_distribution: DemographicDistribution,
    ) -> Dict[str, Any]:
        """
        Waliduj czy wygenerowane persony pasujÄ… do docelowego rozkÅ‚adu (test chi-kwadrat)

        Sprawdza statystycznie czy rzeczywisty rozkÅ‚ad cech demograficznych w wygenerowanych
        personach odpowiada zadanemu rozkÅ‚adowi docelowemu. UÅ¼ywa testu chi-kwadrat dla
        kaÅ¼dej kategorii (wiek, pÅ‚eÄ‡, edukacja, dochÃ³d, lokalizacja).

        Args:
            generated_personas: Lista wygenerowanych person (jako sÅ‚owniki)
            target_distribution: Oczekiwany rozkÅ‚ad demograficzny

        Returns:
            SÅ‚ownik z wynikami testÃ³w dla kaÅ¼dej kategorii oraz ogÃ³lnÄ… ocenÄ…:
            {
                "age": {"p_value": float, "chi_square_statistic": float, ...},
                "gender": {...},
                "overall_valid": bool  # WartoÅ›Ä‡ True oznacza, Å¼e wszystkie p > 0.05
            }
        """
        results = {}

        # Testuj rozkÅ‚ad wieku (tylko jeÅ›li podany)
        if target_distribution.age_groups:
            results["age"] = self._chi_square_test(
                generated_personas, "age_group", target_distribution.age_groups
            )

        # Testuj rozkÅ‚ad pÅ‚ci (tylko jeÅ›li podany)
        if target_distribution.genders:
            results["gender"] = self._chi_square_test(
                generated_personas, "gender", target_distribution.genders
            )

        # Testuj rozkÅ‚ad edukacji (tylko jeÅ›li podany)
        if target_distribution.education_levels:
            results["education"] = self._chi_square_test(
                generated_personas, "education_level", target_distribution.education_levels
            )

        # Testuj rozkÅ‚ad dochodÃ³w (tylko jeÅ›li podany)
        if target_distribution.income_brackets:
            results["income"] = self._chi_square_test(
                generated_personas, "income_bracket", target_distribution.income_brackets
            )

        # Testuj rozkÅ‚ad lokalizacji (tylko jeÅ›li podany)
        if target_distribution.locations:
            results["location"] = self._chi_square_test(
                generated_personas, "location", target_distribution.locations
            )

        # OgÃ³lna walidacja - wszystkie p-wartoÅ›ci powinny byÄ‡ > 0.05
        all_p_values = [r["p_value"] for r in results.values() if "p_value" in r]
        results["overall_valid"] = all(
            p > settings.STATISTICAL_SIGNIFICANCE_THRESHOLD for p in all_p_values
        ) if all_p_values else True

        return results

    def _chi_square_test(
        self, personas: List[Dict[str, Any]], field: str, expected_dist: Dict[str, float]
    ) -> Dict[str, float]:
        """
        Wykonaj test chi-kwadrat dla konkretnego pola demograficznego

        Test chi-kwadrat sprawdza czy obserwowany rozkÅ‚ad kategorii (np. grup wiekowych)
        statystycznie rÃ³Å¼ni siÄ™ od rozkÅ‚adu oczekiwanego. Im wyÅ¼sze p-value, tym lepiej
        (p > 0.05 oznacza Å¼e rozkÅ‚ady sÄ… zgodne).

        Args:
            personas: Lista person do sprawdzenia
            field: Nazwa pola do przetestowania (np. "age_group", "gender")
            expected_dist: Oczekiwany rozkÅ‚ad prawdopodobieÅ„stw

        Returns:
            SÅ‚ownik z wynikami testu:
            - chi_square_statistic: wartoÅ›Ä‡ statystyki chi-kwadrat
            - p_value: p-wartoÅ›Ä‡ (>0.05 = dobre dopasowanie)
            - degrees_of_freedom: liczba stopni swobody
            - observed: obserwowane licznoÅ›ci
            - expected: oczekiwane licznoÅ›ci
        """
        # Filtruj kategorie z niepoprawnymi prawdopodobieÅ„stwami
        valid_categories = [
            (category, probability)
            for category, probability in expected_dist.items()
            if probability and probability > 0
        ]

        if not valid_categories:
            return {
                "chi_square_statistic": 0.0,
                "p_value": 1.0,
                "degrees_of_freedom": 0,
                "observed": {},
                "expected": {},
            }

        # Normalizuj prawdopodobieÅ„stwa do sumy = 1.0
        total_prob = sum(probability for _, probability in valid_categories)
        normalized_probs = {
            category: probability / total_prob for category, probability in valid_categories
        }

        # Policz obserwowane wystÄ…pienia kaÅ¼dej kategorii
        observed_counts = {category: 0 for category in normalized_probs}
        valid_samples = 0
        for persona in personas:
            value = persona.get(field)
            if value in observed_counts:
                observed_counts[value] += 1
                valid_samples += 1

        if valid_samples == 0:
            return {
                "chi_square_statistic": 0.0,
                "p_value": 1.0,
                "degrees_of_freedom": len(observed_counts) - 1,
                "observed": observed_counts,
                "expected": {category: 0.0 for category in observed_counts},
            }

        # Oblicz oczekiwane licznoÅ›ci (probability * total_count)
        expected_counts = {
            category: normalized_probs[category] * valid_samples
            for category in normalized_probs
        }

        # Przygotuj listy do testu chi-kwadrat (scipy wymaga list w tej samej kolejnoÅ›ci)
        observed = [observed_counts[category] for category in normalized_probs]
        expected = [expected_counts[category] for category in normalized_probs]

        # Wykonaj test chi-kwadrat
        chi2_stat, p_value = stats.chisquare(f_obs=observed, f_exp=expected)

        return {
            "chi_square_statistic": float(chi2_stat),
            "p_value": float(p_value),
            "degrees_of_freedom": len(normalized_probs) - 1,
            "observed": observed_counts,
            "expected": expected_counts,
            "sample_size": valid_samples,
        }

    # === SEGMENT-BASED ARCHITECTURE: ENFORCE DEMOGRAPHICS ===

    async def generate_persona_from_segment(
        self,
        segment_id: str,
        segment_name: str,
        segment_context: str,
        demographics_constraints: Dict[str, Any],  # Will be DemographicConstraints from SegmentDefinition
        graph_insights: List[Any] = None,
        rag_citations: List[Any] = None,
        personality_skew: Optional[Dict[str, float]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Generuj personÄ™ Z WYMUSZENIEM demographics z segmentu.

        KLUCZOWA RÃ“Å»NICA vs generate_persona_personality():
        - Demographics sÄ… ENFORCE (nie losowane poza bounds!)
        - Age = random.randint(age_min, age_max)
        - Gender = demographics_constraints.gender (NO randomization!)
        - Education/Income = random.choice z allowed lists

        Args:
            segment_id: ID segmentu
            segment_name: Nazwa segmentu (np. "MÅ‚odzi Prekariusze")
            segment_context: Kontekst spoÅ‚eczny segmentu
            demographics_constraints: Dict z keys: age_min, age_max, gender, education_levels, income_brackets, locations
            graph_insights: Insights filtrowane dla segmentu
            rag_citations: High-quality RAG citations
            personality_skew: Opcjonalne przesuniÄ™cie Big Five

        Returns:
            Tuple (prompt_text, response_dict)
        """
        import logging
        logger = logging.getLogger(__name__)

        # ENFORCE DEMOGRAPHICS
        age_min = demographics_constraints.get('age_min', 25)
        age_max = demographics_constraints.get('age_max', 34)
        age = self._rng.integers(age_min, age_max + 1)

        gender = demographics_constraints.get('gender', 'kobieta')
        education_levels = demographics_constraints.get('education_levels', ['wyÅ¼sze'])
        income_brackets = demographics_constraints.get('income_brackets', ['3000-5000 PLN'])
        locations = demographics_constraints.get('locations') or ['Warszawa']

        education = self._rng.choice(education_levels)
        income = self._rng.choice(income_brackets)
        location = self._rng.choice(locations)

        demographic_profile = {
            "age": age,
            "age_group": f"{age_min}-{age_max}",
            "gender": gender,
            "education_level": education,
            "income_bracket": income,
            "location": location
        }

        logger.info(
            f"ğŸ”’ ENFORCED demographics: age={age}, gender={gender}, "
            f"education={education}, income={income}, segment='{segment_name}'"
        )

        # Sample psychological profile
        psychological_profile = {
            **self.sample_big_five_traits(personality_skew),
            **self.sample_cultural_dimensions()
        }

        # Generate unique persona seed for diversity
        persona_seed = self._rng.integers(1000, 9999)

        # Suggest Polish name
        gender_lower = demographic_profile.get('gender', 'kobieta').lower()
        if 'female' in gender_lower or 'kobieta' in gender_lower:
            suggested_first_name = self._rng.choice(POLISH_FEMALE_NAMES)
        else:
            suggested_first_name = self._rng.choice(POLISH_MALE_NAMES)
        suggested_surname = self._rng.choice(POLISH_SURNAMES)

        # Create prompt with segment context (uÅ¼ywajÄ…c funkcji moduÅ‚owej)
        prompt_text = create_segment_persona_prompt(
            demographic_profile,
            psychological_profile,
            segment_name,
            segment_context,
            graph_insights,
            rag_citations,
            persona_seed,
            suggested_first_name,
            suggested_surname
        )

        try:
            response = await self.persona_chain.ainvoke({"prompt": prompt_text})

            # ENFORCE demographic fields (override LLM if needed)
            response['age'] = age
            response['gender'] = gender
            response['education_level'] = education
            response['income_bracket'] = income
            response['location'] = location

            # Sanityzuj wszystkie pola tekstowe (usuÅ„ nadmiarowe \n\n i whitespace)
            text_fields_single = [
                'occupation', 'full_name', 'location', 'headline',
                'persona_title', 'communication_style', 'decision_making_style'
            ]
            for field in text_fields_single:
                if field in response and isinstance(response[field], str):
                    response[field] = self._sanitize_text(response[field], preserve_paragraphs=False)

            # Sanityzuj background_story zachowujÄ…c podziaÅ‚ na akapity
            if 'background_story' in response and isinstance(response['background_story'], str):
                response['background_story'] = self._sanitize_text(response['background_story'], preserve_paragraphs=True)

            # Add segment tracking
            response['_segment_id'] = segment_id
            response['_segment_name'] = segment_name

            if rag_citations:
                response['_rag_citations'] = rag_citations

            logger.info(f"âœ… Persona generated: {response.get('full_name')} (segment='{segment_name}')")
            return prompt_text, response

        except Exception as e:
            logger.error(f"âŒ Failed to generate persona from segment: {e}", exc_info=True)
            raise ValueError(f"Failed to generate persona from segment '{segment_name}': {e}")
