"""
SegmentBriefService - generowanie i cache'owanie brief√≥w segment√≥w.

Ten serwis zarzƒÖdza:
1. Generowaniem d≈Çugich, ciekawych opis√≥w segment√≥w spo≈Çecznych (SegmentBrief)
2. Cache'owaniem brief√≥w w Redis (TTL 7 dni, wsp√≥lny dla wszystkich person w segmencie)
3. Generowaniem opis√≥w unikalno≈õci person ("Dlaczego ta osoba jest wyjƒÖtkowa")

Architecture:
- segment_id = hash(age, education, location, gender, income)
- Brief jest wsp√≥lny dla wszystkich person w projekcie z tym samym segment_id
- Redis cache: segment_brief:{project_id}:{segment_id}
- Hybrid generation: demografia + RAG + przyk≈Çadowe persony z segmentu
"""

import json
import logging
from datetime import datetime, timezone
from typing import Any
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from config import models, app
from config import prompts
from app.models.persona import Persona
from app.schemas.segment_brief import (
    SegmentBrief,
    SegmentBriefRequest,
    SegmentBriefResponse,
)
from app.services.shared import build_chat_model, get_polish_society_rag

logger = logging.getLogger(__name__)


class SegmentBriefService:
    """
    Serwis do generowania i cache'owania brief√≥w segment√≥w spo≈Çecznych.

    Responsibilities:
    1. Generate segment briefs (d≈Çugie, ciekawe opisy segment√≥w)
    2. Cache briefs in Redis (TTL 7 days, shared across personas in segment)
    3. Generate persona uniqueness descriptions
    4. Hybrid approach: demographics + RAG + example personas

    Performance:
    - Cache hit: < 50ms
    - Cache miss (generation): < 5s (depends on RAG query + LLM)
    - Redis TTL: 7 days (604800 seconds)
    """

    def __init__(self, db: AsyncSession):
        """
        Inicjalizuje serwis z LLM i RAG.

        Args:
            db: AsyncSession dla dostƒôpu do bazy danych
        """
        from config import models

        self.db = db

        # Model config z centralnego registry
        model_config = models.get("personas", "segment_brief")
        self.llm = build_chat_model(**model_config.params)

        # RAG dla kontekstu spo≈Çecznego Polski (singleton)
        self.rag_service = get_polish_society_rag()

        # Redis dla cache (u≈ºywamy tego samego co w innych miejscach)
        self.redis_client = None
        try:
            import redis.asyncio as redis
            self.redis_client = redis.from_url(
                app.redis.url,
                encoding="utf-8",
                decode_responses=True,
            )
            logger.info("‚úÖ SegmentBriefService: Redis po≈ÇƒÖczony")
        except Exception as exc:
            logger.warning(
                "‚ö†Ô∏è SegmentBriefService: Redis niedostƒôpny - cache wy≈ÇƒÖczony. Error: %s",
                exc
            )

        # Cache TTL (7 dni)
        self.CACHE_TTL_SECONDS = 7 * 24 * 60 * 60  # 604800 sekund

    @staticmethod
    def generate_segment_id(demographics: dict[str, Any]) -> str:
        """
        Generuje unikalny ID segmentu z demografii.

        Segment ID to hash demografii - wszystkie persony z tymi samymi
        cechami demograficznymi nale≈ºƒÖ do tego samego segmentu.

        Args:
            demographics: Dict z age, gender, education, location, income

        Returns:
            segment_id (np. "seg_25-34_wy≈ºsze_warszawa_kobieta")

        Example:
            >>> demographics = {
            ...     "age": "25-34",
            ...     "gender": "kobieta",
            ...     "education": "wy≈ºsze",
            ...     "location": "Warszawa",
            ...     "income": "7 500 - 10 000 z≈Ç"
            ... }
            >>> generate_segment_id(demographics)
            'seg_25-34_wy≈ºsze_warszawa_kobieta'
        """
        # Normalizuj klucze (niekt√≥re mogƒÖ byƒá age_group vs age, etc.)
        age = demographics.get("age") or demographics.get("age_group", "unknown")
        gender = demographics.get("gender", "unknown")
        education = demographics.get("education") or demographics.get("education_level", "unknown")
        location = demographics.get("location", "unknown")
        income = demographics.get("income") or demographics.get("income_bracket", "unknown")

        # Sanitize i lowercase dla sp√≥jno≈õci
        age = str(age).lower().replace(" ", "-")
        gender = str(gender).lower().replace(" ", "-")
        education = str(education).lower().replace(" ", "-")
        location = str(location).lower().replace(" ", "-")
        income = str(income).lower().replace(" ", "-")

        # Skr√≥cona wersja dla czytelno≈õci (zamiast hash)
        segment_id = f"seg_{age}_{education}_{location}_{gender}"

        return segment_id

    def _get_cache_key(self, project_id: str, segment_id: str) -> str:
        """Tworzy klucz Redis dla segment brief."""
        return f"segment_brief:{project_id}:{segment_id}"

    async def _get_from_cache(
        self, project_id: str, segment_id: str
    ) -> SegmentBrief | None:
        """
        Pobiera segment brief z Redis cache.

        Args:
            project_id: UUID projektu
            segment_id: ID segmentu

        Returns:
            SegmentBrief je≈õli w cache, None je≈õli nie ma
        """
        if not self.redis_client:
            return None

        cache_key = self._get_cache_key(project_id, segment_id)

        try:
            cached_data = await self.redis_client.get(cache_key)
            if cached_data:
                # Deserializuj JSON do SegmentBrief
                brief_dict = json.loads(cached_data)
                logger.info(
                    "‚úÖ Cache HIT: Segment brief '%s' dla projektu %s",
                    segment_id,
                    project_id
                )
                return SegmentBrief(**brief_dict)

            logger.info(
                "‚ùå Cache MISS: Segment brief '%s' dla projektu %s",
                segment_id,
                project_id
            )
            return None

        except Exception as exc:
            logger.warning(
                "‚ö†Ô∏è B≈ÇƒÖd odczytu z cache dla %s: %s",
                cache_key,
                exc
            )
            return None

    async def _save_to_cache(
        self,
        project_id: str,
        segment_id: str,
        brief: SegmentBrief
    ) -> None:
        """
        Zapisuje segment brief do Redis cache.

        Args:
            project_id: UUID projektu
            segment_id: ID segmentu
            brief: SegmentBrief do zapisania
        """
        if not self.redis_client:
            return

        cache_key = self._get_cache_key(project_id, segment_id)

        try:
            # Serializuj do JSON
            brief_json = brief.model_dump_json()

            # Zapisz z TTL
            await self.redis_client.setex(
                cache_key,
                self.CACHE_TTL_SECONDS,
                brief_json
            )

            logger.info(
                "üíæ Cache SAVE: Segment brief '%s' dla projektu %s (TTL: %s dni)",
                segment_id,
                project_id,
                self.CACHE_TTL_SECONDS // 86400
            )

        except Exception as exc:
            logger.warning(
                "‚ö†Ô∏è B≈ÇƒÖd zapisu do cache dla %s: %s",
                cache_key,
                exc
            )

    async def _get_example_personas_from_segment(
        self,
        project_id: UUID,
        segment_id: str,
        max_personas: int = 3
    ) -> list[Persona]:
        """
        Pobiera przyk≈Çadowe persony z danego segmentu dla projektu.

        Args:
            project_id: UUID projektu
            segment_id: ID segmentu
            max_personas: Max liczba person do pobrania

        Returns:
            Lista person (max 3)
        """
        try:
            result = await self.db.execute(
                select(Persona)
                .where(
                    Persona.project_id == project_id,
                    Persona.segment_id == segment_id,
                    Persona.deleted_at.is_(None)  # Tylko aktywne persony
                )
                .limit(max_personas)
            )
            personas = result.scalars().all()

            logger.info(
                "üìä Znaleziono %s przyk≈Çadowych person dla segmentu '%s'",
                len(personas),
                segment_id
            )

            return personas

        except Exception as exc:
            logger.warning(
                "‚ö†Ô∏è B≈ÇƒÖd pobierania przyk≈Çadowych person dla segmentu %s: %s",
                segment_id,
                exc
            )
            return []

    async def _fetch_rag_context(self, demographics: dict[str, Any]) -> str:
        """
        Pobiera kontekst RAG dla demografii segmentu.

        Args:
            demographics: Dict z age, gender, education, location, income

        Returns:
            Sformatowany kontekst z RAG (max 1500 chars)
        """
        if not self.rag_service or not self.rag_service.vector_store:
            logger.warning("RAG service niedostƒôpny - kontekst pusty")
            return "Brak dostƒôpnego kontekstu RAG."

        # Buduj query z demografii
        age = demographics.get("age") or demographics.get("age_group", "")
        gender = demographics.get("gender", "")
        education = demographics.get("education") or demographics.get("education_level", "")
        location = demographics.get("location", "")

        query = f"demographics statistics Poland {age} {gender} {education} {location}"

        try:
            # Hybrid search (top 5 results)
            documents = await self.rag_service.hybrid_search(query=query, top_k=5)

            # Formatuj kontekst
            if not documents:
                return "Brak danych RAG dla tej demografii."

            formatted = "=== KONTEKST Z RAG (Polskie dane spo≈Çeczne) ===\n\n"
            for idx, doc in enumerate(documents[:5], 1):
                content = doc.page_content[:250]  # Max 250 chars per doc
                formatted += f"[{idx}] {content}\n\n"

            # Truncate do 1500 chars total
            if len(formatted) > 1500:
                formatted = formatted[:1500] + "..."

            logger.info("‚úÖ RAG context pobrano: %s znak√≥w", len(formatted))
            return formatted

        except Exception as exc:
            logger.warning("‚ö†Ô∏è B≈ÇƒÖd pobierania RAG context: %s", exc)
            return "B≈ÇƒÖd podczas pobierania kontekstu RAG."

    def _format_example_personas(self, personas: list[Persona]) -> str:
        """
        Formatuje przyk≈Çadowe persony do prompta.

        Args:
            personas: Lista person

        Returns:
            Sformatowany string z przyk≈Çadami
        """
        if not personas:
            return "Brak przyk≈Çadowych person z tego segmentu."

        formatted = "=== PRZYK≈ÅADOWE PERSONY Z TEGO SEGMENTU ===\n\n"

        for idx, persona in enumerate(personas, 1):
            formatted += f"**Persona {idx}: {persona.full_name}**\n"
            formatted += f"- Wiek: {persona.age}, Zaw√≥d: {persona.occupation}\n"
            formatted += f"- Wykszta≈Çcenie: {persona.education_level}\n"
            formatted += f"- Doch√≥d: {persona.income_bracket}\n"

            # Skr√≥cony background (max 200 chars)
            if persona.background_story:
                bg_short = persona.background_story[:200]
                if len(persona.background_story) > 200:
                    bg_short += "..."
                formatted += f"- Historia: {bg_short}\n"

            # Warto≈õci (top 3)
            if persona.values:
                values_str = ", ".join(persona.values[:3])
                formatted += f"- Warto≈õci: {values_str}\n"

            formatted += "\n"

        return formatted

    async def generate_segment_brief(
        self,
        demographics: dict[str, Any],
        project_id: UUID,
        max_example_personas: int = 3,
        force_refresh: bool = False
    ) -> SegmentBrief:
        """
        Generuje lub pobiera z cache segment brief.

        Flow:
        1. Wygeneruj segment_id z demografii
        2. Sprawd≈∫ cache (je≈õli !force_refresh)
        3. Je≈õli cache miss:
           a. Pobierz RAG context
           b. Pobierz przyk≈Çadowe persony z segmentu
           c. Wygeneruj brief przez LLM (Gemini 2.5 Pro)
           d. Zapisz do cache
        4. Zwr√≥ƒá brief

        Args:
            demographics: Dict z age, gender, education, location, income
            project_id: UUID projektu
            max_example_personas: Max liczba przyk≈Çadowych person
            force_refresh: Czy pominƒÖƒá cache i wygenerowaƒá na nowo

        Returns:
            SegmentBrief

        Performance:
            - Cache hit: < 50ms
            - Cache miss: < 5s (RAG + LLM)
        """
        import time
        start_time = time.time()

        # 1. Wygeneruj segment_id
        segment_id = self.generate_segment_id(demographics)
        logger.info(
            "üéØ Generowanie segment brief dla: %s (project: %s)",
            segment_id,
            project_id
        )

        # 2. Sprawd≈∫ cache (je≈õli !force_refresh)
        if not force_refresh:
            cached_brief = await self._get_from_cache(str(project_id), segment_id)
            if cached_brief:
                elapsed_ms = int((time.time() - start_time) * 1000)
                logger.info(
                    "‚úÖ Segment brief z cache (elapsed: %sms)",
                    elapsed_ms
                )
                return cached_brief

        # 3. Cache miss - generuj brief
        logger.info("ü§ñ Generowanie nowego segment brief (LLM + RAG)...")

        # 3a. Pobierz RAG context
        rag_context = await self._fetch_rag_context(demographics)

        # 3b. Pobierz przyk≈Çadowe persony
        example_personas = await self._get_example_personas_from_segment(
            project_id, segment_id, max_example_personas
        )
        example_personas_text = self._format_example_personas(example_personas)

        # 3c. Wygeneruj segment name (kr√≥tka nazwa segmentu)
        segment_name = await self._generate_segment_name(demographics, rag_context)

        # 3d. Zbuduj prompt dla segment brief
        prompt = self._build_segment_brief_prompt(
            segment_name=segment_name,
            demographics=demographics,
            rag_context=rag_context,
            example_personas=example_personas_text
        )

        # 3e. Wywo≈Çaj LLM
        try:
            response = await self.llm.ainvoke(prompt)
            description = response.content.strip() if hasattr(response, 'content') else str(response).strip()

            # Walidacja d≈Çugo≈õci (400-800 s≈Ç√≥w = ~2400-4800 znak√≥w)
            if len(description) < 1000:
                logger.warning(
                    "‚ö†Ô∏è Generated brief zbyt kr√≥tki (%s chars) - mog≈Ço byƒá przerwane",
                    len(description)
                )
            elif len(description) > 6000:
                logger.warning(
                    "‚ö†Ô∏è Generated brief zbyt d≈Çugi (%s chars) - obciƒôcie do 5000",
                    len(description)
                )
                description = description[:5000]

        except Exception as exc:
            logger.error("‚ùå B≈ÇƒÖd generowania segment brief: %s", exc, exc_info=True)
            # Fallback minimal brief
            description = self._generate_fallback_brief(segment_name, demographics)

        # 3f. Wygeneruj social_context i characteristics
        social_context = await self._generate_social_context(
            segment_name, demographics, rag_context
        )
        characteristics = self._extract_characteristics(demographics, rag_context)

        # 3g. Stw√≥rz SegmentBrief object
        brief = SegmentBrief(
            segment_id=segment_id,
            segment_name=segment_name,
            description=description,
            social_context=social_context,
            characteristics=characteristics,
            based_on_personas_count=len(example_personas),
            demographics=demographics,
            generated_at=datetime.now(timezone.utc),
            generated_by="gemini-2.5-pro"
        )

        # 4. Zapisz do cache
        await self._save_to_cache(str(project_id), segment_id, brief)

        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.info(
            "‚úÖ Segment brief wygenerowany (elapsed: %sms, length: %s chars)",
            elapsed_ms,
            len(description)
        )

        return brief

    async def _generate_segment_name(
        self,
        demographics: dict[str, Any],
        rag_context: str
    ) -> str:
        """
        Generuje m√≥wiƒÖcƒÖ nazwƒô segmentu (np. 'M≈Çodzi Prekariusze').

        Args:
            demographics: Demografia segmentu
            rag_context: Kontekst RAG

        Returns:
            Nazwa segmentu (5-60 chars)
        """
        age = demographics.get("age") or demographics.get("age_group", "unknown")
        gender = demographics.get("gender", "unknown")
        education = demographics.get("education") or demographics.get("education_level", "unknown")
        income = demographics.get("income") or demographics.get("income_bracket", "unknown")

        # Skr√≥cony RAG context (max 200 chars)
        rag_short = rag_context[:200] if rag_context else "Brak"

        prompt = f"""Stw√≥rz trafnƒÖ, M√ìWIƒÑCƒÑ nazwƒô dla segmentu demograficznego.

DEMOGRAFIA:
- Wiek: {age}
- P≈Çeƒá: {gender}
- Wykszta≈Çcenie: {education}
- Doch√≥d: {income}

RAG INSIGHTS:
{rag_short}

ZASADY:
- 2-4 s≈Çowa (np. "M≈Çodzi Prekariusze", "AspirujƒÖce Profesjonalistki 35-44")
- Oddaje kluczowƒÖ charakterystykƒô (wiek + status spo≈Çeczno-ekonomiczny)
- Polski jƒôzyk, naturalnie brzmi
- Unikaj og√≥lnik√≥w

ZWR√ìƒÜ TYLKO NAZWƒò (bez cudzys≈Çow√≥w):"""

        try:
            # Gemini Flash dla szybkiego naming
            llm_flash = build_chat_model(
                model=models.config.get("defaults", {}).get("chat", {}).get("model", "gemini-2.5-flash"),
                temperature=0.7,
                max_tokens=50,
                timeout=10,
            )

            response = await llm_flash.ainvoke(prompt)
            name = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            name = name.strip('"\'')  # Remove quotes

            # Walidacja (5-60 chars)
            if 5 <= len(name) <= 60:
                return name

            # Fallback
            logger.warning("Generated name invalid length: '%s' - using fallback", name)

        except Exception as exc:
            logger.warning("B≈ÇƒÖd generowania segment name: %s - using fallback", exc)

        # Fallback name
        return f"Segment {age}, {gender}"

    def _build_segment_brief_prompt(
        self,
        segment_name: str,
        demographics: dict[str, Any],
        rag_context: str,
        example_personas: str
    ) -> str:
        """Buduje prompt dla generowania segment brief."""

        age_range = demographics.get("age") or demographics.get("age_group", "unknown")
        gender = demographics.get("gender", "unknown")
        education = demographics.get("education") or demographics.get("education_level", "unknown")
        location = demographics.get("location", "unknown")
        income = demographics.get("income") or demographics.get("income_bracket", "unknown")

        # Get prompts from YAML config
        polish_expert = prompts.get("system.polish_society_expert")
        storytelling = prompts.get("system.storytelling")
        conversational = prompts.get("system.conversational_tone")
        segment_brief_prompt = prompts.get("personas.segment_brief")

        # Build system prompt by combining multiple system prompts
        system_messages = []
        for prompt in [polish_expert, storytelling, conversational]:
            # Extract system messages from each prompt
            for msg in prompt.messages:
                if msg.role == "system":
                    system_messages.append(msg.content)

        combined_system_prompt = "\n\n".join(system_messages)

        # Render segment brief prompt with variables
        rendered_brief = segment_brief_prompt.render(
            segment_name=segment_name,
            age_range=age_range,
            gender=gender,
            education=education,
            location=location,
            income=income,
            rag_context=rag_context,
            example_personas=example_personas
        )

        # Extract system content from segment_brief_prompt (it's already in messages format)
        segment_brief_content = rendered_brief[0].content if rendered_brief else ""

        return f"{combined_system_prompt}\n\n{segment_brief_content}"

    async def _generate_social_context(
        self,
        segment_name: str,
        demographics: dict[str, Any],
        rag_context: str
    ) -> str:
        """
        Generuje social context (300-500 s≈Ç√≥w).

        Args:
            segment_name: Nazwa segmentu
            demographics: Demografia
            rag_context: Kontekst RAG

        Returns:
            Social context string
        """
        # Dla prostoty, wyekstraktujemy to z g≈Ç√≥wnego briefu lub u≈ºyjemy uproszczonej wersji
        # W produkcji mo≈ºna to wygenerowaƒá osobnym LLM callem
        age = demographics.get("age") or demographics.get("age_group", "unknown")
        education = demographics.get("education") or demographics.get("education_level", "unknown")
        location = demographics.get("location", "unknown")

        return (
            f"Segment '{segment_name}' charakteryzuje siƒô specyficznym kontekstem spo≈Çeczno-ekonomicznym. "
            f"Osoby w wieku {age}, z wykszta≈Çceniem {education}, mieszkajƒÖce w lokalizacji {location}, "
            f"znajdujƒÖ siƒô w unikalnym momencie ≈ºycia, kt√≥ry kszta≈Çtuje ich warto≈õci, aspiracje i wyzwania."
        )

    def _extract_characteristics(
        self,
        demographics: dict[str, Any],
        rag_context: str
    ) -> list[str]:
        """
        Ekstraktuje 5-7 kluczowych cech segmentu.

        Args:
            demographics: Demografia
            rag_context: Kontekst RAG

        Returns:
            Lista cech (strings)
        """
        characteristics = []

        # Z demografii
        age = demographics.get("age") or demographics.get("age_group", "")
        if "25-34" in str(age):
            characteristics.append("M≈Çodzi profesjonali≈õci")
        elif "35-44" in str(age):
            characteristics.append("Established professionals")

        education = demographics.get("education") or demographics.get("education_level", "")
        if "wy≈ºsze" in str(education).lower() or "magisterskie" in str(education).lower():
            characteristics.append("Wysoko wykszta≈Çceni")

        location = demographics.get("location", "")
        if location.lower() in ["warszawa", "krak√≥w", "wroc≈Çaw", "gda≈Ñsk", "pozna≈Ñ"]:
            characteristics.append("Mieszka≈Ñcy du≈ºych miast")

        # Domy≈õlne cechy (je≈õli za ma≈Ço)
        if len(characteristics) < 3:
            characteristics.extend([
                "Aktywni zawodowo",
                "Otwarci na zmiany",
                "Konsumenci ≈õwiadomi"
            ])

        return characteristics[:7]  # Max 7

    def _generate_fallback_brief(
        self,
        segment_name: str,
        demographics: dict[str, Any]
    ) -> str:
        """Fallback brief gdy LLM zawiedzie."""
        age = demographics.get("age") or demographics.get("age_group", "unknown")
        education = demographics.get("education") or demographics.get("education_level", "unknown")
        location = demographics.get("location", "unknown")

        return (
            f"Segment '{segment_name}' obejmuje osoby w wieku {age}, "
            f"z wykszta≈Çceniem {education}, mieszkajƒÖce w {location}. "
            f"Ta grupa demograficzna stanowi istotnƒÖ czƒô≈õƒá polskiego spo≈Çecze≈Ñstwa "
            f"i charakteryzuje siƒô specyficznymi warto≈õciami, aspiracjami oraz wyzwaniami ≈ºyciowymi."
        )

    async def generate_persona_uniqueness(
        self,
        persona: Persona,
        segment_brief: SegmentBrief
    ) -> str:
        """
        Generuje opis unikalno≈õci persony w kontek≈õcie segmentu.

        Args:
            persona: Persona object
            segment_brief: Brief segmentu do kt√≥rego nale≈ºy persona

        Returns:
            Opis unikalno≈õci (2-4 zdania, max 300 znak√≥w)
        """
        # Skr√≥ƒá segment brief summary (max 500 chars)
        brief_summary = segment_brief.description[:500]
        if len(segment_brief.description) > 500:
            brief_summary += "..."

        # Format values i interests
        values_str = ", ".join(persona.values[:3]) if persona.values else "brak danych"
        interests_str = ", ".join(persona.interests[:3]) if persona.interests else "brak danych"

        # Skr√≥ƒá background (max 400 chars)
        background = persona.background_story[:400] if persona.background_story else "Brak historii"
        if persona.background_story and len(persona.background_story) > 400:
            background += "..."

        # U≈ºyj segment_name z persony (z orchestration) dla consistency
        # segment_brief.segment_name mo≈ºe byƒá inne (generowane dynamicznie przez LLM)
        segment_name_to_use = persona.segment_name or segment_brief.segment_name

        # Get persona uniqueness prompt from YAML config
        uniqueness_prompt = prompts.get("personas.persona_uniqueness")

        # Render prompt with persona data
        rendered_messages = uniqueness_prompt.render(
            persona_name=persona.full_name or "Ta osoba",
            segment_name=segment_name_to_use,
            age=persona.age,
            occupation=persona.occupation or "brak danych",
            background_story=background,
            values=values_str,
            interests=interests_str,
            segment_brief_summary=brief_summary
        )

        # Convert to string prompt (combine all messages)
        prompt = "\n\n".join(msg.content for msg in rendered_messages)

        try:
            # Gemini Flash dla szybkiego generowania
            llm_flash = build_chat_model(
                model=models.config.get("defaults", {}).get("chat", {}).get("model", "gemini-2.5-flash"),
                temperature=0.7,
                max_tokens=600,  # 3-4 akapity (250-400 s≈Ç√≥w)
                timeout=20,
            )

            response = await llm_flash.ainvoke(prompt)
            uniqueness = response.content.strip() if hasattr(response, 'content') else str(response).strip()

            # Brak limitu d≈Çugo≈õci - AI dostaje pe≈ÇnƒÖ swobodƒô (250-400 s≈Ç√≥w jak w promptcie)

            logger.info(
                "‚úÖ Persona uniqueness wygenerowana dla %s (length: %s chars)",
                persona.full_name,
                len(uniqueness)
            )

            return uniqueness

        except Exception as exc:
            logger.warning(
                "‚ö†Ô∏è B≈ÇƒÖd generowania persona uniqueness dla %s: %s",
                persona.full_name,
                exc
            )

            # Fallback
            return (
                f"{persona.full_name or 'Ta osoba'} wyr√≥≈ºnia siƒô w segmencie "
                f"'{segment_brief.segment_name}' swoim unikalnym podej≈õciem do ≈ºycia i pracy."
            )

    async def get_or_generate_segment_brief(
        self,
        request: SegmentBriefRequest
    ) -> SegmentBriefResponse:
        """
        Public API: Pobiera segment brief z cache lub generuje nowy.

        Args:
            request: SegmentBriefRequest

        Returns:
            SegmentBriefResponse z briefem i metadanymi cache
        """
        project_id = UUID(request.project_id)

        # Generuj brief
        brief = await self.generate_segment_brief(
            demographics=request.demographics,
            project_id=project_id,
            max_example_personas=request.max_example_personas,
            force_refresh=request.force_refresh
        )

        # Sprawd≈∫ czy z cache
        from_cache = not request.force_refresh
        if from_cache and self.redis_client:
            # Sprawd≈∫ TTL
            cache_key = self._get_cache_key(str(project_id), brief.segment_id)
            try:
                ttl = await self.redis_client.ttl(cache_key)
                cache_ttl_seconds = ttl if ttl > 0 else None
            except Exception:  # pragma: no cover - best effort cache probe
                cache_ttl_seconds = None
        else:
            cache_ttl_seconds = None

        return SegmentBriefResponse(
            brief=brief,
            from_cache=from_cache,
            cache_ttl_seconds=cache_ttl_seconds
        )
