"""
Generator Person oparty na LangChain i Google Gemini

Ten moduÅ‚ generuje realistyczne, statystycznie reprezentatywne persony
dla badaÅ„ rynkowych przy uÅ¼yciu Google Gemini przez framework LangChain.

Kluczowe funkcjonalnoÅ›ci:
- Generowanie person zgodnie z zadanymi rozkÅ‚adami demograficznymi
- Walidacja statystyczna przy uÅ¼yciu testu chi-kwadrat
- Sampling cech osobowoÅ›ci (Big Five) i wymiarÃ³w kulturowych (Hofstede)
- Integracja z LangChain dla Å‚atwej zmiany modelu LLM
"""

import asyncio
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from scipy import stats
from dataclasses import dataclass

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough

from app.core.config import get_settings
# NOTE: Imports z constants.py usuniÄ™te - demographics teraz z orchestration, nie z sampling
from app.models import Persona
from app.services.core.clients import build_chat_model
from app.core.prompts import (
    COMPREHENSIVE_PERSONA_GENERATION_PROMPT,
    COMPREHENSIVE_PERSONA_GENERATION_SCHEMA,
    COMPREHENSIVE_PERSONA_MODEL_PARAMS,
)

settings = get_settings()

try:  # Opcjonalny import wyjÄ…tku z pakietu Google API (moÅ¼e nie byÄ‡ dostÄ™pny w testach)
    from google.api_core.exceptions import ServiceUnavailable  # type: ignore
except Exception:  # pragma: no cover - brak zaleÅ¼noÅ›ci w Å›rodowisku testowym
    ServiceUnavailable = None  # type: ignore

# Import RAG service (opcjonalny - tylko jeÅ›li RAG wÅ‚Ä…czony)
try:
    if settings.RAG_ENABLED:
        from app.services.rag.rag_hybrid_search_service import PolishSocietyRAG
        _rag_service_available = True
    else:
        _rag_service_available = False
except ImportError:
    _rag_service_available = False


@dataclass
class DemographicDistribution:
    """
    RozkÅ‚ad demograficzny populacji docelowej

    KaÅ¼de pole to sÅ‚ownik mapujÄ…cy kategorie na prawdopodobieÅ„stwa (sumujÄ…ce siÄ™ do 1.0)
    PrzykÅ‚ad: {"18-24": 0.3, "25-34": 0.5, "35-44": 0.2}
    """
    age_groups: Dict[str, float]        # Grupy wiekowe
    genders: Dict[str, float]           # PÅ‚eÄ‡
    education_levels: Dict[str, float]  # Poziomy edukacji
    income_brackets: Dict[str, float]   # PrzedziaÅ‚y dochodowe
    locations: Dict[str, float]         # Lokalizacje geograficzne


class PersonaGeneratorLangChain:
    """
    Generator statystycznie reprezentatywnych person przy uÅ¼yciu LangChain + Gemini

    UÅ¼ywa Google Gemini do generowania realistycznych profili person na podstawie
    zadanych rozkÅ‚adÃ³w demograficznych i psychologicznych.
    """

    def __init__(self):
        """Inicjalizuj generator z konfiguracjÄ… LangChain i Gemini"""
        import logging
        logger = logging.getLogger(__name__)

        self.settings = settings
        self._rng = np.random.default_rng(self.settings.RANDOM_SEED)

        # Inicjalizujemy model Gemini z wyÅ¼szÄ… temperaturÄ… dla wiÄ™kszej rÃ³Å¼norodnoÅ›ci
        persona_model = getattr(settings, "PERSONA_GENERATION_MODEL", settings.DEFAULT_MODEL)

        self.llm = build_chat_model(
            model=persona_model,
            temperature=0.9,  # Podniesiona wartoÅ›Ä‡ dla bardziej kreatywnych, zrÃ³Å¼nicowanych person
            max_tokens=settings.MAX_TOKENS,
            top_p=0.95,
            top_k=40,
        )

        # Konfigurujemy parser JSON, aby wymusiÄ‡ strukturalnÄ… odpowiedÅº
        self.json_parser = JsonOutputParser()

        # Budujemy szablon promptu do generowania person
        self.persona_prompt = ChatPromptTemplate.from_messages([
            ("system", "JesteÅ› ekspertem od badaÅ„ rynkowych tworzÄ…cym realistyczne syntetyczne persony dla polskiego rynku. Zawsze odpowiadaj poprawnym JSONem."),
            ("user", "{prompt}")
        ])

        # SkÅ‚adamy Å‚aÅ„cuch LangChain (prompt -> LLM -> parser)
        self.persona_chain = (
            self.persona_prompt
            | self.llm
            | self.json_parser
        )

        # Inicjalizuj RAG service (opcjonalnie - tylko jeÅ›li wÅ‚Ä…czony)
        self.rag_service = None
        if _rag_service_available and settings.RAG_ENABLED:
            try:
                self.rag_service = PolishSocietyRAG()
                logger.info("RAG service initialized successfully in PersonaGenerator")
            except Exception as e:
                logger.warning(f"RAG service unavailable: {e}")

    # NOTE: sample_demographic_profile(), _weighted_sample(), _prepare_distribution()
    # zostaÅ‚y USUNIÄ˜TE - demographics teraz pochodzÄ… z orchestration, nie z sampling

    def sample_big_five_traits(self, personality_skew: Dict[str, float] = None) -> Dict[str, float]:
        """
        PrÃ³bkuj cechy osobowoÅ›ci Big Five z rozkÅ‚adÃ³w normalnych

        Model Big Five (OCEAN) mierzy piÄ™Ä‡ gÅ‚Ã³wnych wymiarÃ³w osobowoÅ›ci:
        - Openness (otwartoÅ›Ä‡): ciekawoÅ›Ä‡, kreatywnoÅ›Ä‡
        - Conscientiousness (sumiennoÅ›Ä‡): organizacja, dyscyplina
        - Extraversion (ekstrawersja): towarzyskoÅ›Ä‡, energia
        - Agreeableness (ugodowoÅ›Ä‡): empatia, wspÃ³Å‚praca
        - Neuroticism (neurotyzm): emocjonalnoÅ›Ä‡, podatnoÅ›Ä‡ na stres

        Args:
            personality_skew: Opcjonalny sÅ‚ownik do przesuniÄ™cia rozkÅ‚adÃ³w.
                              Klucze: 'openness', 'conscientiousness', etc.
                              WartoÅ›ci: 0.0-1.0 (0=niskie, 0.5=zbalansowane, 1.0=wysokie)

        Returns:
            SÅ‚ownik z wartoÅ›ciami cech w przedziale [0, 1]
        """
        skew = personality_skew or {}

        traits = {}
        for trait in ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            # DomyÅ›lnie: Å›rednia = 0.5, odchylenie standardowe = 0.15
            mean = skew.get(trait, 0.5)
            # Upewnij siÄ™ Å¼e Å›rednia jest w przedziale [0, 1]
            mean = np.clip(mean, 0.0, 1.0)

            # Losuj z rozkÅ‚adu normalnego i przytnij do [0, 1]
            value = np.clip(self._rng.normal(mean, 0.15), 0, 1)
            traits[trait] = value

        return traits

    def sample_cultural_dimensions(self) -> Dict[str, float]:
        """
        PrÃ³bkuj wymiary kulturowe Hofstede

        Model Hofstede opisuje rÃ³Å¼nice kulturowe w 6 wymiarach:
        - power_distance: akceptacja nierÃ³wnoÅ›ci wÅ‚adzy
        - individualism: indywidualizm vs kolektywizm
        - masculinity: asertywnoÅ›Ä‡ vs troska o innych
        - uncertainty_avoidance: unikanie niepewnoÅ›ci
        - long_term_orientation: orientacja dÅ‚ugo- vs krÃ³tkoterminowa
        - indulgence: pobÅ‚aÅ¼liwoÅ›Ä‡ vs powÅ›ciÄ…gliwoÅ›Ä‡

        Returns:
            SÅ‚ownik z wartoÅ›ciami wymiarÃ³w w przedziale [0, 1]
        """
        return {
            "power_distance": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "individualism": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "masculinity": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "uncertainty_avoidance": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "long_term_orientation": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
            "indulgence": np.clip(self._rng.normal(0.5, 0.2), 0, 1),
        }

    async def _get_rag_context_for_persona(
        self, demographic: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Pobierz kontekst z RAG dla danego profilu demograficznego

        Args:
            demographic: Profil demograficzny persony

        Returns:
            Dict z kluczami: context (str), citations (list), query (str),
            graph_context (str), graph_nodes (list), search_type (str)
            lub None jeÅ›li RAG niedostÄ™pny
        """
        if not self.rag_service:
            return None

        import logging
        logger = logging.getLogger(__name__)

        try:
            context_data = await self.rag_service.get_demographic_insights(
                age_group=demographic.get('age_group', '25-34'),
                education=demographic.get('education_level', 'wyÅ¼sze'),
                location=demographic.get('location', 'Warszawa'),
                gender=demographic.get('gender', 'mÄ™Å¼czyzna')
            )

            # Loguj szczegÃ³Å‚y RAG context
            context_len = len(context_data.get('context', ''))
            graph_nodes_count = len(context_data.get('graph_nodes', []))
            search_type = context_data.get('search_type', 'unknown')
            citations_count = len(context_data.get('citations', []))

            logger.info(
                f"RAG context retrieved: {context_len} chars, "
                f"{graph_nodes_count} graph nodes, "
                f"{citations_count} citations, "
                f"search_type={search_type}"
            )

            # JeÅ›li mamy graph nodes, loguj ich typy
            if graph_nodes_count > 0:
                node_types = [node.get('type', 'Unknown') for node in context_data.get('graph_nodes', [])]
                logger.info(f"Graph node types: {', '.join(node_types)}")

            return context_data
        except Exception as e:
            logger.error(f"RAG context retrieval failed: {e}", exc_info=True)
            return None

    async def generate_persona_personality(
        self,
        demographic_profile: Dict[str, Any],
        psychological_profile: Dict[str, Any],
        use_rag: bool = True,  # NOWY PARAMETR - domyÅ›lnie wÅ‚Ä…czony
        advanced_options: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Generuj osobowoÅ›Ä‡ persony przy uÅ¼yciu LangChain + Gemini

        Tworzy szczegÃ³Å‚owy prompt oparty na profilach demograficznym i psychologicznym,
        opcjonalnie wzbogacony o kontekst RAG z bazy wiedzy o polskim spoÅ‚eczeÅ„stwie.

        Args:
            demographic_profile: SÅ‚ownik z danymi demograficznymi (wiek, pÅ‚eÄ‡, lokalizacja, etc.)
            psychological_profile: SÅ‚ownik z cechami Big Five i wymiarami Hofstede
            use_rag: Czy uÅ¼yÄ‡ kontekstu z bazy wiedzy RAG (default: True)
            advanced_options: Opcjonalne zaawansowane opcje generowania (nieuÅ¼ywane obecnie)

        Returns:
            Krotka (prompt_text, response_dict) gdzie:
            - prompt_text: PeÅ‚ny tekst wysÅ‚any do LLM (do logowania/debugowania)
            - response_dict: Sparsowana odpowiedÅº JSON z polami persony
                            + pole '_rag_citations' jeÅ›li uÅ¼yto RAG

        Raises:
            ValueError: JeÅ›li generowanie siÄ™ nie powiedzie lub odpowiedÅº jest niepoprawna
        """

        import logging
        logger = logging.getLogger(__name__)

        # Pobierz kontekst RAG jeÅ›li wÅ‚Ä…czony
        rag_context = None
        rag_citations = None
        rag_context_details = None
        if use_rag and self.rag_service:
            rag_data = await self._get_rag_context_for_persona(demographic_profile)
            if rag_data:
                rag_context = rag_data.get('context')
                rag_citations = rag_data.get('citations')

                # Przygotuj rag_context_details dla View Details
                rag_context_details = {
                    "search_type": rag_data.get('search_type', 'unknown'),
                    "num_results": rag_data.get('num_results', 0),
                    "graph_nodes_count": rag_data.get('graph_nodes_count', len(rag_data.get('graph_nodes', []))),
                    "graph_nodes": rag_data.get('graph_nodes', []),
                    "graph_context": rag_data.get('graph_context', ''),
                    "enriched_chunks": rag_data.get('enriched_chunks_count', 0),
                }

                if rag_data.get('query'):
                    rag_context_details["query"] = rag_data.get('query')
                if rag_context:
                    rag_context_details["context_preview"] = rag_context[:1500]
                    rag_context_details["context_length"] = len(rag_context)
                if rag_citations is not None:
                    rag_context_details["citations_count"] = len(rag_citations or [])

                logger.info(
                    f"Using RAG context: {len(rag_context or '')} chars, "
                    f"{len(rag_citations or [])} citations, "
                    f"search_type={rag_context_details['search_type']}"
                )

        # Pobierz target_audience_description i orchestration brief z advanced_options
        target_audience_desc = None
        orchestration_brief = None
        graph_insights = None
        allocation_reasoning = None

        if advanced_options:
            target_audience_desc = advanced_options.get('target_audience_description')
            orchestration_brief = advanced_options.get('orchestration_brief')
            graph_insights = advanced_options.get('graph_insights')
            allocation_reasoning = advanced_options.get('allocation_reasoning')

            if target_audience_desc:
                logger.info(f"Using target audience description: {target_audience_desc[:100]}...")
            if orchestration_brief:
                logger.info(f"Using orchestration brief: {orchestration_brief[:150]}... ({len(orchestration_brief)} chars)")

        # Generuj prompt (z RAG, target audience, i orchestration brief jeÅ›li dostÄ™pne)
        prompt_text = self._create_persona_prompt(
            demographic_profile,
            psychological_profile,
            rag_context=rag_context,
            target_audience_description=target_audience_desc,
            orchestration_brief=orchestration_brief
        )

        try:
            logger.info(
                f"Generating persona with demographics: {demographic_profile.get('age_group')}, "
                f"{demographic_profile.get('gender')}, {demographic_profile.get('location')}"
                f" | RAG: {'YES' if rag_context else 'NO'}"
            )
            # WywoÅ‚aj Å‚aÅ„cuch LangChain (prompt -> LLM -> parser JSON)
            response = await self.persona_chain.ainvoke({"prompt": prompt_text})

            # Loguj odpowiedÅº do debugowania
            logger.info(f"LLM response type: {type(response)}, keys: {response.keys() if isinstance(response, dict) else 'N/A'}")

            # Waliduj wymagane pola
            required_fields = ["full_name", "persona_title", "headline", "background_story", "values", "interests"]
            missing_fields = [field for field in required_fields if not response.get(field)]
            if missing_fields:
                logger.error(
                    f"LLM response missing required fields: {missing_fields}. "
                    f"Response keys: {list(response.keys()) if isinstance(response, dict) else 'NOT A DICT'}"
                )

            # Dodaj RAG citations i details do response (jeÅ›li byÅ‚y uÅ¼ywane)
            if rag_citations:
                response['_rag_citations'] = rag_citations
            if rag_context_details:
                response['_rag_context_details'] = rag_context_details

            return prompt_text, response
        except Exception as e:
            logger.error(f"Failed to generate persona: {str(e)[:500]}", exc_info=True)
            # Fallback dla bÅ‚Ä™dÃ³w parsowania
            raise ValueError(f"Failed to generate persona: {str(e)}")

    def _create_persona_prompt(
        self,
        demographic: Dict[str, Any],
        psychological: Dict[str, Any],
        rag_context: Optional[str] = None,
        target_audience_description: Optional[str] = None,
        orchestration_brief: Optional[str] = None  # NOWY PARAMETR - dÅ‚ugi brief od Gemini 2.5 Pro
    ) -> str:
        """
        UtwÃ³rz prompt dla LLM do generowania persony - WERSJA POLSKA

        Tworzy szczegÃ³Å‚owy prompt zawierajÄ…cy:
        - Dane demograficzne i psychologiczne
        - InterpretacjÄ™ cech Big Five i Hofstede PO POLSKU
        - 3 przykÅ‚ady few-shot z polskimi personami
        - Opcjonalny kontekst RAG z bazy wiedzy o polskim spoÅ‚eczeÅ„stwie
        - Opcjonalny dodatkowy opis grupy docelowej od uÅ¼ytkownika
        - Opcjonalny orchestration brief (900-1200 znakÃ³w) od Gemini 2.5 Pro
        - Instrukcje jak stworzyÄ‡ unikalnÄ… polskÄ… personÄ™

        Args:
            demographic: Profil demograficzny (wiek, pÅ‚eÄ‡, edukacja, etc.)
            psychological: Profil psychologiczny (Big Five + Hofstede)
            rag_context: Opcjonalny kontekst z RAG (fragmenty z dokumentÃ³w)
            target_audience_description: Opcjonalny dodatkowy opis grupy docelowej
            orchestration_brief: Opcjonalny DÅUGI brief od orchestration agent (Gemini 2.5 Pro)

        Returns:
            PeÅ‚ny tekst prompta gotowy do wysÅ‚ania do LLM (po polsku)
        """

        # Generuj unikalny seed dla tej persony (do rÃ³Å¼nicowania)
        persona_seed = self._rng.integers(1000, 9999)

        # Pobierz wartoÅ›ci Big Five (interpretacjÄ™ robi LLM)
        openness_val = psychological.get('openness', 0.5)
        conscientiousness_val = psychological.get('conscientiousness', 0.5)
        extraversion_val = psychological.get('extraversion', 0.5)
        agreeableness_val = psychological.get('agreeableness', 0.5)
        neuroticism_val = psychological.get('neuroticism', 0.5)

        # Unified context section (merge RAG + Target Audience + Orchestration Brief)
        unified_context = ""
        if rag_context or target_audience_description or orchestration_brief:
            context_parts = []

            if rag_context:
                context_parts.append(f"ğŸ“Š KONTEKST RAG:\n{rag_context}")
            if orchestration_brief and orchestration_brief.strip():
                context_parts.append(f"ğŸ“‹ ORCHESTRATION BRIEF:\n{orchestration_brief.strip()}")
            if target_audience_description and target_audience_description.strip():
                context_parts.append(f"ğŸ¯ GRUPA DOCELOWA:\n{target_audience_description.strip()}")

            unified_context = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KONTEKST (RAG + Brief + Audience):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{chr(10).join(context_parts)}

âš ï¸ KLUCZOWE ZASADY:
â€¢ UÅ¼yj kontekstu jako TÅA Å¼ycia persony (nie cytuj statystyk!)
â€¢ StwÃ³rz FASCYNUJÄ„CÄ„ historiÄ™ - kontekst to fundament, nie lista faktÃ³w
â€¢ WskaÅºniki â†’ konkretne detale Å¼ycia (housing crisis â†’ wynajmuje, oszczÄ™dza)
â€¢ Trendy â†’ doÅ›wiadczenia Å¼yciowe (mobilnoÅ›Ä‡ â†’ zmiana 3 prac w 5 lat)
â€¢ NaturalnoÅ›Ä‡: "Jak wielu rÃ³wieÅ›nikÃ³w..." zamiast "67% absolwentÃ³w..."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""

        return f"""Expert: Syntetyczne persony dla polskiego rynku - UNIKALNE, REALISTYCZNE, SPÃ“JNE.

{unified_context}PERSONA #{persona_seed}

PROFIL:
â€¢ Wiek: {demographic.get('age_group')} | PÅ‚eÄ‡: {demographic.get('gender')} | Lokalizacja: {demographic.get('location')}
â€¢ WyksztaÅ‚cenie: {demographic.get('education_level')} | DochÃ³d: {demographic.get('income_bracket')}

OSOBOWOÅšÄ† (Big Five - wartoÅ›ci 0-1):
â€¢ OtwartoÅ›Ä‡ (Openness): {openness_val:.2f}
â€¢ SumiennoÅ›Ä‡ (Conscientiousness): {conscientiousness_val:.2f}
â€¢ Ekstrawersja (Extraversion): {extraversion_val:.2f}
â€¢ UgodowoÅ›Ä‡ (Agreeableness): {agreeableness_val:.2f}
â€¢ Neurotyzm (Neuroticism): {neuroticism_val:.2f}

Interpretacja Big Five: <0.4 = niskie, 0.4-0.6 = Å›rednie, >0.6 = wysokie.
Wykorzystaj te wartoÅ›ci do stworzenia spÃ³jnej osobowoÅ›ci i historii Å¼yciowej.

HOFSTEDE (wartoÅ›ci 0-1): PD={psychological.get('power_distance', 0.5):.2f} | IND={psychological.get('individualism', 0.5):.2f} | UA={psychological.get('uncertainty_avoidance', 0.5):.2f}

ZASADY IMION I NAZWISK:
â€¢ UÅ¼ywaj TYPOWYCH POLSKICH imion pasujÄ…cych do wieku i pÅ‚ci persony
â€¢ PrzykÅ‚ady imion: dla 25-34 lat (Julia, Kacper), dla 45-54 lat (MaÅ‚gorzata, Krzysztof), dla 55+ (GraÅ¼yna, StanisÅ‚aw)
â€¢ Nazwiska neutralne, popularne (np. Kowalski, Nowak, WiÅ›niewski, Kowalczyk)
â€¢ Imiona muszÄ… byÄ‡ realistyczne dla Polski 2025 i pasowaÄ‡ do pokolenia
â€¢ NIE wymyÅ›laj egzotycznych lub nieprawdopodobnych kombinacji

ZASADY ZAWODÃ“W:
â€¢ ZawÃ³d = wyksztaÅ‚cenie + dochÃ³d + brief
â€¢ UÅ¼ywaj TYLKO konkretnych, istniejÄ…cych zawodÃ³w w Polsce (nie abstrakcyjnych tytuÅ‚Ã³w)
â€¢ OsobowoÅ›Ä‡ â†’ historia (Oâ†’podrÃ³Å¼e, Sâ†’planowanie)
â€¢ Detale: dzielnice, marki, konkretne hobby

PRZYKÅAD:
{{"full_name": "Marek Kowalczyk", "persona_title": "GÅ‚Ã³wny KsiÄ™gowy", "headline": "PoznaÅ„ski ksiÄ™gowy (56) planujÄ…cy emeryturÄ™", "background_story": "28 lat w firmie, Å¼onaty, dwoje dorosÅ‚ych dzieci, kupiÅ‚ dziaÅ‚kÄ™ pod Poznaniem, skarbnik parafii", "values": ["StabilnoÅ›Ä‡", "LojalnoÅ›Ä‡", "Rodzina", "OdpowiedzialnoÅ›Ä‡"], "interests": ["WÄ™dkarstwo", "Majsterkowanie", "Grillowanie"], "communication_style": "formalny, face-to-face", "decision_making_style": "metodyczny, unika ryzyka", "typical_concerns": ["Emerytura", "Sukcesja", "Zdrowie"]}}

Generuj KOMPLETNIE INNÄ„ personÄ™. WYÅÄ„CZNIE JSON (bez markdown):
{{
  "full_name": "<polskie imiÄ™+nazwisko - typowe dla tego wieku>",
  "persona_title": "<konkretny zawÃ³d istniejÄ…cy w Polsce>",
  "headline": "<1 zdanie: wiek, zawÃ³d, motywacje>",
  "background_story": "<2-3 zdania: Å¼ycie, kariera, kontekst>",
  "values": ["<5-7 wartoÅ›ci - konkretne, Å¼yciowe>"],
  "interests": ["<5-7 hobby/aktywnoÅ›ci - realistyczne>"],
  "communication_style": "<jak siÄ™ komunikuje>",
  "decision_making_style": "<jak podejmuje decyzje>",
  "typical_concerns": ["<3-5 zmartwieÅ„/priorytetÃ³w>"]
}}"""

    def validate_distribution(
        self,
        generated_personas: List[Dict[str, Any]],
        target_distribution: DemographicDistribution,
    ) -> Dict[str, Any]:
        """
        Waliduj czy wygenerowane persony pasujÄ… do docelowego rozkÅ‚adu (test chi-kwadrat)

        Sprawdza statystycznie czy rzeczywisty rozkÅ‚ad cech demograficznych w wygenerowanych
        personach odpowiada zadanemu rozkÅ‚adowi docelowemu. UÅ¼ywa testu chi-kwadrat dla
        kaÅ¼dej kategorii (wiek, pÅ‚eÄ‡, edukacja, dochÃ³d, lokalizacja).

        Args:
            generated_personas: Lista wygenerowanych person (jako sÅ‚owniki)
            target_distribution: Oczekiwany rozkÅ‚ad demograficzny

        Returns:
            SÅ‚ownik z wynikami testÃ³w dla kaÅ¼dej kategorii oraz ogÃ³lnÄ… ocenÄ…:
            {
                "age": {"p_value": float, "chi_square_statistic": float, ...},
                "gender": {...},
                "overall_valid": bool  # WartoÅ›Ä‡ True oznacza, Å¼e wszystkie p > 0.05
            }
        """
        results = {}

        # Testuj rozkÅ‚ad wieku (tylko jeÅ›li podany)
        if target_distribution.age_groups:
            results["age"] = self._chi_square_test(
                generated_personas, "age_group", target_distribution.age_groups
            )

        # Testuj rozkÅ‚ad pÅ‚ci (tylko jeÅ›li podany)
        if target_distribution.genders:
            results["gender"] = self._chi_square_test(
                generated_personas, "gender", target_distribution.genders
            )

        # Testuj rozkÅ‚ad edukacji (tylko jeÅ›li podany)
        if target_distribution.education_levels:
            results["education"] = self._chi_square_test(
                generated_personas, "education_level", target_distribution.education_levels
            )

        # Testuj rozkÅ‚ad dochodÃ³w (tylko jeÅ›li podany)
        if target_distribution.income_brackets:
            results["income"] = self._chi_square_test(
                generated_personas, "income_bracket", target_distribution.income_brackets
            )

        # Testuj rozkÅ‚ad lokalizacji (tylko jeÅ›li podany)
        if target_distribution.locations:
            results["location"] = self._chi_square_test(
                generated_personas, "location", target_distribution.locations
            )

        # OgÃ³lna walidacja - wszystkie p-wartoÅ›ci powinny byÄ‡ > 0.05
        all_p_values = [r["p_value"] for r in results.values() if "p_value" in r]
        results["overall_valid"] = all(
            p > settings.STATISTICAL_SIGNIFICANCE_THRESHOLD for p in all_p_values
        ) if all_p_values else True

        return results

    def _chi_square_test(
        self, personas: List[Dict[str, Any]], field: str, expected_dist: Dict[str, float]
    ) -> Dict[str, float]:
        """
        Wykonaj test chi-kwadrat dla konkretnego pola demograficznego

        Test chi-kwadrat sprawdza czy obserwowany rozkÅ‚ad kategorii (np. grup wiekowych)
        statystycznie rÃ³Å¼ni siÄ™ od rozkÅ‚adu oczekiwanego. Im wyÅ¼sze p-value, tym lepiej
        (p > 0.05 oznacza Å¼e rozkÅ‚ady sÄ… zgodne).

        Args:
            personas: Lista person do sprawdzenia
            field: Nazwa pola do przetestowania (np. "age_group", "gender")
            expected_dist: Oczekiwany rozkÅ‚ad prawdopodobieÅ„stw

        Returns:
            SÅ‚ownik z wynikami testu:
            - chi_square_statistic: wartoÅ›Ä‡ statystyki chi-kwadrat
            - p_value: p-wartoÅ›Ä‡ (>0.05 = dobre dopasowanie)
            - degrees_of_freedom: liczba stopni swobody
            - observed: obserwowane licznoÅ›ci
            - expected: oczekiwane licznoÅ›ci
        """
        # Filtruj kategorie z niepoprawnymi prawdopodobieÅ„stwami
        valid_categories = [
            (category, probability)
            for category, probability in expected_dist.items()
            if probability and probability > 0
        ]

        if not valid_categories:
            return {
                "chi_square_statistic": 0.0,
                "p_value": 1.0,
                "degrees_of_freedom": 0,
                "observed": {},
                "expected": {},
            }

        # Normalizuj prawdopodobieÅ„stwa do sumy = 1.0
        total_prob = sum(probability for _, probability in valid_categories)
        normalized_probs = {
            category: probability / total_prob for category, probability in valid_categories
        }

        # Policz obserwowane wystÄ…pienia kaÅ¼dej kategorii
        observed_counts = {category: 0 for category in normalized_probs}
        valid_samples = 0
        for persona in personas:
            value = persona.get(field)
            if value in observed_counts:
                observed_counts[value] += 1
                valid_samples += 1

        if valid_samples == 0:
            return {
                "chi_square_statistic": 0.0,
                "p_value": 1.0,
                "degrees_of_freedom": len(observed_counts) - 1,
                "observed": observed_counts,
                "expected": {category: 0.0 for category in observed_counts},
            }

        # Oblicz oczekiwane licznoÅ›ci (probability * total_count)
        expected_counts = {
            category: normalized_probs[category] * valid_samples
            for category in normalized_probs
        }

        # Przygotuj listy do testu chi-kwadrat (scipy wymaga list w tej samej kolejnoÅ›ci)
        observed = [observed_counts[category] for category in normalized_probs]
        expected = [expected_counts[category] for category in normalized_probs]

        # Wykonaj test chi-kwadrat
        chi2_stat, p_value = stats.chisquare(f_obs=observed, f_exp=expected)

        return {
            "chi_square_statistic": float(chi2_stat),
            "p_value": float(p_value),
            "degrees_of_freedom": len(normalized_probs) - 1,
            "observed": observed_counts,
            "expected": expected_counts,
            "sample_size": valid_samples,
        }

    # === SEGMENT-BASED ARCHITECTURE: ENFORCE DEMOGRAPHICS ===

    async def generate_persona_from_segment(
        self,
        segment_id: str,
        segment_name: str,
        segment_context: str,
        demographics_constraints: Dict[str, Any],  # Will be DemographicConstraints from SegmentDefinition
        graph_insights: List[Any] = None,
        rag_citations: List[Any] = None,
        personality_skew: Optional[Dict[str, float]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Generuj personÄ™ Z WYMUSZENIEM demographics z segmentu.

        KLUCZOWA RÃ“Å»NICA vs generate_persona_personality():
        - Demographics sÄ… ENFORCE (nie losowane poza bounds!)
        - Age = random.randint(age_min, age_max)
        - Gender = demographics_constraints.gender (NO randomization!)
        - Education/Income = random.choice z allowed lists

        Args:
            segment_id: ID segmentu
            segment_name: Nazwa segmentu (np. "MÅ‚odzi Prekariusze")
            segment_context: Kontekst spoÅ‚eczny segmentu
            demographics_constraints: Dict z keys: age_min, age_max, gender, education_levels, income_brackets, locations
            graph_insights: Insights filtrowane dla segmentu
            rag_citations: High-quality RAG citations
            personality_skew: Opcjonalne przesuniÄ™cie Big Five

        Returns:
            Tuple (prompt_text, response_dict)
        """
        import logging
        logger = logging.getLogger(__name__)

        # ENFORCE DEMOGRAPHICS
        age_min = demographics_constraints.get('age_min', 25)
        age_max = demographics_constraints.get('age_max', 34)
        age = self._rng.integers(age_min, age_max + 1)

        gender = demographics_constraints.get('gender', 'kobieta')
        education_levels = demographics_constraints.get('education_levels', ['wyÅ¼sze'])
        income_brackets = demographics_constraints.get('income_brackets', ['3000-5000 PLN'])
        locations = demographics_constraints.get('locations') or ['Warszawa']

        education = self._rng.choice(education_levels)
        income = self._rng.choice(income_brackets)
        location = self._rng.choice(locations)

        demographic_profile = {
            "age": age,
            "age_group": f"{age_min}-{age_max}",
            "gender": gender,
            "education_level": education,
            "income_bracket": income,
            "location": location
        }

        logger.info(
            f"ğŸ”’ ENFORCED demographics: age={age}, gender={gender}, "
            f"education={education}, income={income}, segment='{segment_name}'"
        )

        # Sample psychological profile
        psychological_profile = {
            **self.sample_big_five_traits(personality_skew),
            **self.sample_cultural_dimensions()
        }

        # Create prompt with segment context
        prompt_text = self._create_segment_persona_prompt(
            demographic_profile,
            psychological_profile,
            segment_name,
            segment_context,
            graph_insights,
            rag_citations
        )

        try:
            response = await self.persona_chain.ainvoke({"prompt": prompt_text})

            # ENFORCE demographic fields (override LLM if needed)
            response['age'] = age
            response['gender'] = gender
            response['education_level'] = education
            response['income_bracket'] = income
            response['location'] = location

            # Add segment tracking
            response['_segment_id'] = segment_id
            response['_segment_name'] = segment_name

            if rag_citations:
                response['_rag_citations'] = rag_citations

            logger.info(f"âœ… Persona generated: {response.get('full_name')} (segment='{segment_name}')")
            return prompt_text, response

        except Exception as e:
            logger.error(f"âŒ Failed to generate persona from segment: {e}", exc_info=True)
            raise ValueError(f"Failed to generate persona from segment '{segment_name}': {e}")

    async def generate_comprehensive_persona(
        self,
        orchestration_brief: str,
        segment_characteristics: List[str],
        demographic_guidance: Dict[str, Any],
        rag_context: Optional[str] = None,
        psychological_profile: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """
        Generuj KOMPLETNÄ„ personÄ™ uÅ¼ywajÄ…c comprehensive prompt (LLM generuje ALL DATA).

        Ta metoda rÃ³Å¼ni siÄ™ od generate_persona_personality():
        - LLM generuje WSZYSTKIE dane razem (demographics + background_story + values + interests)
        - Brak post-processing polonizacji - LLM generuje od razu po polsku
        - demographic_guidance jest tylko sugestiÄ… dla LLM, nie requirement
        - UÅ¼ywa structured output dla reliability

        Args:
            orchestration_brief: Brief segmentu z orchestration service (kontekst spoÅ‚eczny)
            segment_characteristics: Lista 4-6 charakterystyk segmentu
            demographic_guidance: Orientacyjne demographics (guidance, nie requirement)
            rag_context: Opcjonalny kontekst z RAG
            psychological_profile: Opcjonalny Big Five + Hofstede (jeÅ›li None â†’ samplingujemy)

        Returns:
            Dict z ALL DATA:
            - Demographics: age, gender, location, education_level, income_bracket, occupation
            - Content: full_name, background_story, values, interests
            - Psychographics: openness, conscientiousness, ... (z psychological_profile)

        Raises:
            ValueError: JeÅ›li generowanie siÄ™ nie powiedzie
        """
        import logging
        logger = logging.getLogger(__name__)

        # Sample psychographics jeÅ›li nie podane
        if not psychological_profile:
            psychological_profile = {
                **self.sample_big_five_traits(),
                **self.sample_cultural_dimensions()
            }

        # Format segment characteristics
        segment_chars_text = "\n".join(f"- {char}" for char in segment_characteristics) if segment_characteristics else "Brak dodatkowych charakterystyk"

        # Format demographic guidance
        demographic_guidance_text = (
            f"â€¢ Wiek: {demographic_guidance.get('age', 'elastyczny')}\n"
            f"â€¢ PÅ‚eÄ‡: {demographic_guidance.get('gender', 'elastyczna')}\n"
            f"â€¢ Lokalizacja: {demographic_guidance.get('location', 'elastyczna')}\n"
            f"â€¢ WyksztaÅ‚cenie: {demographic_guidance.get('education_level', 'elastyczne')}\n"
            f"â€¢ DochÃ³d: {demographic_guidance.get('income_bracket', 'elastyczny')}"
        )

        # Build prompt
        prompt_text = COMPREHENSIVE_PERSONA_GENERATION_PROMPT.format(
            orchestration_brief=orchestration_brief or "Brak briefu",
            segment_characteristics=segment_chars_text,
            rag_context=rag_context or "Brak kontekstu RAG",
            demographic_guidance=demographic_guidance_text
        )

        try:
            # Build model with structured output
            comprehensive_model = build_chat_model(
                model=settings.PERSONA_GENERATION_MODEL,
                temperature=COMPREHENSIVE_PERSONA_MODEL_PARAMS["temperature"],
                max_tokens=COMPREHENSIVE_PERSONA_MODEL_PARAMS["max_tokens"],
                top_p=COMPREHENSIVE_PERSONA_MODEL_PARAMS["top_p"],
                top_k=COMPREHENSIVE_PERSONA_MODEL_PARAMS["top_k"],
            )

            # Use with_structured_output dla JSON reliability
            structured_model = comprehensive_model.with_structured_output(
                COMPREHENSIVE_PERSONA_GENERATION_SCHEMA
            )

            # Invoke LLM with simple retry logic for transient Gemini failures
            max_attempts = 3
            response: Optional[Dict[str, Any]] = None
            last_exception: Optional[Exception] = None

            for attempt in range(1, max_attempts + 1):
                try:
                    logger.info(
                        "Generating comprehensive persona with LLM (attempt %s/%s)...",
                        attempt,
                        max_attempts,
                    )
                    result = await structured_model.ainvoke([
                        {"role": "system", "content": "JesteÅ› ekspertem od badaÅ„ rynkowych tworzÄ…cym realistyczne syntetyczne persony dla polskiego rynku."},
                        {"role": "user", "content": prompt_text}
                    ])
                except Exception as invoke_exc:  # pragma: no cover - zaleÅ¼ne od klienta Gemini
                    if attempt < max_attempts and self._is_retryable_gemini_error(invoke_exc):
                        logger.warning(
                            "Gemini API call failed (attempt %s/%s): %s. Retrying...",
                            attempt,
                            max_attempts,
                            invoke_exc,
                        )
                        last_exception = invoke_exc
                        await asyncio.sleep(0.5 * attempt)
                        continue
                    raise

                if isinstance(result, dict):
                    response = result
                    break

                if result is None and attempt < max_attempts:
                    logger.warning(
                        "Structured output zwrÃ³ciÅ‚ None (attempt %s/%s). Retrying...",
                        attempt,
                        max_attempts,
                    )
                    await asyncio.sleep(0.5 * attempt)
                    continue

                raise ValueError(f"Expected dict from structured output, got {type(result)}")

            if response is None:
                if last_exception:
                    raise ValueError(f"Failed to generate comprehensive persona after retries: {last_exception}") from last_exception
                raise ValueError("Expected dict from structured output, got <class 'NoneType'>")

            # Normalizuj gender do standardowego formatu (case-insensitive, obsÅ‚uga wariantÃ³w)
            if 'gender' in response:
                gender_raw = str(response['gender']).strip().lower()
                if gender_raw in ['kobieta', 'woman', 'female', 'f']:
                    response['gender'] = 'Kobieta'
                elif gender_raw in ['mÄ™Å¼czyzna', 'mezczyzna', 'man', 'male', 'm']:
                    response['gender'] = 'MÄ™Å¼czyzna'
                else:
                    # Fallback do capitalize jeÅ›li nieznany format
                    response['gender'] = response['gender'].capitalize()

            # Dodaj psychographics do response
            response.update(psychological_profile)

            # Dodaj persona_title jako alias dla occupation (backward compatibility)
            if 'occupation' in response and 'persona_title' not in response:
                response['persona_title'] = response['occupation']

            # Dodaj headline jeÅ›li brakuje (fallback)
            if 'headline' not in response or not response.get('headline'):
                response['headline'] = f"{response.get('full_name', 'Persona')}, {response.get('age', '?')} lat - {response.get('occupation', 'ZawÃ³d')}"

            logger.info(
                f"âœ… Comprehensive persona generated: {response.get('full_name')} "
                f"({response.get('age')} lat, {response.get('gender')}, {response.get('location')})"
            )

            return response

        except Exception as e:
            logger.error(f"âŒ Failed to generate comprehensive persona: {e}", exc_info=True)
            raise ValueError(f"Failed to generate comprehensive persona: {e}")

    @staticmethod
    def _is_retryable_gemini_error(exc: Exception) -> bool:
        """Heurystycznie okreÅ›l, czy bÅ‚Ä…d Gemini warto sprÃ³bowaÄ‡ ponowiÄ‡."""
        if ServiceUnavailable is not None and isinstance(exc, ServiceUnavailable):
            return True

        message = str(exc).lower()
        retry_markers = (
            "503",
            "service unavailable",
            "temporarily overloaded",
            "retry later",
        )
        return any(marker in message for marker in retry_markers)

    def _create_segment_persona_prompt(
        self,
        demographic: Dict[str, Any],
        psychological: Dict[str, Any],
        segment_name: str,
        segment_context: str,
        graph_insights: List[Any],
        rag_citations: List[Any]
    ) -> str:
        """Create prompt for segment-based persona generation."""

        age = demographic.get('age', 30)
        gender = demographic.get('gender', 'kobieta')

        # Format insights
        insights_text = ""
        if graph_insights:
            insights_text = "\n".join([
                f"- {ins.get('summary', ins.get('streszczenie', 'N/A'))}"
                for ins in graph_insights[:5]
            ])

        return f"""Wygeneruj realistycznÄ… personÄ™ dla segmentu "{segment_name}".

CONSTRAINTS (MUSISZ PRZESTRZEGAÄ†!):
â€¢ Wiek: {age} lat
â€¢ PÅ‚eÄ‡: {gender}
â€¢ WyksztaÅ‚cenie: {demographic.get('education_level')}
â€¢ DochÃ³d: {demographic.get('income_bracket')}
â€¢ Lokalizacja: {demographic.get('location')}

KONTEKST SEGMENTU:
{segment_context}

INSIGHTS:
{insights_text or "Brak insights"}

OSOBOWOÅšÄ† (Big Five):
â€¢ OtwartoÅ›Ä‡: {psychological.get('openness', 0.5):.2f}
â€¢ SumiennoÅ›Ä‡: {psychological.get('conscientiousness', 0.5):.2f}
â€¢ Ekstrawersja: {psychological.get('extraversion', 0.5):.2f}

ZASADY IMION:
â€¢ UÅ¼ywaj TYPOWYCH POLSKICH imion pasujÄ…cych do wieku {age} lat i pÅ‚ci {gender}
â€¢ PrzykÅ‚ady: dla 25-34 lat (Julia, Kacper), dla 45-54 lat (MaÅ‚gorzata, Krzysztof)
â€¢ Nazwiska popularne (Kowalski, Nowak, WiÅ›niewski)

ZASADY:
â€¢ Persona MUSI pasowaÄ‡ do constraints
â€¢ ZawÃ³d = wyksztaÅ‚cenie + dochÃ³d + kontekst segmentu
â€¢ UÅ¼ywaj kontekstu jako tÅ‚a (nie cytuj statystyk!)

ZWRÃ“Ä† JSON:
{{
  "full_name": "<typowe polskie imiÄ™+nazwisko dla tego wieku i pÅ‚ci>",
  "persona_title": "<konkretny zawÃ³d>",
  "headline": "<{age} lat, zawÃ³d, motywacje>",
  "background_story": "<2-3 zdania>",
  "values": ["<5-7 wartoÅ›ci>"],
  "interests": ["<5-7 hobby>"],
  "communication_style": "<styl>",
  "decision_making_style": "<styl>",
  "typical_concerns": ["<3-5 zmartwieÅ„>"]
}}"""
