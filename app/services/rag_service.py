"""Serwisy RAG odpowiedzialne za budowÄ™ grafu wiedzy i hybrydowe wyszukiwanie.

ModuÅ‚ udostÄ™pnia dwie komplementarne klasy:

* :class:`RAGDocumentService` â€“ odpowiada za peÅ‚ny pipeline przetwarzania
  dokumentÃ³w (ingest), utrzymanie indeksu wektorowego oraz grafu wiedzy w Neo4j
  i obsÅ‚ugÄ™ zapytaÅ„ Graph RAG.
* :class:`PolishSocietyRAG` â€“ realizuje hybrydowe wyszukiwanie kontekstu
  (vector + keyword + RRF) wykorzystywane w generatorze person.

Dokumentacja i komentarze pozostajÄ… po polsku, aby uÅ‚atwiÄ‡ wspÃ³Å‚pracÄ™ zespoÅ‚u.
"""

from __future__ import annotations

import asyncio
import logging
from pathlib import Path
from typing import Any, Dict, List, Tuple
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader
from langchain_community.graphs import Neo4jGraph
from langchain_community.vectorstores import Neo4jVector
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_experimental.graph_transformers.llm import LLMGraphTransformer
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from app.core.config import get_settings
from app.models.rag_document import RAGDocument

settings = get_settings()
logger = logging.getLogger(__name__)


class GraphRAGQuery(BaseModel):
    """Struktura odpowiedzi z LLM przeksztaÅ‚cajÄ…ca pytanie w zapytanie Cypher."""

    entities: List[str] = Field(
        default_factory=list,
        description="Lista najwaÅ¼niejszych encji z pytania uÅ¼ytkownika.",
    )
    cypher_query: str = Field(
        description="Zapytanie Cypher, ktÃ³re ma zostaÄ‡ wykonane na grafie wiedzy.",
    )


class RAGDocumentService:
    """Serwis zarzÄ…dzajÄ…cy dokumentami, grafem wiedzy i zapytaniami Graph RAG.

    Zakres odpowiedzialnoÅ›ci:

    1. Wczytywanie dokumentÃ³w PDF/DOCX i dzielenie ich na fragmenty.
    2. Generowanie embeddingÃ³w i zapis chunkÃ³w w indeksie wektorowym Neo4j.
    3. Budowa uniwersalnego grafu wiedzy na bazie `LLMGraphTransformer`.
    4. Odpowiadanie na pytania uÅ¼ytkownikÃ³w z wykorzystaniem Graph RAG
       (poÅ‚Ä…czenie kontekstu grafowego i semantycznego).
    5. ZarzÄ…dzanie dokumentami w bazie PostgreSQL (lista, usuwanie z czyszczeniem
       danych w Neo4j).
    """

    def __init__(self) -> None:
        """Inicjalizuje wszystkie niezbÄ™dne komponenty LangChain i Neo4j."""

        self.settings = settings

        # Model konwersacyjny wykorzystywany zarÃ³wno do budowy grafu, jak i
        # generowania finalnych odpowiedzi Graph RAG.
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=self.settings.GOOGLE_API_KEY,
            temperature=0,
        )

        # Embeddingi Google Gemini wykorzystywane przez indeks wektorowy Neo4j.
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=settings.EMBEDDING_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
        )

        # Inicjalizacja Neo4j Vector Store â€“ krytyczna dla dziaÅ‚ania RAG.
        try:
            self.vector_store = Neo4jVector(
                url=settings.NEO4J_URI,
                username=settings.NEO4J_USER,
                password=settings.NEO4J_PASSWORD,
                embedding=self.embeddings,
                index_name="rag_document_embeddings",
                node_label="RAGChunk",
                text_node_property="text",
                embedding_node_property="embedding",
            )
            logger.info("Neo4j Vector Store zostaÅ‚ poprawnie zainicjalizowany.")
        except Exception as exc:  # pragma: no cover - logujemy problem konfiguracyjny
            logger.error("Nie udaÅ‚o siÄ™ zainicjalizowaÄ‡ Neo4j Vector Store: %s", exc)
            self.vector_store = None

        # Inicjalizacja Neo4j Graph â€“ moÅ¼e siÄ™ nie udaÄ‡, ale wtedy Graph RAG
        # zostanie tymczasowo wyÅ‚Ä…czony (pozostanie klasyczne RAG).
        try:
            self.graph_store = Neo4jGraph(
                url=settings.NEO4J_URI,
                username=settings.NEO4J_USER,
                password=settings.NEO4J_PASSWORD,
            )
            logger.info("Neo4j Graph Store zostaÅ‚ poprawnie zainicjalizowany.")
        except Exception as exc:  # pragma: no cover - logujemy problem konfiguracyjny
            logger.error("Nie udaÅ‚o siÄ™ zainicjalizowaÄ‡ Neo4j Graph Store: %s", exc)
            self.graph_store = None

    async def ingest_document(self, file_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Przetwarza dokument przez peÅ‚ny pipeline: load â†’ chunk â†’ graph â†’ vector.

        Args:
            file_path: ÅšcieÅ¼ka do pliku PDF lub DOCX zapisanej kopii dokumentu.
            metadata: Metadane dokumentu (doc_id, title, country, itp.).

        Returns:
            SÅ‚ownik zawierajÄ…cy liczbÄ™ chunkÃ³w oraz status zakoÅ„czenia procesu.

        Raises:
            RuntimeError: Gdy brakuje poÅ‚Ä…czenia z Neo4j (vector store jest kluczowy).
            FileNotFoundError: JeÅ›li plik nie istnieje.
            ValueError: Przy nieobsÅ‚ugiwanym rozszerzeniu lub braku treÅ›ci.
        """

        if not self.vector_store:
            raise RuntimeError("Brak poÅ‚Ä…czenia z Neo4j Vector Store â€“ ingest niemoÅ¼liwy.")

        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"Nie znaleziono pliku: {file_path}")

        logger.info("Rozpoczynam przetwarzanie dokumentu: %s", path.name)

        try:
            # 1. LOAD â€“ wybÃ³r loadera zaleÅ¼nie od rozszerzenia pliku.
            file_extension = path.suffix.lower()
            if file_extension == ".pdf":
                loader = PyPDFLoader(str(path))
                logger.info("UÅ¼ywam PyPDFLoader dla pliku %s", path.name)
            elif file_extension == ".docx":
                loader = Docx2txtLoader(str(path))
                logger.info("UÅ¼ywam Docx2txtLoader dla pliku %s", path.name)
            else:
                raise ValueError(
                    f"NieobsÅ‚ugiwany typ pliku: {file_extension}. Dozwolone: PDF, DOCX."
                )

            documents = await asyncio.to_thread(loader.load)
            if not documents:
                raise ValueError("Nie udaÅ‚o siÄ™ odczytaÄ‡ zawartoÅ›ci dokumentu.")
            logger.info("Wczytano %s segmentÃ³w dokumentu ÅºrÃ³dÅ‚owego.", len(documents))

            # 2. SPLIT â€“ dzielenie tekstu na fragmenty z kontrolowanym overlapem.
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.RAG_CHUNK_SIZE,
                chunk_overlap=settings.RAG_CHUNK_OVERLAP,
                separators=["\n\n", "\n", ". ", " ", ""],
                length_function=len,
            )
            chunks = text_splitter.split_documents(documents)
            if not chunks:
                raise ValueError("Nie wygenerowano Å¼adnych fragmentÃ³w tekstu.")
            logger.info(
                "Podzielono dokument na %s fragmentÃ³w (chunk_size=%s, overlap=%s)",
                len(chunks),
                settings.RAG_CHUNK_SIZE,
                settings.RAG_CHUNK_OVERLAP,
            )

            # 3. METADATA â€“ wzbogacenie kaÅ¼dego chunku o metadane identyfikujÄ…ce.
            doc_id = metadata.get("doc_id")
            for index, chunk in enumerate(chunks):
                chunk.metadata.update(
                    {
                        "doc_id": str(doc_id),
                        "chunk_index": index,
                        "title": metadata.get("title", "Nieznany dokument"),
                        "country": metadata.get("country", "Poland"),
                        "source_file": path.name,
                    }
                )

            # 4. GRAPH â€“ prÃ³bujemy zbudowaÄ‡ graf wiedzy, jeÅ›li Neo4j Graph jest dostÄ™pny.
            if self.graph_store:
                try:
                    logger.info("GenerujÄ™ strukturÄ™ grafowÄ… na podstawie uniwersalnego modelu.")
                    transformer = LLMGraphTransformer(
                        llm=self.llm,
                        allowed_nodes=[
                            "Observation",
                            "Indicator",
                            "Demographic",
                            "Trend",
                            "Location",
                            "Cause",
                            "Effect",
                        ],
                        allowed_relationships=[
                            "DESCRIBES",
                            "APPLIES_TO",
                            "SHOWS_TREND",
                            "LOCATED_IN",
                            "CAUSED_BY",
                            "LEADS_TO",
                            "COMPARES_TO",
                        ],
                        node_properties=[
                            "description",      # SzczegÃ³Å‚owy opis wÄ™zÅ‚a (2-3 zdania)
                            "summary",          # KrÃ³tkie podsumowanie (1 zdanie)
                            "key_facts",        # Lista kluczowych faktÃ³w (string, separated by semicolons)
                            "time_period",      # Okres czasu (jeÅ›li dotyczy, format: "YYYY" lub "YYYY-YYYY")
                            "magnitude",        # WielkoÅ›Ä‡/skala dla wskaÅºnikÃ³w (string z jednostkÄ…)
                            "source_context",   # BezpoÅ›redni cytat lub kontekst ÅºrÃ³dÅ‚owy
                            "confidence_level"  # Poziom pewnoÅ›ci informacji: "high", "medium", "low"
                        ],
                        relationship_properties=[
                            "confidence",       # PewnoÅ›Ä‡ relacji (0.0-1.0 jako string)
                            "evidence",         # DowÃ³d/uzasadnienie relacji (cytat lub wyjaÅ›nienie)
                            "strength"          # SiÅ‚a relacji: "strong", "moderate", "weak"
                        ],
                        additional_instructions="""
JÄ˜ZYK: Wszystkie wÅ‚aÅ›ciwoÅ›ci wÄ™zÅ‚Ã³w i relacji (streszczenie, opis, kluczowe_fakty, zrodlo, dowÃ³d) MUSZÄ„ byÄ‡ po polsku.

WÄ˜ZÅY - KaÅ¼dy wÄ™zeÅ‚ zawiera:
- opis: WyczerpujÄ…cy opis kontekstu (2-3 zdania)
- streszczenie: Jednozdaniowe streszczenie
- kluczowe_fakty: Lista faktÃ³w oddzielonych Å›rednikami (min. 2-3)
- okres_czasu: Okres czasu (YYYY lub YYYY-YYYY)
- skala: WartoÅ›Ä‡ z jednostkÄ… (np. "67%", "1.2 mln osÃ³b")
- zrodlo: Cytat ze ÅºrÃ³dÅ‚a (20-50 sÅ‚Ã³w)
- pewnosc: "wysoka" (dane bezpoÅ›rednie), "srednia" (wnioski), "niska" (spekulacje)

RELACJE - KaÅ¼da relacja zawiera:
- pewnoÅ›Ä‡: PewnoÅ›Ä‡ 0.0-1.0 (string)
- dowÃ³d: DowÃ³d z tekstu uzasadniajÄ…cy relacjÄ™
- siÅ‚a: "silna" (bezpoÅ›rednia), "umiarkowana" (prawdopodobna), "sÅ‚aba" (moÅ¼liwa)

TYPY WÄ˜ZÅÃ“W (precyzyjnie):
Observation (obserwacje), Indicator (wskaÅºniki liczbowe), Demographic (grupy), Trend (trendy czasowe), Location (miejsca), Cause (przyczyny), Effect (skutki)

METADANE TECHNICZNE (KRYTYCZNE):
Zachowaj doc_id i chunk_index w kaÅ¼dym wÄ™Åºle dla pÃ³Åºniejszego usuwania.
                        """.strip(),
                    )
                    graph_documents = await transformer.aconvert_to_graph_documents(chunks)

                    # Wzbogacenie wÄ™zÅ‚Ã³w o metadane dokumentu
                    enriched_graph_documents = self._enrich_graph_nodes(
                        graph_documents,
                        doc_id=str(doc_id),
                        metadata=metadata
                    )

                    self.graph_store.add_graph_documents(enriched_graph_documents, include_source=True)
                    logger.info("Zapisano strukturÄ™ grafowÄ… dla dokumentu %s", doc_id)
                except Exception as graph_exc:  # pragma: no cover - logujemy, ale nie przerywamy
                    logger.error(
                        "Nie udaÅ‚o siÄ™ wygenerowaÄ‡ grafu wiedzy dla dokumentu %s: %s",
                        doc_id,
                        graph_exc,
                        exc_info=True,
                    )

            else:
                logger.warning(
                    "Neo4j Graph Store nie jest dostÄ™pny â€“ dokument zostanie przetworzony "
                    "bez struktury grafowej."
                )

            # 5. VECTOR â€“ zapis chunkÃ³w do indeksu wektorowego w Neo4j.
            logger.info("GenerujÄ™ embeddingi i zapisujÄ™ je w indeksie wektorowym...")
            await self.vector_store.aadd_documents(chunks)
            logger.info(
                "ZakoÅ„czono przetwarzanie %s fragmentÃ³w dokumentu %s",
                len(chunks),
                doc_id,
            )

            return {"num_chunks": len(chunks), "status": "ready"}

        except Exception as exc:  # pragma: no cover - logujemy peÅ‚nÄ… diagnostykÄ™
            logger.error(
                "BÅ‚Ä…d podczas przetwarzania dokumentu %s: %s",
                file_path,
                exc,
                exc_info=True,
            )
            return {"num_chunks": 0, "status": "failed", "error": str(exc)}

    def _enrich_graph_nodes(
        self,
        graph_documents: List[Any],
        doc_id: str,
        metadata: Dict[str, Any]
    ) -> List[Any]:
        """Wzbogaca wÄ™zÅ‚y grafu o metadane dokumentu i waliduje jakoÅ›Ä‡ danych.

        Args:
            graph_documents: Lista GraphDocument z LLMGraphTransformer
            doc_id: UUID dokumentu ÅºrÃ³dÅ‚owego
            metadata: Metadane dokumentu (title, country, itp.)

        Returns:
            Lista wzbogaconych GraphDocument z peÅ‚nymi metadanymi

        Proces wzbogacania:
            1. Dodaje doc_id, chunk_index do kaÅ¼dego wÄ™zÅ‚a
            2. Dodaje metadane dokumentu: title, country, date
            3. Waliduje jakoÅ›Ä‡ metadanych wÄ™zÅ‚Ã³w (sprawdza czy summary i description nie sÄ… puste)
            4. Dodaje timestamp przetwarzania
            5. Normalizuje formaty danych (confidence, magnitude)
        """
        from datetime import datetime, timezone

        logger.info(
            "Wzbogacam %s dokumentÃ³w grafu o metadane doc_id=%s",
            len(graph_documents),
            doc_id
        )

        enriched_count = 0
        validation_warnings = 0

        for graph_doc in graph_documents:
            # Pobierz chunk_index z source document jeÅ›li dostÄ™pny
            chunk_index = graph_doc.source.metadata.get("chunk_index", 0) if graph_doc.source else 0

            # Wzbogacenie wÄ™zÅ‚Ã³w
            for node in graph_doc.nodes:
                # 1. METADANE TECHNICZNE (krytyczne dla usuwania dokumentÃ³w)
                if not hasattr(node, 'properties'):
                    node.properties = {}

                node.properties['doc_id'] = doc_id
                node.properties['chunk_index'] = chunk_index
                node.properties['processed_at'] = datetime.now(timezone.utc).isoformat()

                # 2. METADANE DOKUMENTU (kontekst ÅºrÃ³dÅ‚owy)
                node.properties['document_title'] = metadata.get('title', 'Unknown')
                node.properties['document_country'] = metadata.get('country', 'Poland')
                if metadata.get('date'):
                    node.properties['document_year'] = str(metadata.get('date'))

                # 3. WALIDACJA JAKOÅšCI METADANYCH
                # SprawdÅº czy kluczowe wÅ‚aÅ›ciwoÅ›ci sÄ… wypeÅ‚nione
                if node.properties.get('summary') in (None, '', 'N/A'):
                    validation_warnings += 1
                    logger.debug(
                        "WÄ™zeÅ‚ '%s' nie ma summary - LLM moÅ¼e nie wyekstraktowaÄ‡ wszystkich wÅ‚aÅ›ciwoÅ›ci",
                        node.id
                    )

                if node.properties.get('description') in (None, '', 'N/A'):
                    validation_warnings += 1

                # 4. NORMALIZACJA FORMATÃ“W
                # Confidence level normalizacja
                confidence = node.properties.get('confidence_level', '').lower()
                if confidence not in ('high', 'medium', 'low'):
                    node.properties['confidence_level'] = 'medium'  # default

                # Magnitude - upewnij siÄ™ Å¼e jest stringiem
                if node.properties.get('magnitude') and not isinstance(node.properties['magnitude'], str):
                    node.properties['magnitude'] = str(node.properties['magnitude'])

                enriched_count += 1

            # Wzbogacenie relacji
            for relationship in graph_doc.relationships:
                if not hasattr(relationship, 'properties'):
                    relationship.properties = {}

                # Metadane techniczne
                relationship.properties['doc_id'] = doc_id
                relationship.properties['chunk_index'] = chunk_index

                # Normalizacja confidence (string -> float validation)
                if relationship.properties.get('confidence'):
                    try:
                        conf_value = float(relationship.properties['confidence'])
                        # Clamp do 0.0-1.0
                        relationship.properties['confidence'] = str(max(0.0, min(1.0, conf_value)))
                    except (ValueError, TypeError):
                        relationship.properties['confidence'] = '0.5'  # default

                # Normalizacja strength
                strength = relationship.properties.get('strength', '').lower()
                if strength not in ('strong', 'moderate', 'weak'):
                    relationship.properties['strength'] = 'moderate'  # default

        logger.info(
            "Wzbogacono %s wÄ™zÅ‚Ã³w. OstrzeÅ¼enia walidacji: %s",
            enriched_count,
            validation_warnings
        )

        if validation_warnings > enriched_count * 0.3:  # >30% wÄ™zÅ‚Ã³w bez summary
            logger.warning(
                "Wysoki odsetek wÄ™zÅ‚Ã³w (%s%%) bez peÅ‚nych metadanych. "
                "LLM moÅ¼e potrzebowaÄ‡ lepszych instrukcji lub wiÄ™kszego kontekstu.",
                int(validation_warnings / enriched_count * 100)
            )

        return graph_documents

    def _generate_cypher_query(self, question: str) -> GraphRAGQuery:
        """UÅ¼ywa LLM do przeÅ‚oÅ¼enia pytania uÅ¼ytkownika na zapytanie Cypher."""

        if not self.graph_store:
            raise RuntimeError("Graph RAG nie jest dostÄ™pny â€“ brak poÅ‚Ä…czenia z Neo4j Graph.")

        graph_schema = self.graph_store.get_schema()
        cypher_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    JesteÅ› analitykiem badaÅ„ spoÅ‚ecznych. Twoim zadaniem jest zamiana pytania
                    uÅ¼ytkownika na zapytanie Cypher korzystajÄ…ce z poniÅ¼szego schematu grafu.

                    DOSTÄ˜PNE WÅAÅšCIWOÅšCI WÄ˜ZÅÃ“W:
                    - description: SzczegÃ³Å‚owy opis kontekstu (2-3 zdania)
                    - summary: Jednozdaniowe podsumowanie
                    - key_facts: Lista kluczowych faktÃ³w (oddzielone Å›rednikami)
                    - time_period: Okres czasu (YYYY lub YYYY-YYYY)
                    - magnitude: WielkoÅ›Ä‡/skala z jednostkÄ…
                    - source_context: Cytat ze ÅºrÃ³dÅ‚a
                    - confidence_level: PewnoÅ›Ä‡ danych (high/medium/low)
                    - document_title: TytuÅ‚ dokumentu ÅºrÃ³dÅ‚owego
                    - document_country: Kraj dokumentu
                    - document_year: Rok dokumentu

                    DOSTÄ˜PNE WÅAÅšCIWOÅšCI RELACJI:
                    - confidence: PewnoÅ›Ä‡ relacji (0.0-1.0)
                    - evidence: DowÃ³d/uzasadnienie relacji
                    - strength: SiÅ‚a relacji (strong/moderate/weak)

                    INSTRUKCJE TWORZENIA ZAPYTAÅƒ:
                    1. Skup siÄ™ na odnajdywaniu Å›cieÅ¼ek do gÅ‚Ä™bokoÅ›ci 3 miÄ™dzy encjami
                       (Observation, Indicator, Demographic, Trend, Location, Cause, Effect)
                    2. ZAWSZE zwracaj wÅ‚aÅ›ciwoÅ›ci wÄ™zÅ‚Ã³w (description, summary, key_facts, itp.)
                    3. Filtruj po confidence_level wÄ™zÅ‚Ã³w jeÅ›li uÅ¼ytkownik pyta o pewne fakty
                    4. Filtruj po strength relacji jeÅ›li uÅ¼ytkownik pyta o silne zaleÅ¼noÅ›ci
                    5. Sortuj po magnitude dla pytaÅ„ o najwiÄ™ksze/najmniejsze wskaÅºniki
                    6. Filtruj po time_period dla pytaÅ„ czasowych
                    7. UÅ¼ywaj source_context w odpowiedziach dla weryfikowalnoÅ›ci

                    PRZYKÅADY ZAPYTAÅƒ:
                    - "Jakie sÄ… najwiÄ™ksze wskaÅºniki?" -> sortuj po magnitude
                    - "Jakie sÄ… pewne fakty o X?" -> filtruj confidence_level = 'high'
                    - "Jak X wpÅ‚ywa na Y?" -> filtruj strength = 'strong' w relacjach LEADS_TO
                    - "Co siÄ™ zmieniÅ‚o w latach 2020-2023?" -> filtruj time_period

                    Schemat grafu: {graph_schema}
                    """.strip(),
                ),
                ("human", "Pytanie uÅ¼ytkownika: {question}"),
            ]
        )

        chain = cypher_prompt | self.llm.with_structured_output(GraphRAGQuery)
        return chain.invoke({"question": question, "graph_schema": graph_schema})

    async def answer_question(self, question: str) -> Dict[str, Any]:
        """Realizuje peÅ‚en przepÅ‚yw Graph RAG i zwraca ustrukturyzowanÄ… odpowiedÅº."""

        if not self.graph_store or not self.vector_store:
            raise ConnectionError(
                "Graph RAG wymaga jednoczesnego dostÄ™pu do grafu i indeksu wektorowego."
            )

        logger.info("GenerujÄ™ zapytanie Cypher dla pytania: %s", question)
        rag_query = self._generate_cypher_query(question)
        logger.info("Wygenerowane zapytanie Cypher: %s", rag_query.cypher_query)

        # 1. Kontekst grafowy â€“ zapytanie Cypher wygenerowane przez LLM.
        try:
            graph_context = self.graph_store.query(rag_query.cypher_query)
        except Exception as exc:  # pragma: no cover - logujemy i kontynuujemy
            logger.error("BÅ‚Ä…d wykonania zapytania Cypher: %s", exc, exc_info=True)
            graph_context = []

        # 2. Kontekst wektorowy â€“ semantyczne wyszukiwanie po encjach.
        vector_context_docs: List[Document] = []
        if rag_query.entities:
            search_query = " ".join(rag_query.entities)
            vector_context_docs = await self.vector_store.asimilarity_search(search_query, k=5)

        # 3. Agregacja kontekstu i wygenerowanie odpowiedzi koÅ„cowej.
        final_context = "KONTEKST Z GRAFU WIEDZY:\n" + str(graph_context)
        final_context += "\n\nFRAGMENTY Z DOKUMENTÃ“W:\n"
        for doc in vector_context_docs:
            final_context += f"- {doc.page_content}\n"

        answer_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    JesteÅ› ekspertem od analiz spoÅ‚ecznych. Odpowiadasz wyÅ‚Ä…cznie na
                    podstawie dostarczonego kontekstu z grafu i dokumentÃ³w. Udzielaj
                    precyzyjnych, zweryfikowalnych odpowiedzi po polsku.
                    """.strip(),
                ),
                ("human", "Pytanie: {question}\n\nKontekst:\n{context}"),
            ]
        )

        response = await (answer_prompt | self.llm).ainvoke(
            {"question": question, "context": final_context}
        )

        return {
            "answer": response.content,
            "graph_context": graph_context,
            "vector_context": [doc.to_json() for doc in vector_context_docs],
            "cypher_query": rag_query.cypher_query,
        }

    async def list_documents(self, db: AsyncSession) -> List[RAGDocument]:
        """Zwraca listÄ™ aktywnych dokumentÃ³w posortowanych malejÄ…co po dacie."""

        result = await db.execute(
            select(RAGDocument)
            .where(RAGDocument.is_active.is_(True))
            .order_by(RAGDocument.created_at.desc())
        )
        return result.scalars().all()

    async def delete_document(self, doc_id: UUID, db: AsyncSession) -> None:
        """Usuwa dokument z PostgreSQL i czyÅ›ci powiÄ…zane dane w Neo4j."""

        doc = await db.get(RAGDocument, doc_id)
        if not doc:
            raise ValueError(f"Document {doc_id} not found")

        doc.is_active = False
        await db.commit()

        await self._delete_chunks_from_neo4j(str(doc_id))

        if self.graph_store:
            try:
                self.graph_store.query(
                    "MATCH (n {doc_id: $doc_id}) DETACH DELETE n",
                    params={"doc_id": str(doc_id)},
                )
                logger.info("UsuniÄ™to wÄ™zÅ‚y grafu dla dokumentu %s", doc_id)
            except Exception as exc:  # pragma: no cover - logujemy, ale nie przerywamy
                logger.error(
                    "Nie udaÅ‚o siÄ™ usunÄ…Ä‡ wÄ™zÅ‚Ã³w grafu dokumentu %s: %s",
                    doc_id,
                    exc,
                )

    async def _delete_chunks_from_neo4j(self, doc_id: str) -> None:
        """CzyÅ›ci wszystkie chunki dokumentu z indeksu Neo4j Vector."""

        if not self.vector_store:
            return

        try:
            driver = self.vector_store._driver  # DostÄ™p wewnÄ™trzny â€“ akceptowalny w serwisie.

            def delete_chunks() -> None:
                with driver.session() as session:
                    session.execute_write(
                        lambda tx: tx.run(
                            "MATCH (n:RAGChunk {doc_id: $doc_id}) DETACH DELETE n",
                            doc_id=doc_id,
                        )
                    )

            await asyncio.to_thread(delete_chunks)
            logger.info("UsuniÄ™to wektorowe chunki dokumentu %s", doc_id)
        except Exception as exc:  # pragma: no cover - logujemy, ale nie przerywamy
            logger.error("Nie udaÅ‚o siÄ™ usunÄ…Ä‡ chunkÃ³w dokumentu %s z Neo4j: %s", doc_id, exc)

    def get_demographic_graph_context(
        self,
        age_group: str,
        location: str,
        education: str,
        gender: str
    ) -> List[Dict[str, Any]]:
        """Pobiera strukturalny kontekst z grafu wiedzy dla profilu demograficznego.

        Wykonuje zapytania Cypher na grafie aby znaleÅºÄ‡:
        1. Indicators (wskaÅºniki) - z magnitude, confidence_level
        2. Observations (obserwacje) - z key_facts
        3. Trends (trendy) - z time_period
        4. Demographics nodes powiÄ…zane z profilem

        Args:
            age_group: Grupa wiekowa (np. "25-34")
            location: Lokalizacja (np. "Warszawa")
            education: Poziom wyksztaÅ‚cenia (np. "wyÅ¼sze")
            gender: PÅ‚eÄ‡ (np. "kobieta")

        Returns:
            Lista sÅ‚ownikÃ³w z wÄ™zÅ‚ami grafu i ich wÅ‚aÅ›ciwoÅ›ciami:
            [
                {
                    "type": "Indicator" | "Observation" | "Trend" | "Demographic",
                    "summary": str,
                    "key_facts": str,
                    "magnitude": str,
                    "confidence_level": str,
                    "time_period": str,
                    "source_context": str
                },
                ...
            ]
        """
        if not self.graph_store:
            logger.warning("Graph store nie jest dostÄ™pny - zwracam pusty kontekst grafowy")
            return []

        # Mapowanie PL â†’ EN dla dual-language search
        # Problem: Graph nodes mogÄ… mieÄ‡ wÅ‚aÅ›ciwoÅ›ci w jÄ™zyku angielskim (legacy data)
        # RozwiÄ…zanie: Dodajemy angielskie odpowiedniki polskich terminÃ³w
        PL_TO_EN_SEARCH_TERMS = {
            "wyksztaÅ‚cenie wyÅ¼sze": "higher education",
            "studia": "university",
            "uniwersytet": "university",
            "wyksztaÅ‚cenie Å›rednie": "secondary education",
            "liceum": "high school",
            "zasadnicze zawodowe": "vocational",
            "demografia": "demographic",
            "spoÅ‚eczeÅ„stwo polskie": "Polish society",
            "populacja": "population",
            "kobieta": "female",
            "mÄ™Å¼czyzna": "male",
            "zatrudnienie": "employment",
            "praca": "work",
            "dochÃ³d": "income",
            "zarobki": "earnings",
            "mieszkanie": "housing",
            "edukacja": "education",
            "zdrowie": "health",
            "kultura": "culture",
            "wskaÅºnik": "indicator",
            "statystyka": "statistic",
            "dane": "data",
            "badanie": "research",
            "raport": "report",
            "analiza": "analysis",
        }

        # Budujemy search terms dla zapytania
        # Normalizuj wartoÅ›ci dla lepszego matchingu
        search_terms = []

        # Wiek - ekstrakcja liczb z zakresu
        if "-" in age_group:
            age_parts = age_group.split("-")
            search_terms.extend(age_parts)
        search_terms.append(age_group)

        # Lokalizacja
        search_terms.append(location)

        # WyksztaÅ‚cenie - normalizuj + dual-language
        if "wyÅ¼sze" in education.lower():
            search_terms.extend(["wyksztaÅ‚cenie wyÅ¼sze", "studia", "uniwersytet"])
        elif "Å›rednie" in education.lower():
            search_terms.extend(["wyksztaÅ‚cenie Å›rednie", "liceum"])
        elif "zawodowe" in education.lower():
            search_terms.extend(["zasadnicze zawodowe", "zawodowe"])
        else:
            search_terms.append(education)

        # PÅ‚eÄ‡
        search_terms.append(gender)

        # Dodaj ogÃ³lne terminy demograficzne
        search_terms.extend(["demografia", "spoÅ‚eczeÅ„stwo polskie", "populacja"])

        # DUAL-LANGUAGE: Dodaj angielskie odpowiedniki dla polskich terminÃ³w
        # Tworzy kopiÄ™ aby nie modyfikowaÄ‡ listy podczas iteracji
        search_terms_copy = search_terms.copy()
        for pl_term in search_terms_copy:
            pl_normalized = pl_term.lower().strip()
            if pl_normalized in PL_TO_EN_SEARCH_TERMS:
                en_term = PL_TO_EN_SEARCH_TERMS[pl_normalized]
                if en_term not in search_terms:  # Unikaj duplikatÃ³w
                    search_terms.append(en_term)

        logger.debug(
            "Graph context search terms (PL+EN): %s",
            search_terms[:10]  # Log tylko pierwsze 10 aby nie zaÅ›miecaÄ‡ logÃ³w
        )

        logger.info(
            "Pobieranie graph context dla: wiek=%s, lokalizacja=%s, wyksztaÅ‚cenie=%s, pÅ‚eÄ‡=%s",
            age_group, location, education, gender
        )

        try:
            # Zapytanie Cypher: ZnajdÅº wÄ™zÅ‚y ktÃ³re pasujÄ… do search terms
            # confidence_level jest opcjonalny - preferujemy 'high' ale akceptujemy wszystkie
            cypher_query = """
            // Parametry: $search_terms - lista sÅ‚Ã³w kluczowych do matchingu

            // 1. ZnajdÅº Indicators (preferuj high confidence jeÅ›li istnieje)
            MATCH (ind:Indicator)
            WHERE ANY(term IN $search_terms WHERE
                ind.summary CONTAINS term OR
                ind.description CONTAINS term OR
                ind.key_facts CONTAINS term
            )
            WITH ind
            ORDER BY
                CASE WHEN ind.confidence_level = 'high' THEN 0
                     WHEN ind.confidence_level = 'medium' THEN 1
                     ELSE 2 END,
                size(coalesce(ind.key_facts, '')) DESC
            LIMIT 3
            WITH collect({
                type: 'Indicator',
                summary: ind.summary,
                key_facts: ind.key_facts,
                magnitude: ind.magnitude,
                confidence_level: coalesce(ind.confidence_level, 'unknown'),
                time_period: ind.time_period,
                source_context: ind.source_context,
                description: ind.description
            }) AS indicators

            // 2. ZnajdÅº Observations (preferuj high confidence jeÅ›li istnieje)
            MATCH (obs:Observation)
            WHERE ANY(term IN $search_terms WHERE
                obs.summary CONTAINS term OR
                obs.description CONTAINS term OR
                obs.key_facts CONTAINS term
            )
            WITH indicators, obs
            ORDER BY
                CASE WHEN obs.confidence_level = 'high' THEN 0
                     WHEN obs.confidence_level = 'medium' THEN 1
                     ELSE 2 END,
                size(coalesce(obs.key_facts, '')) DESC
            LIMIT 3
            WITH indicators, collect({
                type: 'Observation',
                summary: obs.summary,
                key_facts: obs.key_facts,
                confidence_level: coalesce(obs.confidence_level, 'unknown'),
                time_period: obs.time_period,
                source_context: obs.source_context,
                description: obs.description
            }) AS observations

            // 3. ZnajdÅº Trends
            MATCH (trend:Trend)
            WHERE ANY(term IN $search_terms WHERE
                trend.summary CONTAINS term OR
                trend.description CONTAINS term
            )
            WITH indicators, observations, trend
            ORDER BY size(coalesce(trend.key_facts, '')) DESC
            LIMIT 2
            WITH indicators, observations, collect({
                type: 'Trend',
                summary: trend.summary,
                key_facts: trend.key_facts,
                time_period: trend.time_period,
                source_context: trend.source_context,
                description: trend.description
            }) AS trends

            // 4. ZnajdÅº Demographics nodes
            MATCH (demo:Demographic)
            WHERE ANY(term IN $search_terms WHERE
                demo.summary CONTAINS term OR
                demo.description CONTAINS term
            )
            WITH indicators, observations, trends, demo
            ORDER BY
                CASE WHEN demo.confidence_level = 'high' THEN 0
                     WHEN demo.confidence_level = 'medium' THEN 1
                     ELSE 2 END
            LIMIT 2
            WITH indicators, observations, trends, collect({
                type: 'Demographic',
                summary: demo.summary,
                key_facts: demo.key_facts,
                confidence_level: coalesce(demo.confidence_level, 'unknown'),
                source_context: demo.source_context,
                description: demo.description
            }) AS demographics

            // 5. PoÅ‚Ä…cz wszystkie wyniki
            RETURN indicators + observations + trends + demographics AS graph_context
            """

            result = self.graph_store.query(
                cypher_query,
                params={"search_terms": search_terms}
            )

            if not result or not result[0].get('graph_context'):
                logger.info("Brak wynikÃ³w z grafu dla podanego profilu demograficznego")
                return []

            graph_context = result[0]['graph_context']
            logger.info("Pobrano %s wÄ™zÅ‚Ã³w grafu z kontekstem demograficznym", len(graph_context))

            return graph_context

        except Exception as exc:
            logger.error(
                "BÅ‚Ä…d podczas pobierania graph context: %s",
                exc,
                exc_info=True
            )
            return []


class PolishSocietyRAG:
    """Hybrydowe wyszukiwanie kontekstu dla generatora person.

    ÅÄ…czy wyszukiwanie semantyczne (embeddingi) oraz peÅ‚notekstowe (fulltext index)
    korzystajÄ…c z techniki Reciprocal Rank Fusion. Klasa jest niezaleÅ¼na od
    Graph RAG, ale wspÃ³Å‚dzieli te same ustawienia i konwencjÄ™ metadanych.
    """

    def __init__(self) -> None:
        """Przygotowuje wektorowe i keywordowe zaplecze wyszukiwawcze."""

        self.settings = settings

        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=settings.EMBEDDING_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
        )

        try:
            self.vector_store = Neo4jVector(
                url=settings.NEO4J_URI,
                username=settings.NEO4J_USER,
                password=settings.NEO4J_PASSWORD,
                embedding=self.embeddings,
                index_name="rag_document_embeddings",
                node_label="RAGChunk",
                text_node_property="text",
                embedding_node_property="embedding",
            )
            logger.info("PolishSocietyRAG zostaÅ‚ poprawnie zainicjalizowany.")

            if settings.RAG_USE_HYBRID_SEARCH:
                asyncio.create_task(self._ensure_fulltext_index())

            # Inicjalizuj RAGDocumentService dla dostÄ™pu do graph context
            # UÅ¼ywamy leniwej inicjalizacji aby uniknÄ…Ä‡ circular dependency
            self._rag_doc_service = None

            # Inicjalizuj cross-encoder dla reranking (opcjonalny)
            self.reranker = None
            if settings.RAG_USE_RERANKING:
                try:
                    from sentence_transformers import CrossEncoder
                    self.reranker = CrossEncoder(
                        settings.RAG_RERANKER_MODEL,
                        max_length=512
                    )
                    logger.info(
                        "Cross-encoder reranker zainicjalizowany: %s",
                        settings.RAG_RERANKER_MODEL
                    )
                except ImportError:
                    logger.warning(
                        "sentence-transformers nie jest zainstalowany - reranking wyÅ‚Ä…czony. "
                        "Zainstaluj: pip install sentence-transformers"
                    )
                except Exception as rerank_exc:
                    logger.warning(
                        "Nie udaÅ‚o siÄ™ zaÅ‚adowaÄ‡ reranker: %s - kontynuacja bez rerankingu",
                        rerank_exc
                    )

        except Exception as exc:  # pragma: no cover - logujemy, ale nie przerywamy
            logger.error("Nie udaÅ‚o siÄ™ zainicjalizowaÄ‡ PolishSocietyRAG: %s", exc)
            self.vector_store = None

    @property
    def rag_doc_service(self) -> RAGDocumentService:
        """LeniwÄ… inicjalizacja RAGDocumentService dla dostÄ™pu do graph context."""
        if self._rag_doc_service is None:
            self._rag_doc_service = RAGDocumentService()
        return self._rag_doc_service

    async def _ensure_fulltext_index(self) -> None:
        """Tworzy indeks fulltext w Neo4j na potrzeby wyszukiwania keywordowego."""

        if not self.vector_store:
            return

        try:
            driver = self.vector_store._driver

            def check_and_create_index() -> None:
                with driver.session() as session:
                    def ensure(tx) -> None:
                        indexes = [record["name"] for record in tx.run("SHOW INDEXES")]
                        if "rag_fulltext_index" not in indexes:
                            tx.run(
                                """
                                CREATE FULLTEXT INDEX rag_fulltext_index IF NOT EXISTS
                                FOR (n:RAGChunk)
                                ON EACH [n.text]
                                """
                            )

                    session.execute_write(ensure)

            await asyncio.to_thread(check_and_create_index)
            logger.info("Fulltext index 'rag_fulltext_index' jest gotowy.")
        except Exception as exc:  # pragma: no cover - indeks nie jest krytyczny
            logger.warning("Nie udaÅ‚o siÄ™ utworzyÄ‡ indeksu fulltext: %s", exc)

    async def _keyword_search(self, query: str, k: int = 5) -> List[Document]:
        """Wykonuje wyszukiwanie peÅ‚notekstowe w Neo4j i zwraca dokumenty LangChain."""

        if not self.vector_store:
            return []

        try:
            driver = self.vector_store._driver

            def search() -> List[Document]:
                with driver.session() as session:
                    def run_query(tx):
                        result = tx.run(
                            """
                            CALL db.index.fulltext.queryNodes('rag_fulltext_index', $search_query)
                            YIELD node, score
                            RETURN node.text AS text,
                                   node.doc_id AS doc_id,
                                   node.title AS title,
                                   node.chunk_index AS chunk_index,
                                   score
                            ORDER BY score DESC
                            LIMIT $limit
                            """,
                            search_query=query,
                            limit=k,
                        )
                        documents: List[Document] = []
                        for record in result:
                            documents.append(
                                Document(
                                    page_content=record["text"],
                                    metadata={
                                        "doc_id": record["doc_id"],
                                        "title": record["title"],
                                        "chunk_index": record["chunk_index"],
                                        "keyword_score": record["score"],
                                    },
                                )
                            )
                        return documents

                    return session.execute_read(run_query)

            documents = await asyncio.to_thread(search)
            logger.info("Keyword search zwrÃ³ciÅ‚o %s wynikÃ³w", len(documents))
            return documents
        except Exception as exc:  # pragma: no cover - fallback do vector search
            logger.warning("Keyword search nie powiodÅ‚o siÄ™, uÅ¼ywam fallbacku: %s", exc)
            return []

    def _format_graph_context(self, graph_nodes: List[Dict[str, Any]]) -> str:
        """Formatuje wÄ™zÅ‚y grafu do czytelnego kontekstu tekstowego dla LLM.

        Args:
            graph_nodes: Lista wÄ™zÅ‚Ã³w z grafu z wÅ‚aÅ›ciwoÅ›ciami

        Returns:
            Sformatowany string z strukturalnÄ… wiedzÄ… z grafu
        """
        if not graph_nodes:
            return ""

        # Grupuj wÄ™zÅ‚y po typie
        indicators = [n for n in graph_nodes if n.get('type') == 'Indicator']
        observations = [n for n in graph_nodes if n.get('type') == 'Observation']
        trends = [n for n in graph_nodes if n.get('type') == 'Trend']
        demographics = [n for n in graph_nodes if n.get('type') == 'Demographic']

        sections = []

        # Sekcja Indicators
        if indicators:
            sections.append("ğŸ“Š WSKAÅ¹NIKI DEMOGRAFICZNE (Indicators):\n")
            for ind in indicators:
                summary = ind.get('summary', 'Brak podsumowania')
                magnitude = ind.get('magnitude', 'N/A')
                confidence = ind.get('confidence_level', 'N/A')
                key_facts = ind.get('key_facts', '')
                time_period = ind.get('time_period', '')

                sections.append(f"â€¢ {summary}")
                if magnitude and magnitude != 'N/A':
                    sections.append(f"  WielkoÅ›Ä‡: {magnitude}")
                if time_period:
                    sections.append(f"  Okres: {time_period}")
                sections.append(f"  PewnoÅ›Ä‡: {confidence}")
                if key_facts:
                    sections.append(f"  Kluczowe fakty: {key_facts}")
                sections.append("")

        # Sekcja Observations
        if observations:
            sections.append("\nğŸ‘¥ OBSERWACJE DEMOGRAFICZNE (Observations):\n")
            for obs in observations:
                summary = obs.get('summary', 'Brak podsumowania')
                confidence = obs.get('confidence_level', 'N/A')
                key_facts = obs.get('key_facts', '')
                time_period = obs.get('time_period', '')

                sections.append(f"â€¢ {summary}")
                sections.append(f"  PewnoÅ›Ä‡: {confidence}")
                if time_period:
                    sections.append(f"  Okres: {time_period}")
                if key_facts:
                    sections.append(f"  Kluczowe fakty: {key_facts}")
                sections.append("")

        # Sekcja Trends
        if trends:
            sections.append("\nğŸ“ˆ TRENDY DEMOGRAFICZNE (Trends):\n")
            for trend in trends:
                summary = trend.get('summary', 'Brak podsumowania')
                time_period = trend.get('time_period', 'N/A')
                key_facts = trend.get('key_facts', '')

                sections.append(f"â€¢ {summary}")
                sections.append(f"  Okres: {time_period}")
                if key_facts:
                    sections.append(f"  Kluczowe fakty: {key_facts}")
                sections.append("")

        # Sekcja Demographics
        if demographics:
            sections.append("\nğŸ¯ GRUPY DEMOGRAFICZNE (Demographics):\n")
            for demo in demographics:
                summary = demo.get('summary', 'Brak podsumowania')
                confidence = demo.get('confidence_level', 'N/A')
                key_facts = demo.get('key_facts', '')

                sections.append(f"â€¢ {summary}")
                sections.append(f"  PewnoÅ›Ä‡: {confidence}")
                if key_facts:
                    sections.append(f"  Kluczowe fakty: {key_facts}")
                sections.append("")

        return "\n".join(sections)

    def _rerank_with_cross_encoder(
        self,
        query: str,
        candidates: List[Tuple[Document, float]],
        top_k: int = 5
    ) -> List[Tuple[Document, float]]:
        """UÅ¼yj cross-encoder aby precyzyjnie re-score query-document pairs.

        Cross-encoder ma attention mechanism ktÃ³ry widzi query i document razem,
        co daje lepszÄ… precision niÅ¼ bi-encoder (uÅ¼ywany w vector search).

        Args:
            query: Query uÅ¼ytkownika
            candidates: Lista (Document, RRF_score) par z RRF fusion
            top_k: Liczba top wynikÃ³w do zwrÃ³cenia

        Returns:
            Lista (Document, rerank_score) sorted by rerank_score descending
        """
        if not self.reranker or not candidates:
            logger.info("Reranker niedostÄ™pny lub brak candidates - skip reranking")
            return candidates[:top_k]

        try:
            # Przygotuj pary (query, document) dla cross-encoder
            pairs = [(query, doc.page_content[:512]) for doc, _ in candidates]
            # Limit do 512 znakÃ³w dla cross-encoder (max_length)

            # Cross-encoder prediction (sync, ale szybkie ~100-200ms dla 20 docs)
            scores = self.reranker.predict(pairs)

            # PoÅ‚Ä…cz dokumenty z nowymi scores
            reranked = list(zip([doc for doc, _ in candidates], scores))

            # Sortuj po cross-encoder score (descending)
            reranked.sort(key=lambda x: x[1], reverse=True)

            logger.info(
                "Reranking completed: %s candidates â†’ top %s results",
                len(candidates),
                top_k
            )

            return reranked[:top_k]

        except Exception as exc:
            logger.error("Reranking failed: %s - fallback to RRF ranking", exc)
            return candidates[:top_k]

    def _find_related_graph_nodes(
        self,
        chunk_doc: Document,
        graph_nodes: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ZnajdÅº graph nodes ktÃ³re sÄ… powiÄ…zane z danym chunkiem.

        Matching bazuje na:
        1. WspÃ³lnych sÅ‚owach kluczowych (z summary/key_facts)
        2. Dokumencie ÅºrÃ³dÅ‚owym (doc_id)

        Args:
            chunk_doc: Document chunk z vector/keyword search
            graph_nodes: Lista graph nodes z get_demographic_graph_context()

        Returns:
            Lista graph nodes ktÃ³re sÄ… powiÄ…zane z chunkiem
        """
        if not graph_nodes:
            return []

        related = []
        chunk_text = chunk_doc.page_content.lower()
        chunk_doc_id = chunk_doc.metadata.get('doc_id', '')

        for node in graph_nodes:
            # SprawdÅº czy node pochodzi z tego samego dokumentu
            node_doc_id = node.get('doc_id', '')
            if node_doc_id and node_doc_id == chunk_doc_id:
                related.append(node)
                continue

            # SprawdÅº overlap sÅ‚Ã³w kluczowych
            summary = (node.get('summary', '') or '').lower()
            key_facts = (node.get('key_facts', '') or '').lower()

            # Ekstraktuj sÅ‚owa kluczowe (> 5 chars)
            summary_words = {w for w in summary.split() if len(w) > 5}
            key_facts_words = {w for w in key_facts.split() if len(w) > 5}
            node_keywords = summary_words | key_facts_words

            # Policz overlap
            matches = sum(1 for keyword in node_keywords if keyword in chunk_text)

            # JeÅ›li >=2 matching keywords, uznaj za related
            if matches >= 2:
                related.append(node)

        return related

    def _enrich_chunk_with_graph(
        self,
        chunk_text: str,
        related_nodes: List[Dict[str, Any]]
    ) -> str:
        """WzbogaÄ‡ chunk o powiÄ…zane graph nodes w naturalny sposÃ³b.

        Args:
            chunk_text: Oryginalny tekst chunku
            related_nodes: PowiÄ…zane graph nodes

        Returns:
            Enriched chunk text z embedded graph context
        """
        if not related_nodes:
            return chunk_text

        # Grupuj nodes po typie
        indicators = [n for n in related_nodes if n.get('type') == 'Indicator']
        observations = [n for n in related_nodes if n.get('type') == 'Observation']
        trends = [n for n in related_nodes if n.get('type') == 'Trend']

        enrichments = []

        # Dodaj wskaÅºniki
        if indicators:
            enrichments.append("\nğŸ’¡ PowiÄ…zane wskaÅºniki:")
            for ind in indicators[:2]:  # Max 2 na chunk
                summary = ind.get('summary', '')
                magnitude = ind.get('magnitude', '')
                if summary:
                    if magnitude:
                        enrichments.append(f"  â€¢ {summary} ({magnitude})")
                    else:
                        enrichments.append(f"  â€¢ {summary}")

        # Dodaj obserwacje
        if observations:
            enrichments.append("\nğŸ” PowiÄ…zane obserwacje:")
            for obs in observations[:2]:  # Max 2 na chunk
                summary = obs.get('summary', '')
                if summary:
                    enrichments.append(f"  â€¢ {summary}")

        # Dodaj trendy
        if trends:
            enrichments.append("\nğŸ“ˆ PowiÄ…zane trendy:")
            for trend in trends[:1]:  # Max 1 na chunk
                summary = trend.get('summary', '')
                time_period = trend.get('time_period', '')
                if summary:
                    if time_period:
                        enrichments.append(f"  â€¢ {summary} ({time_period})")
                    else:
                        enrichments.append(f"  â€¢ {summary}")

        if enrichments:
            return chunk_text + "\n" + "\n".join(enrichments)
        else:
            return chunk_text

    async def get_demographic_insights(
        self,
        age_group: str,
        education: str,
        location: str,
        gender: str,
    ) -> Dict[str, Any]:
        """Buduje kontekst raportowy dla wskazanego profilu demograficznego.

        ÅÄ…czy trzy ÅºrÃ³dÅ‚a kontekstu:
        1. **Graph RAG** - Strukturalna wiedza z grafu (Indicators, Observations, Trends)
        2. **Vector Search** - Semantyczne wyszukiwanie w embeddingach
        3. **Keyword Search** - Leksykalne wyszukiwanie fulltext (opcjonalnie)

        Returns:
            Dict z kluczami:
            - context: PeÅ‚ny kontekst (graph + chunks tekstowe)
            - graph_context: Sformatowany kontekst z grafu (string)
            - graph_nodes: Surowe wÄ™zÅ‚y grafu (list)
            - citations: Citations z hybrid search
            - query: Query uÅ¼yte do wyszukiwania
            - num_results: Liczba wynikÃ³w z hybrid search
            - search_type: "hybrid+graph" | "vector_only+graph" | "hybrid" | "vector_only"
        """

        if not self.vector_store:
            logger.warning("Vector store niedostÄ™pny â€“ zwracam pusty kontekst.")
            return {"context": "", "citations": [], "query": "", "num_results": 0}

        query = (
            f"Profil demograficzny: {gender}, wiek {age_group}, wyksztaÅ‚cenie {education}, "
            f"lokalizacja {location} w Polsce. Jakie sÄ… typowe cechy, wartoÅ›ci, zainteresowania, "
            f"style Å¼ycia oraz aspiracje dla tej grupy?"
        )

        logger.info(
            "RAG hybrid search + Graph RAG dla profilu: wiek=%s, edukacja=%s, lokalizacja=%s, pÅ‚eÄ‡=%s",
            age_group,
            education,
            location,
            gender,
        )

        try:
            # 1. GRAPH RAG - Pobierz strukturalny kontekst z grafu wiedzy
            graph_nodes = []
            graph_context_formatted = ""
            try:
                graph_nodes = self.rag_doc_service.get_demographic_graph_context(
                    age_group=age_group,
                    location=location,
                    education=education,
                    gender=gender
                )
                if graph_nodes:
                    graph_context_formatted = self._format_graph_context(graph_nodes)
                    logger.info("Pobrano %s wÄ™zÅ‚Ã³w grafu z kontekstem demograficznym", len(graph_nodes))
                else:
                    logger.info("Brak wynikÃ³w z graph context dla podanego profilu")
            except Exception as graph_exc:
                logger.warning("Nie udaÅ‚o siÄ™ pobraÄ‡ graph context: %s", graph_exc)

            # 2. HYBRID SEARCH (Vector + Keyword) - Pobierz chunki tekstowe
            if settings.RAG_USE_HYBRID_SEARCH:
                # ZwiÄ™kszamy k aby mieÄ‡ wiÄ™cej candidates dla reranking
                candidates_k = settings.RAG_RERANK_CANDIDATES if settings.RAG_USE_RERANKING else settings.RAG_TOP_K * 2

                vector_results = await self.vector_store.asimilarity_search_with_score(
                    query,
                    k=candidates_k,
                )
                keyword_results = await self._keyword_search(
                    query,
                    k=candidates_k,
                )
                fused_results = self._rrf_fusion(
                    vector_results,
                    keyword_results,
                    k=settings.RAG_RRF_K,
                )

                # 2b. RERANKING (opcjonalne) - Precyzyjny re-scoring z cross-encoder
                if settings.RAG_USE_RERANKING and self.reranker:
                    logger.info("Applying cross-encoder reranking on top %s candidates", len(fused_results))
                    final_results = self._rerank_with_cross_encoder(
                        query=query,
                        candidates=fused_results[:settings.RAG_RERANK_CANDIDATES],
                        top_k=settings.RAG_TOP_K
                    )
                    search_type = "hybrid+rerank+graph" if graph_nodes else "hybrid+rerank"
                else:
                    final_results = fused_results[:settings.RAG_TOP_K]
                    search_type = "hybrid+graph" if graph_nodes else "hybrid"
            else:
                final_results = await self.vector_store.asimilarity_search_with_score(
                    query,
                    k=settings.RAG_TOP_K,
                )
                search_type = "vector_only+graph" if graph_nodes else "vector_only"

            # 3. UNIFIED CONTEXT - WzbogaÄ‡ chunki o powiÄ…zane graph nodes
            context_chunks: List[str] = []
            citations: List[Dict[str, Any]] = []
            enriched_chunks_count = 0

            # Dodaj graph context na poczÄ…tku (jeÅ›li istnieje)
            if graph_context_formatted:
                context_chunks.append("=== STRUKTURALNA WIEDZA Z GRAFU WIEDZY ===\n")
                context_chunks.append(graph_context_formatted)
                context_chunks.append("\n=== KONTEKST Z DOKUMENTÃ“W (WZBOGACONY) ===\n")

            # Dodaj chunki tekstowe WZBOGACONE o powiÄ…zane graph nodes
            for doc, score in final_results:
                # ZnajdÅº graph nodes powiÄ…zane z tym chunkiem
                related_nodes = self._find_related_graph_nodes(doc, graph_nodes)

                # WzbogaÄ‡ chunk jeÅ›li sÄ… related nodes
                if related_nodes:
                    enriched_text = self._enrich_chunk_with_graph(
                        chunk_text=doc.page_content,
                        related_nodes=related_nodes
                    )
                    enriched_chunks_count += 1
                else:
                    enriched_text = doc.page_content

                # Truncate jeÅ›li za dÅ‚ugi
                if len(enriched_text) > 1000:
                    enriched_text = enriched_text[:1000] + "\n[...fragment obciÄ™ty...]"

                context_chunks.append(enriched_text)
                citations.append(
                    {
                        "text": doc.page_content[:500],  # Original dla citation
                        "score": float(score),
                        "metadata": doc.metadata,
                        "enriched": len(related_nodes) > 0,
                        "related_nodes_count": len(related_nodes)
                    }
                )

            if enriched_chunks_count > 0:
                logger.info(
                    "Unified context: %s/%s chunks enriched with graph nodes",
                    enriched_chunks_count,
                    len(final_results)
                )

            context = "\n\n---\n\n".join(context_chunks)
            if len(context) > settings.RAG_MAX_CONTEXT_CHARS:
                context = context[: settings.RAG_MAX_CONTEXT_CHARS] + "\n\n[... kontekst obciÄ™ty]"

            return {
                "context": context,
                "graph_context": graph_context_formatted,
                "graph_nodes": graph_nodes,
                "citations": citations,
                "query": query,
                "num_results": len(final_results),
                "search_type": search_type,
            }
        except Exception as exc:  # pragma: no cover - zwracamy pusty kontekst
            logger.error("Hybrydowe wyszukiwanie nie powiodÅ‚o siÄ™: %s", exc, exc_info=True)
            return {"context": "", "citations": [], "query": query, "num_results": 0}

    def _rrf_fusion(
        self,
        vector_results: List[Tuple[Document, float]],
        keyword_results: List[Document],
        k: int = 60,
    ) -> List[Tuple[Document, float]]:
        """ÅÄ…czy wyniki vector i keyword search przy pomocy Reciprocal Rank Fusion."""

        scores: Dict[int, float] = {}
        doc_map: Dict[int, Tuple[Document, float]] = {}

        for rank, (doc, original_score) in enumerate(vector_results):
            doc_hash = hash(doc.page_content)
            scores[doc_hash] = scores.get(doc_hash, 0.0) + 1.0 / (k + rank + 1)
            doc_map[doc_hash] = (doc, original_score)

        for rank, doc in enumerate(keyword_results):
            doc_hash = hash(doc.page_content)
            scores[doc_hash] = scores.get(doc_hash, 0.0) + 1.0 / (k + rank + 1)
            if doc_hash not in doc_map:
                doc_map[doc_hash] = (doc, 0.0)

        fused = sorted(scores.items(), key=lambda item: item[1], reverse=True)
        return [(doc_map[doc_hash][0], fused_score) for doc_hash, fused_score in fused]