"""Serwisy RAG odpowiedzialne za budowƒô grafu wiedzy i hybrydowe wyszukiwanie.

Modu≈Ç udostƒôpnia dwie komplementarne klasy:

* :class:`RAGDocumentService` ‚Äì odpowiada za pe≈Çny pipeline przetwarzania
  dokument√≥w (ingest), utrzymanie indeksu wektorowego oraz grafu wiedzy w Neo4j
  i obs≈Çugƒô zapyta≈Ñ Graph RAG.
* :class:`PolishSocietyRAG` ‚Äì realizuje hybrydowe wyszukiwanie kontekstu
  (vector + keyword + RRF) wykorzystywane w generatorze person.

Dokumentacja i komentarze pozostajƒÖ po polsku, aby u≈Çatwiƒá wsp√≥≈Çpracƒô zespo≈Çu.
"""

from __future__ import annotations

import asyncio
import logging
from pathlib import Path
from typing import Any, Dict, List, Tuple
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader
from langchain_community.graphs import Neo4jGraph
from langchain_community.vectorstores import Neo4jVector
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_experimental.graph_transformers.llm import LLMGraphTransformer
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

from app.core.config import get_settings
from app.models.rag_document import RAGDocument

settings = get_settings()
logger = logging.getLogger(__name__)


class GraphRAGQuery(BaseModel):
    """Struktura odpowiedzi z LLM przekszta≈ÇcajƒÖca pytanie w zapytanie Cypher."""

    entities: List[str] = Field(
        default_factory=list,
        description="Lista najwa≈ºniejszych encji z pytania u≈ºytkownika.",
    )
    cypher_query: str = Field(
        description="Zapytanie Cypher, kt√≥re ma zostaƒá wykonane na grafie wiedzy.",
    )


class RAGDocumentService:
    """Serwis zarzƒÖdzajƒÖcy dokumentami, grafem wiedzy i zapytaniami Graph RAG.

    Zakres odpowiedzialno≈õci:

    1. Wczytywanie dokument√≥w PDF/DOCX i dzielenie ich na fragmenty.
    2. Generowanie embedding√≥w i zapis chunk√≥w w indeksie wektorowym Neo4j.
    3. Budowa uniwersalnego grafu wiedzy na bazie `LLMGraphTransformer`.
    4. Odpowiadanie na pytania u≈ºytkownik√≥w z wykorzystaniem Graph RAG
       (po≈ÇƒÖczenie kontekstu grafowego i semantycznego).
    5. ZarzƒÖdzanie dokumentami w bazie PostgreSQL (lista, usuwanie z czyszczeniem
       danych w Neo4j).
    """

    def __init__(self) -> None:
        """Inicjalizuje wszystkie niezbƒôdne komponenty LangChain i Neo4j."""

        self.settings = settings

        # Model konwersacyjny wykorzystywany zar√≥wno do budowy grafu, jak i
        # generowania finalnych odpowiedzi Graph RAG.
        self.llm = ChatGoogleGenerativeAI(
            model=settings.GRAPH_MODEL,
            google_api_key=self.settings.GOOGLE_API_KEY,
            temperature=0,
        )

        # Embeddingi Google Gemini wykorzystywane przez indeks wektorowy Neo4j.
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=settings.EMBEDDING_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
        )

        # Inicjalizacja Neo4j Vector Store z retry logic (dla Docker startup race condition)
        self.vector_store = self._init_vector_store_with_retry()

        # Inicjalizacja Neo4j Graph Store z retry logic
        self.graph_store = self._init_graph_store_with_retry()

    def _init_vector_store_with_retry(self, max_retries: int = 10, initial_delay: float = 1.0):
        """Inicjalizuje Neo4j Vector Store z retry logic (dla Docker startup).

        Neo4j w Dockerze potrzebuje 10-15s na start (plugins: APOC, GDS).
        Retry z exponential backoff zapobiega race condition przy startup.

        Args:
            max_retries: Maksymalna liczba pr√≥b (default: 10 = ~30s total)
            initial_delay: PoczƒÖtkowe op√≥≈∫nienie w sekundach (default: 1.0s)

        Returns:
            Neo4jVector instance lub None je≈õli wszystkie pr√≥by failed
        """
        import time

        logger.info("üîÑ Inicjalizacja Neo4j Vector Store (z retry logic)")
        logger.info("   URL: %s, User: %s", settings.NEO4J_URI, settings.NEO4J_USER)

        delay = initial_delay
        for attempt in range(1, max_retries + 1):
            try:
                vector_store = Neo4jVector(
                    url=settings.NEO4J_URI,
                    username=settings.NEO4J_USER,
                    password=settings.NEO4J_PASSWORD,
                    embedding=self.embeddings,
                    index_name="rag_document_embeddings",
                    node_label="RAGChunk",
                    text_node_property="text",
                    embedding_node_property="embedding",
                )
                logger.info("‚úÖ Neo4j Vector Store po≈ÇƒÖczony (pr√≥ba %d/%d)", attempt, max_retries)
                return vector_store

            except Exception as exc:
                if attempt < max_retries:
                    logger.warning(
                        "‚ö†Ô∏è  Neo4j Vector Store - pr√≥ba %d/%d failed: %s. Retry za %.1fs...",
                        attempt, max_retries, str(exc)[:100], delay
                    )
                    time.sleep(delay)
                    delay = min(delay * 1.5, 10.0)  # Exponential backoff (cap at 10s)
                else:
                    logger.error(
                        "‚ùå Neo4j Vector Store - wszystkie %d pr√≥b failed. RAG wy≈ÇƒÖczony.",
                        max_retries,
                        exc_info=True
                    )
                    return None

        return None

    def _init_graph_store_with_retry(self, max_retries: int = 10, initial_delay: float = 1.0):
        """Inicjalizuje Neo4j Graph Store z retry logic (dla Docker startup).

        Args:
            max_retries: Maksymalna liczba pr√≥b (default: 10 = ~30s total)
            initial_delay: PoczƒÖtkowe op√≥≈∫nienie w sekundach (default: 1.0s)

        Returns:
            Neo4jGraph instance lub None je≈õli wszystkie pr√≥by failed
        """
        import time

        logger.info("üîÑ Inicjalizacja Neo4j Graph Store (z retry logic)")

        delay = initial_delay
        for attempt in range(1, max_retries + 1):
            try:
                graph_store = Neo4jGraph(
                    url=settings.NEO4J_URI,
                    username=settings.NEO4J_USER,
                    password=settings.NEO4J_PASSWORD,
                )
                logger.info("‚úÖ Neo4j Graph Store po≈ÇƒÖczony (pr√≥ba %d/%d)", attempt, max_retries)
                return graph_store

            except Exception as exc:
                if attempt < max_retries:
                    logger.warning(
                        "‚ö†Ô∏è  Neo4j Graph Store - pr√≥ba %d/%d failed: %s. Retry za %.1fs...",
                        attempt, max_retries, str(exc)[:100], delay
                    )
                    time.sleep(delay)
                    delay = min(delay * 1.5, 10.0)  # Exponential backoff (cap at 10s)
                else:
                    logger.error(
                        "‚ùå Neo4j Graph Store - wszystkie %d pr√≥b failed. GraphRAG wy≈ÇƒÖczony.",
                        max_retries,
                        exc_info=True
                    )
                    return None

        return None

    async def ingest_document(self, file_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Przetwarza dokument przez pe≈Çny pipeline: load ‚Üí chunk ‚Üí graph ‚Üí vector.

        Args:
            file_path: ≈öcie≈ºka do pliku PDF lub DOCX zapisanej kopii dokumentu.
            metadata: Metadane dokumentu (doc_id, title, country, itp.).

        Returns:
            S≈Çownik zawierajƒÖcy liczbƒô chunk√≥w oraz status zako≈Ñczenia procesu.

        Raises:
            RuntimeError: Gdy brakuje po≈ÇƒÖczenia z Neo4j (vector store jest kluczowy).
            FileNotFoundError: Je≈õli plik nie istnieje.
            ValueError: Przy nieobs≈Çugiwanym rozszerzeniu lub braku tre≈õci.
        """

        if not self.vector_store:
            raise RuntimeError("Brak po≈ÇƒÖczenia z Neo4j Vector Store ‚Äì ingest niemo≈ºliwy.")

        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"Nie znaleziono pliku: {file_path}")

        logger.info("Rozpoczynam przetwarzanie dokumentu: %s", path.name)

        try:
            # 1. LOAD ‚Äì wyb√≥r loadera zale≈ºnie od rozszerzenia pliku.
            file_extension = path.suffix.lower()
            if file_extension == ".pdf":
                loader = PyPDFLoader(str(path))
                logger.info("U≈ºywam PyPDFLoader dla pliku %s", path.name)
            elif file_extension == ".docx":
                loader = Docx2txtLoader(str(path))
                logger.info("U≈ºywam Docx2txtLoader dla pliku %s", path.name)
            else:
                raise ValueError(
                    f"Nieobs≈Çugiwany typ pliku: {file_extension}. Dozwolone: PDF, DOCX."
                )

            documents = await asyncio.to_thread(loader.load)
            if not documents:
                raise ValueError("Nie uda≈Ço siƒô odczytaƒá zawarto≈õci dokumentu.")
            logger.info("Wczytano %s segment√≥w dokumentu ≈∫r√≥d≈Çowego.", len(documents))

            # 2. SPLIT ‚Äì dzielenie tekstu na fragmenty z kontrolowanym overlapem.
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.RAG_CHUNK_SIZE,
                chunk_overlap=settings.RAG_CHUNK_OVERLAP,
                separators=["\n\n", "\n", ". ", " ", ""],
                length_function=len,
            )
            chunks = text_splitter.split_documents(documents)
            if not chunks:
                raise ValueError("Nie wygenerowano ≈ºadnych fragment√≥w tekstu.")
            logger.info(
                "Podzielono dokument na %s fragment√≥w (chunk_size=%s, overlap=%s)",
                len(chunks),
                settings.RAG_CHUNK_SIZE,
                settings.RAG_CHUNK_OVERLAP,
            )

            # 3. METADATA ‚Äì wzbogacenie ka≈ºdego chunku o metadane identyfikujƒÖce.
            doc_id = metadata.get("doc_id")
            for index, chunk in enumerate(chunks):
                chunk.metadata.update(
                    {
                        "doc_id": str(doc_id),
                        "chunk_index": index,
                        "title": metadata.get("title", "Nieznany dokument"),
                        "country": metadata.get("country", "Poland"),
                        "source_file": path.name,
                    }
                )

            # 4. GRAPH ‚Äì pr√≥bujemy zbudowaƒá graf wiedzy, je≈õli Neo4j Graph jest dostƒôpny.
            if self.graph_store:
                try:
                    logger.info("Generujƒô strukturƒô grafowƒÖ na podstawie uniwersalnego modelu.")
                    transformer = LLMGraphTransformer(
                        llm=self.llm,
                        allowed_nodes=[
                            "Obserwacja",   # Fakty, obserwacje (merge Przyczyna, Skutek tutaj)
                            "Wskaznik",     # Wska≈∫niki liczbowe, statystyki
                            "Demografia",   # Grupy demograficzne
                            "Trend",        # Trendy czasowe, zmiany w czasie
                            "Lokalizacja",  # Miejsca geograficzne
                        ],
                        allowed_relationships=[
                            "OPISUJE",           # Opisuje cechƒô/w≈Ça≈õciwo≈õƒá
                            "DOTYCZY",           # Dotyczy grupy/kategorii
                            "POKAZUJE_TREND",    # Pokazuje trend czasowy
                            "ZLOKALIZOWANY_W",   # Zlokalizowane w miejscu
                            "POWIAZANY_Z",       # Og√≥lne powiƒÖzanie (merge: przyczynowo≈õƒá, por√≥wnania)
                        ],
                        node_properties=[
                            "streszczenie",     # MUST: Jednozdaniowe podsumowanie (max 150 znak√≥w)
                            "skala",            # Wielko≈õƒá/warto≈õƒá z jednostkƒÖ (np. "67%", "1.2 mln")
                            "pewnosc",          # MUST: Pewno≈õƒá: "wysoka", "srednia", "niska"
                            "okres_czasu",      # Okres czasu (YYYY lub YYYY-YYYY)
                            "kluczowe_fakty",   # Opcjonalnie: max 3 fakty (separated by semicolons)
                        ],
                        relationship_properties=[
                            "sila",  # Si≈Ça relacji: "silna", "umiarkowana", "slaba"
                        ],
                        additional_instructions="""
JƒòZYK: Wszystkie nazwy i warto≈õci MUSZƒÑ byƒá PO POLSKU.

=== TYPY WƒòZ≈Å√ìW (5) ===
- Obserwacja: Fakty, obserwacje spo≈Çeczne (w≈ÇƒÖcznie z przyczynami i skutkami)
- Wskaznik: Wska≈∫niki liczbowe, statystyki (np. stopa zatrudnienia)
- Demografia: Grupy demograficzne (np. m≈Çodzi doro≈õli)
- Trend: Trendy czasowe, zmiany w czasie
- Lokalizacja: Miejsca geograficzne

=== TYPY RELACJI (5) ===
- OPISUJE: Opisuje cechƒô/w≈Ça≈õciwo≈õƒá
- DOTYCZY: Dotyczy grupy/kategorii
- POKAZUJE_TREND: Pokazuje trend czasowy
- ZLOKALIZOWANY_W: Zlokalizowane w miejscu
- POWIAZANY_Z: Og√≥lne powiƒÖzanie (przyczynowo≈õƒá, por√≥wnania, korelacje)

=== PROPERTIES WƒòZ≈Å√ìW (5 - uproszczone!) ===
- streszczenie (MUST): 1 zdanie, max 150 znak√≥w
- skala: Warto≈õƒá z jednostkƒÖ (np. "78.4%", "5000 PLN", "1.2 mln os√≥b")
- pewnosc (MUST): "wysoka" / "srednia" / "niska"
- okres_czasu: YYYY lub YYYY-YYYY
- kluczowe_fakty: Max 3 fakty oddzielone ≈õrednikami

=== PROPERTIES RELACJI (1) ===
- sila: "silna" / "umiarkowana" / "slaba"

=== PRZYK≈ÅADY (FEW-SHOT) ===

PRZYK≈ÅAD 1 - Wskaznik:
Tekst: "W 2022 stopa zatrudnienia kobiet 25-34 z wy≈ºszym wynosi≈Ça 78.4% wed≈Çug GUS"
Wƒôze≈Ç: {{
  type: "Wskaznik",
  streszczenie: "Stopa zatrudnienia kobiet 25-34 z wy≈ºszym wykszta≈Çceniem",
  skala: "78.4%",
  pewnosc: "wysoka",
  okres_czasu: "2022",
  kluczowe_fakty: "wysoka stopa zatrudnienia; kobiety m≈Çode; wykszta≈Çcenie wy≈ºsze"
}}

PRZYK≈ÅAD 2 - Obserwacja:
Tekst: "M≈Çodzi mieszka≈Ñcy du≈ºych miast coraz czƒô≈õciej wynajmujƒÖ mieszkania zamiast kupowaƒá"
Wƒôze≈Ç: {{
  type: "Obserwacja",
  streszczenie: "M≈Çodzi w miastach preferujƒÖ wynajem nad zakup mieszka≈Ñ",
  pewnosc: "srednia",
  kluczowe_fakty: "m≈Çodzi doro≈õli; du≈ºe miasta; wynajem mieszka≈Ñ"
}}

PRZYK≈ÅAD 3 - Trend:
Tekst: "Od 2018 do 2023 wzr√≥s≈Ç odsetek os√≥b pracujƒÖcych zdalnie z 12% do 31%"
Wƒôze≈Ç: {{
  type: "Trend",
  streszczenie: "Wzrost pracy zdalnej w Polsce",
  skala: "12% ‚Üí 31%",
  pewnosc: "wysoka",
  okres_czasu: "2018-2023",
  kluczowe_fakty: "praca zdalna; wzrost; pandemia"
}}

=== VALIDATION RULES ===
- streszczenie: Zawsze wype≈Çnij (1 zdanie, max 150 znak√≥w)
- pewnosc: Zawsze wype≈Çnij ("wysoka", "srednia", "niska")
- skala: Tylko dla Wskaznik (inne: opcjonalnie)
- kluczowe_fakty: Max 3 fakty, separated by semicolons
- doc_id, chunk_index: KRYTYCZNE dla lifecycle (zachowane automatycznie)

=== FOCUS ===
Priorytet: streszczenie + pewnosc. Nie traƒá czasu na zbƒôdne opisy.
                        """.strip(),
                    )
                    graph_documents = await transformer.aconvert_to_graph_documents(chunks)

                    # Wzbogacenie wƒôz≈Ç√≥w o metadane dokumentu
                    enriched_graph_documents = self._enrich_graph_nodes(
                        graph_documents,
                        doc_id=str(doc_id),
                        metadata=metadata
                    )

                    self.graph_store.add_graph_documents(enriched_graph_documents, include_source=True)
                    logger.info("Zapisano strukturƒô grafowƒÖ dla dokumentu %s", doc_id)
                except Exception as graph_exc:  # pragma: no cover - logujemy, ale nie przerywamy
                    logger.error(
                        "Nie uda≈Ço siƒô wygenerowaƒá grafu wiedzy dla dokumentu %s: %s",
                        doc_id,
                        graph_exc,
                        exc_info=True,
                    )

            else:
                logger.warning(
                    "Neo4j Graph Store nie jest dostƒôpny ‚Äì dokument zostanie przetworzony "
                    "bez struktury grafowej."
                )

            # 5. VECTOR ‚Äì zapis chunk√≥w do indeksu wektorowego w Neo4j.
            logger.info("Generujƒô embeddingi i zapisujƒô je w indeksie wektorowym...")
            await self.vector_store.aadd_documents(chunks)
            logger.info(
                "Zako≈Ñczono przetwarzanie %s fragment√≥w dokumentu %s",
                len(chunks),
                doc_id,
            )

            return {"num_chunks": len(chunks), "status": "ready"}

        except Exception as exc:  # pragma: no cover - logujemy pe≈ÇnƒÖ diagnostykƒô
            logger.error(
                "B≈ÇƒÖd podczas przetwarzania dokumentu %s: %s",
                file_path,
                exc,
                exc_info=True,
            )
            return {"num_chunks": 0, "status": "failed", "error": str(exc)}

    def _enrich_graph_nodes(
        self,
        graph_documents: List[Any],
        doc_id: str,
        metadata: Dict[str, Any]
    ) -> List[Any]:
        """Wzbogaca wƒôz≈Çy grafu o metadane dokumentu i waliduje jako≈õƒá danych.

        Args:
            graph_documents: Lista GraphDocument z LLMGraphTransformer
            doc_id: UUID dokumentu ≈∫r√≥d≈Çowego
            metadata: Metadane dokumentu (title, country, itp.)

        Returns:
            Lista wzbogaconych GraphDocument z pe≈Çnymi metadanymi

        Proces wzbogacania:
            1. Dodaje doc_id, chunk_index do ka≈ºdego wƒôz≈Ça
            2. Dodaje metadane dokumentu: title, country, date
            3. Waliduje jako≈õƒá metadanych wƒôz≈Ç√≥w (sprawdza czy summary i description nie sƒÖ puste)
            4. Dodaje timestamp przetwarzania
            5. Normalizuje formaty danych (confidence, magnitude)
        """
        from datetime import datetime, timezone

        logger.info(
            "Wzbogacam %s dokument√≥w grafu o metadane doc_id=%s",
            len(graph_documents),
            doc_id
        )

        enriched_count = 0
        validation_warnings = 0

        for graph_doc in graph_documents:
            # Pobierz chunk_index z source document je≈õli dostƒôpny
            chunk_index = graph_doc.source.metadata.get("chunk_index", 0) if graph_doc.source else 0

            # Wzbogacenie wƒôz≈Ç√≥w
            for node in graph_doc.nodes:
                # 1. METADANE TECHNICZNE (krytyczne dla usuwania dokument√≥w)
                if not hasattr(node, 'properties'):
                    node.properties = {}

                node.properties['doc_id'] = doc_id
                node.properties['chunk_index'] = chunk_index
                node.properties['processed_at'] = datetime.now(timezone.utc).isoformat()

                # 2. METADANE DOKUMENTU (kontekst ≈∫r√≥d≈Çowy)
                node.properties['document_title'] = metadata.get('title', 'Unknown')
                node.properties['document_country'] = metadata.get('country', 'Poland')
                if metadata.get('date'):
                    node.properties['document_year'] = str(metadata.get('date'))

                # 3. WALIDACJA JAKO≈öCI METADANYCH
                # Sprawd≈∫ czy kluczowe w≈Ça≈õciwo≈õci sƒÖ wype≈Çnione
                if node.properties.get('streszczenie') in (None, '', 'N/A'):
                    validation_warnings += 1
                    logger.debug(
                        "Wƒôze≈Ç '%s' nie ma streszczenia - LLM mo≈ºe nie wyekstraktowaƒá wszystkich w≈Ça≈õciwo≈õci",
                        node.id
                    )

                # 4. NORMALIZACJA FORMAT√ìW
                # Pewno≈õƒá normalizacja
                pewnosc = node.properties.get('pewnosc', '').lower()
                if pewnosc not in ('wysoka', 'srednia', 'niska'):
                    node.properties['pewnosc'] = 'srednia'  # default

                # Skala - upewnij siƒô ≈ºe jest stringiem
                if node.properties.get('skala') and not isinstance(node.properties['skala'], str):
                    node.properties['skala'] = str(node.properties['skala'])

                enriched_count += 1

            # Wzbogacenie relacji
            for relationship in graph_doc.relationships:
                if not hasattr(relationship, 'properties'):
                    relationship.properties = {}

                # Metadane techniczne
                relationship.properties['doc_id'] = doc_id
                relationship.properties['chunk_index'] = chunk_index

                # Normalizacja si≈Çy (jedyna property relacji)
                sila = relationship.properties.get('sila', '').lower()
                if sila not in ('silna', 'umiarkowana', 'slaba'):
                    relationship.properties['sila'] = 'umiarkowana'  # default

        logger.info(
            "Wzbogacono %s wƒôz≈Ç√≥w. Ostrze≈ºenia walidacji: %s",
            enriched_count,
            validation_warnings
        )

        if validation_warnings > enriched_count * 0.3:  # >30% wƒôz≈Ç√≥w bez summary
            logger.warning(
                "Wysoki odsetek wƒôz≈Ç√≥w (%s%%) bez pe≈Çnych metadanych. "
                "LLM mo≈ºe potrzebowaƒá lepszych instrukcji lub wiƒôkszego kontekstu.",
                int(validation_warnings / enriched_count * 100)
            )

        return graph_documents

    def _generate_cypher_query(self, question: str) -> GraphRAGQuery:
        """U≈ºywa LLM do prze≈Ço≈ºenia pytania u≈ºytkownika na zapytanie Cypher."""

        if not self.graph_store:
            raise RuntimeError("Graph RAG nie jest dostƒôpny ‚Äì brak po≈ÇƒÖczenia z Neo4j Graph.")

        graph_schema = self.graph_store.get_schema()
        cypher_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    Jeste≈õ analitykiem bada≈Ñ spo≈Çecznych. Twoim zadaniem jest zamiana pytania
                    u≈ºytkownika na zapytanie Cypher korzystajƒÖce z poni≈ºszego schematu grafu.

                    === DOSTƒòPNE TYPY WƒòZ≈Å√ìW (5) ===
                    - Obserwacja: Fakty, obserwacje (w≈ÇƒÖcznie z przyczynami i skutkami)
                    - Wskaznik: Wska≈∫niki liczbowe, statystyki
                    - Demografia: Grupy demograficzne
                    - Trend: Trendy czasowe
                    - Lokalizacja: Miejsca geograficzne

                    === DOSTƒòPNE TYPY RELACJI (5) ===
                    - OPISUJE, DOTYCZY, POKAZUJE_TREND, ZLOKALIZOWANY_W
                    - POWIAZANY_Z: Og√≥lne powiƒÖzanie (przyczynowo≈õƒá, por√≥wnania)

                    === DOSTƒòPNE W≈ÅA≈öCIWO≈öCI WƒòZ≈Å√ìW (5 - uproszczone!) ===
                    - streszczenie: Jednozdaniowe podsumowanie (max 150 znak√≥w)
                    - skala: Wielko≈õƒá/warto≈õƒá z jednostkƒÖ (np. "78.4%")
                    - pewnosc: Pewno≈õƒá danych ("wysoka", "srednia", "niska")
                    - okres_czasu: Okres czasu (YYYY lub YYYY-YYYY)
                    - kluczowe_fakty: Max 3 fakty (separated by semicolons)
                    - document_title, document_country, document_year: Metadane dokumentu

                    === DOSTƒòPNE W≈ÅA≈öCIWO≈öCI RELACJI (1) ===
                    - sila: Si≈Ça relacji ("silna", "umiarkowana", "slaba")

                    === INSTRUKCJE TWORZENIA ZAPYTA≈É ===
                    1. U≈ºywaj POLSKICH nazw properties (streszczenie, pewnosc, skala, etc.)
                    2. ZAWSZE zwracaj streszczenie + kluczowe_fakty
                    3. Filtruj po pewnosc je≈õli u≈ºytkownik pyta o pewne fakty ("wysoka")
                    4. Filtruj po sila relacji dla silnych zale≈ºno≈õci ("silna")
                    5. Sortuj po skala dla pyta≈Ñ o najwiƒôksze wska≈∫niki
                    6. Filtruj po okres_czasu dla pyta≈Ñ czasowych
                    7. U≈ºywaj POWIAZANY_Z dla pyta≈Ñ o zale≈ºno≈õci/przyczyny

                    === PRZYK≈ÅADY ZAPYTA≈É (nowy schema) ===
                    ```cypher
                    // Najwiƒôksze wska≈∫niki
                    MATCH (n:Wskaznik) WHERE n.skala IS NOT NULL
                    RETURN n.streszczenie, n.skala, n.pewnosc, n.okres_czasu
                    ORDER BY toFloat(split(n.skala, '%')[0]) DESC LIMIT 10

                    // Pewne fakty o X
                    MATCH (n:Obserwacja)
                    WHERE n.streszczenie CONTAINS 'X' AND n.pewnosc = 'wysoka'
                    RETURN n.streszczenie, n.kluczowe_fakty, n.okres_czasu

                    // PowiƒÖzania X ‚Üí Y (u≈ºywa POWIAZANY_Z)
                    MATCH (n1)-[r:POWIAZANY_Z]->(n2)
                    WHERE n1.streszczenie CONTAINS 'X' AND r.sila = 'silna'
                    RETURN n1.streszczenie, r.sila, n2.streszczenie
                    ```

                    Schemat grafu: {graph_schema}
                    """.strip(),
                ),
                ("human", "Pytanie u≈ºytkownika: {question}"),
            ]
        )

        chain = cypher_prompt | self.llm.with_structured_output(GraphRAGQuery)
        return chain.invoke({"question": question, "graph_schema": graph_schema})

    async def answer_question(self, question: str) -> Dict[str, Any]:
        """Realizuje pe≈Çen przep≈Çyw Graph RAG i zwraca ustrukturyzowanƒÖ odpowied≈∫."""

        if not self.graph_store or not self.vector_store:
            raise ConnectionError(
                "Graph RAG wymaga jednoczesnego dostƒôpu do grafu i indeksu wektorowego."
            )

        logger.info("Generujƒô zapytanie Cypher dla pytania: %s", question)
        rag_query = self._generate_cypher_query(question)
        logger.info("Wygenerowane zapytanie Cypher: %s", rag_query.cypher_query)

        # 1. Kontekst grafowy ‚Äì zapytanie Cypher wygenerowane przez LLM.
        try:
            graph_context = self.graph_store.query(rag_query.cypher_query)
        except Exception as exc:  # pragma: no cover - logujemy i kontynuujemy
            logger.error("B≈ÇƒÖd wykonania zapytania Cypher: %s", exc, exc_info=True)
            graph_context = []

        # 2. Kontekst wektorowy ‚Äì semantyczne wyszukiwanie po encjach.
        vector_context_docs: List[Document] = []
        if rag_query.entities:
            search_query = " ".join(rag_query.entities)
            vector_context_docs = await self.vector_store.asimilarity_search(search_query, k=5)

        # 3. Agregacja kontekstu i wygenerowanie odpowiedzi ko≈Ñcowej.
        final_context = "KONTEKST Z GRAFU WIEDZY:\n" + str(graph_context)
        final_context += "\n\nFRAGMENTY Z DOKUMENT√ìW:\n"
        for doc in vector_context_docs:
            final_context += f"- {doc.page_content}\n"

        answer_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    Jeste≈õ ekspertem od analiz spo≈Çecznych. Odpowiadasz wy≈ÇƒÖcznie na
                    podstawie dostarczonego kontekstu z grafu i dokument√≥w. Udzielaj
                    precyzyjnych, zweryfikowalnych odpowiedzi po polsku.
                    """.strip(),
                ),
                ("human", "Pytanie: {question}\n\nKontekst:\n{context}"),
            ]
        )

        response = await (answer_prompt | self.llm).ainvoke(
            {"question": question, "context": final_context}
        )

        return {
            "answer": response.content,
            "graph_context": graph_context,
            "vector_context": [doc.to_json() for doc in vector_context_docs],
            "cypher_query": rag_query.cypher_query,
        }

    async def list_documents(self, db: AsyncSession) -> List[RAGDocument]:
        """Zwraca listƒô aktywnych dokument√≥w posortowanych malejƒÖco po dacie."""

        result = await db.execute(
            select(RAGDocument)
            .where(RAGDocument.is_active.is_(True))
            .order_by(RAGDocument.created_at.desc())
        )
        return result.scalars().all()

    async def delete_document(self, doc_id: UUID, db: AsyncSession) -> None:
        """Usuwa dokument z PostgreSQL i czy≈õci powiƒÖzane dane w Neo4j."""

        doc = await db.get(RAGDocument, doc_id)
        if not doc:
            raise ValueError(f"Document {doc_id} not found")

        doc.is_active = False
        await db.commit()

        await self._delete_chunks_from_neo4j(str(doc_id))

        if self.graph_store:
            try:
                self.graph_store.query(
                    "MATCH (n {doc_id: $doc_id}) DETACH DELETE n",
                    params={"doc_id": str(doc_id)},
                )
                logger.info("Usuniƒôto wƒôz≈Çy grafu dla dokumentu %s", doc_id)
            except Exception as exc:  # pragma: no cover - logujemy, ale nie przerywamy
                logger.error(
                    "Nie uda≈Ço siƒô usunƒÖƒá wƒôz≈Ç√≥w grafu dokumentu %s: %s",
                    doc_id,
                    exc,
                )

    async def _delete_chunks_from_neo4j(self, doc_id: str) -> None:
        """Czy≈õci wszystkie chunki dokumentu z indeksu Neo4j Vector."""

        if not self.vector_store:
            return

        try:
            driver = self.vector_store._driver  # Dostƒôp wewnƒôtrzny ‚Äì akceptowalny w serwisie.

            def delete_chunks() -> None:
                with driver.session() as session:
                    session.execute_write(
                        lambda tx: tx.run(
                            "MATCH (n:RAGChunk {doc_id: $doc_id}) DETACH DELETE n",
                            doc_id=doc_id,
                        )
                    )

            await asyncio.to_thread(delete_chunks)
            logger.info("Usuniƒôto wektorowe chunki dokumentu %s", doc_id)
        except Exception as exc:  # pragma: no cover - logujemy, ale nie przerywamy
            logger.error("Nie uda≈Ço siƒô usunƒÖƒá chunk√≥w dokumentu %s z Neo4j: %s", doc_id, exc)

    def get_demographic_graph_context(
        self,
        age_group: str,
        location: str,
        education: str,
        gender: str
    ) -> List[Dict[str, Any]]:
        """Pobiera strukturalny kontekst z grafu wiedzy dla profilu demograficznego.

        Wykonuje zapytania Cypher na grafie aby znale≈∫ƒá:
        1. Indicators (wska≈∫niki) - z magnitude, confidence_level
        2. Observations (obserwacje) - z key_facts
        3. Trends (trendy) - z time_period
        4. Demographics nodes powiƒÖzane z profilem

        Args:
            age_group: Grupa wiekowa (np. "25-34")
            location: Lokalizacja (np. "Warszawa")
            education: Poziom wykszta≈Çcenia (np. "wy≈ºsze")
            gender: P≈Çeƒá (np. "kobieta")

        Returns:
            Lista s≈Çownik√≥w z wƒôz≈Çami grafu i ich w≈Ça≈õciwo≈õciami:
            [
                {
                    "type": "Indicator" | "Observation" | "Trend" | "Demographic",
                    "summary": str,
                    "key_facts": str,
                    "magnitude": str,
                    "confidence_level": str,
                    "time_period": str,
                    "source_context": str
                },
                ...
            ]
        """
        if not self.graph_store:
            logger.warning("Graph store nie jest dostƒôpny - zwracam pusty kontekst grafowy")
            return []

        # Mapowanie PL ‚Üí EN dla dual-language search
        # Problem: Graph nodes mogƒÖ mieƒá w≈Ça≈õciwo≈õci w jƒôzyku angielskim (legacy data)
        # RozwiƒÖzanie: Dodajemy angielskie odpowiedniki polskich termin√≥w
        PL_TO_EN_SEARCH_TERMS = {
            "wykszta≈Çcenie wy≈ºsze": "higher education",
            "studia": "university",
            "uniwersytet": "university",
            "wykszta≈Çcenie ≈õrednie": "secondary education",
            "liceum": "high school",
            "zasadnicze zawodowe": "vocational",
            "demografia": "demographic",
            "spo≈Çecze≈Ñstwo polskie": "Polish society",
            "populacja": "population",
            "kobieta": "female",
            "mƒô≈ºczyzna": "male",
            "zatrudnienie": "employment",
            "praca": "work",
            "doch√≥d": "income",
            "zarobki": "earnings",
            "mieszkanie": "housing",
            "edukacja": "education",
            "zdrowie": "health",
            "kultura": "culture",
            "wska≈∫nik": "indicator",
            "statystyka": "statistic",
            "dane": "data",
            "badanie": "research",
            "raport": "report",
            "analiza": "analysis",
        }

        # Budujemy search terms dla zapytania
        # Normalizuj warto≈õci dla lepszego matchingu
        search_terms = []

        # Wiek - ekstrakcja liczb z zakresu
        if "-" in age_group:
            age_parts = age_group.split("-")
            search_terms.extend(age_parts)
        search_terms.append(age_group)

        # Lokalizacja
        search_terms.append(location)

        # Wykszta≈Çcenie - normalizuj + dual-language
        if "wy≈ºsze" in education.lower():
            search_terms.extend(["wykszta≈Çcenie wy≈ºsze", "studia", "uniwersytet"])
        elif "≈õrednie" in education.lower():
            search_terms.extend(["wykszta≈Çcenie ≈õrednie", "liceum"])
        elif "zawodowe" in education.lower():
            search_terms.extend(["zasadnicze zawodowe", "zawodowe"])
        else:
            search_terms.append(education)

        # P≈Çeƒá
        search_terms.append(gender)

        # Dodaj og√≥lne terminy demograficzne (wiƒôcej aby zwiƒôkszyƒá szanse na match)
        search_terms.extend([
            "demografia", "spo≈Çecze≈Ñstwo polskie", "populacja",
            "ludno≈õƒá", "mieszka≈Ñcy", "obywatele",
            "m≈Çodzi", "doro≈õli", "seniorzy",  # Generic age terms
            "wiek", "pokolenie", "generacja",
            "miasto", "miejski", "miejska",  # Generic location terms
            "polska", "polski", "polacy"  # Country terms
        ])

        # DUAL-LANGUAGE: Dodaj angielskie odpowiedniki dla polskich termin√≥w
        # Tworzy kopiƒô aby nie modyfikowaƒá listy podczas iteracji
        search_terms_copy = search_terms.copy()
        for pl_term in search_terms_copy:
            pl_normalized = pl_term.lower().strip()
            if pl_normalized in PL_TO_EN_SEARCH_TERMS:
                en_term = PL_TO_EN_SEARCH_TERMS[pl_normalized]
                if en_term not in search_terms:  # Unikaj duplikat√≥w
                    search_terms.append(en_term)

        logger.info(
            "üìä Graph context search - Profil: wiek=%s, lokalizacja=%s, wykszta≈Çcenie=%s, p≈Çeƒá=%s",
            age_group, location, education, gender
        )
        logger.info(
            "üîç Search terms (%s total): %s",
            len(search_terms),
            search_terms[:15]  # Log pierwsze 15 dla debugowania
        )

        try:
            # Zapytanie Cypher: Znajd≈∫ wƒôz≈Çy kt√≥re pasujƒÖ do search terms
            # pewnosc jest opcjonalny - preferujemy 'wysoka' ale akceptujemy wszystkie
            # UWAGA: Schema uproszczony - properties: streszczenie, skala, pewnosc, okres_czasu, kluczowe_fakty
            cypher_query = """
            // Parametry: $search_terms - lista s≈Ç√≥w kluczowych do matchingu
            // UWAGA: Schema u≈ºywa POLSKICH property names (streszczenie, skala, pewnosc, etc.)

            // 1. Znajd≈∫ Wska≈∫niki (preferuj wysokƒÖ pewno≈õƒá je≈õli istnieje)
            // Case-insensitive search
            MATCH (ind:Wskaznik)
            WHERE ANY(term IN $search_terms WHERE
                toLower(coalesce(ind.streszczenie, '')) CONTAINS toLower(term) OR
                toLower(coalesce(ind.kluczowe_fakty, '')) CONTAINS toLower(term)
            )
            WITH ind
            ORDER BY
                CASE WHEN ind.pewnosc = 'wysoka' THEN 0
                     WHEN ind.pewnosc = 'srednia' THEN 1
                     ELSE 2 END,
                size(coalesce(ind.kluczowe_fakty, '')) DESC
            LIMIT 3
            WITH collect({
                type: 'Wskaznik',
                streszczenie: ind.streszczenie,
                kluczowe_fakty: ind.kluczowe_fakty,
                skala: ind.skala,
                pewnosc: coalesce(ind.pewnosc, 'nieznana'),
                okres_czasu: ind.okres_czasu
            }) AS indicators

            // 2. Znajd≈∫ Obserwacje (preferuj wysokƒÖ pewno≈õƒá je≈õli istnieje)
            // Case-insensitive search
            MATCH (obs:Obserwacja)
            WHERE ANY(term IN $search_terms WHERE
                toLower(coalesce(obs.streszczenie, '')) CONTAINS toLower(term) OR
                toLower(coalesce(obs.kluczowe_fakty, '')) CONTAINS toLower(term)
            )
            WITH indicators, obs
            ORDER BY
                CASE WHEN obs.pewnosc = 'wysoka' THEN 0
                     WHEN obs.pewnosc = 'srednia' THEN 1
                     ELSE 2 END,
                size(coalesce(obs.kluczowe_fakty, '')) DESC
            LIMIT 3
            WITH indicators, collect({
                type: 'Obserwacja',
                streszczenie: obs.streszczenie,
                kluczowe_fakty: obs.kluczowe_fakty,
                pewnosc: coalesce(obs.pewnosc, 'nieznana'),
                okres_czasu: obs.okres_czasu
            }) AS observations

            // 3. Znajd≈∫ Trendy
            // Case-insensitive search
            MATCH (trend:Trend)
            WHERE ANY(term IN $search_terms WHERE
                toLower(coalesce(trend.streszczenie, '')) CONTAINS toLower(term) OR
                toLower(coalesce(trend.kluczowe_fakty, '')) CONTAINS toLower(term)
            )
            WITH indicators, observations, trend
            ORDER BY size(coalesce(trend.kluczowe_fakty, '')) DESC
            LIMIT 2
            WITH indicators, observations, collect({
                type: 'Trend',
                streszczenie: trend.streszczenie,
                kluczowe_fakty: trend.kluczowe_fakty,
                okres_czasu: trend.okres_czasu
            }) AS trends

            // 4. Znajd≈∫ wƒôz≈Çy Demografii
            // Case-insensitive search
            MATCH (demo:Demografia)
            WHERE ANY(term IN $search_terms WHERE
                toLower(coalesce(demo.streszczenie, '')) CONTAINS toLower(term) OR
                toLower(coalesce(demo.kluczowe_fakty, '')) CONTAINS toLower(term)
            )
            WITH indicators, observations, trends, demo
            ORDER BY
                CASE WHEN demo.pewnosc = 'wysoka' THEN 0
                     WHEN demo.pewnosc = 'srednia' THEN 1
                     ELSE 2 END
            LIMIT 2
            WITH indicators, observations, trends, collect({
                type: 'Demografia',
                streszczenie: demo.streszczenie,
                kluczowe_fakty: demo.kluczowe_fakty,
                pewnosc: coalesce(demo.pewnosc, 'nieznana')
            }) AS demographics

            // 5. Po≈ÇƒÖcz wszystkie wyniki
            RETURN indicators + observations + trends + demographics AS graph_context
            """

            result = self.graph_store.query(
                cypher_query,
                params={"search_terms": search_terms}
            )

            if not result or not result[0].get('graph_context'):
                logger.warning(
                    "‚ùå Brak wynik√≥w z grafu dla profilu: wiek=%s, wykszta≈Çcenie=%s, lokalizacja=%s, p≈Çeƒá=%s",
                    age_group, education, location, gender
                )
                return []

            graph_context = result[0]['graph_context']

            if not graph_context:
                logger.warning("‚ùå Graph context jest pusty (query zwr√≥ci≈Ço empty array)")
                return []

            logger.info(
                "‚úÖ Pobrano %s wƒôz≈Ç√≥w grafu: %s Wskaznik, %s Obserwacja, %s Trend, %s Demografia",
                len(graph_context),
                len([n for n in graph_context if n.get('type') == 'Wskaznik']),
                len([n for n in graph_context if n.get('type') == 'Obserwacja']),
                len([n for n in graph_context if n.get('type') == 'Trend']),
                len([n for n in graph_context if n.get('type') == 'Demografia'])
            )

            return graph_context

        except Exception as exc:
            logger.error(
                "B≈ÇƒÖd podczas pobierania graph context: %s",
                exc,
                exc_info=True
            )
            return []


class PolishSocietyRAG:
    """Hybrydowe wyszukiwanie kontekstu dla generatora person.

    ≈ÅƒÖczy wyszukiwanie semantyczne (embeddingi) oraz pe≈Çnotekstowe (fulltext index)
    korzystajƒÖc z techniki Reciprocal Rank Fusion. Klasa jest niezale≈ºna od
    Graph RAG, ale wsp√≥≈Çdzieli te same ustawienia i konwencjƒô metadanych.
    """

    def __init__(self) -> None:
        """Przygotowuje wektorowe i keywordowe zaplecze wyszukiwawcze."""

        self.settings = settings

        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=settings.EMBEDDING_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
        )

        # Inicjalizacja Neo4j Vector Store z retry logic (dla Docker startup race condition)
        self.vector_store = self._init_vector_store_with_retry()

        if self.vector_store:
            logger.info("‚úÖ PolishSocietyRAG: Neo4j Vector Store po≈ÇƒÖczony")

            # Fulltext index bƒôdzie tworzony lazy - przy pierwszym u≈ºyciu keyword search
            # (nie mo≈ºemy u≈ºyƒá asyncio.create_task() w __init__ bo mo≈ºe nie byƒá event loop)
            self._fulltext_index_initialized = False

            # Inicjalizuj RAGDocumentService dla dostƒôpu do graph context
            # U≈ºywamy leniwej inicjalizacji aby uniknƒÖƒá circular dependency
            self._rag_doc_service = None

            # Inicjalizuj cross-encoder dla reranking (opcjonalny)
            self.reranker = None
            if settings.RAG_USE_RERANKING:
                try:
                    from sentence_transformers import CrossEncoder
                    self.reranker = CrossEncoder(
                        settings.RAG_RERANKER_MODEL,
                        max_length=512
                    )
                    logger.info(
                        "Cross-encoder reranker zainicjalizowany: %s",
                        settings.RAG_RERANKER_MODEL
                    )
                except ImportError:
                    logger.warning(
                        "sentence-transformers nie jest zainstalowany - reranking wy≈ÇƒÖczony. "
                        "Zainstaluj: pip install sentence-transformers"
                    )
                except Exception as rerank_exc:
                    logger.warning(
                        "Nie uda≈Ço siƒô za≈Çadowaƒá reranker: %s - kontynuacja bez rerankingu",
                        rerank_exc
                    )
        else:
            logger.error("‚ùå PolishSocietyRAG: Neo4j Vector Store failed - RAG wy≈ÇƒÖczony")
            self._fulltext_index_initialized = False
            self._rag_doc_service = None
            self.reranker = None

    def _init_vector_store_with_retry(self, max_retries: int = 10, initial_delay: float = 1.0):
        """Inicjalizuje Neo4j Vector Store z retry logic (dla Docker startup).

        Neo4j w Dockerze potrzebuje 10-15s na start (plugins: APOC, GDS).
        Retry z exponential backoff zapobiega race condition przy startup.

        Args:
            max_retries: Maksymalna liczba pr√≥b (default: 10 = ~30s total)
            initial_delay: PoczƒÖtkowe op√≥≈∫nienie w sekundach (default: 1.0s)

        Returns:
            Neo4jVector instance lub None je≈õli wszystkie pr√≥by failed
        """
        import time

        logger.info("üîÑ PolishSocietyRAG: Inicjalizacja Neo4j Vector Store (z retry logic)")

        delay = initial_delay
        for attempt in range(1, max_retries + 1):
            try:
                vector_store = Neo4jVector(
                    url=settings.NEO4J_URI,
                    username=settings.NEO4J_USER,
                    password=settings.NEO4J_PASSWORD,
                    embedding=self.embeddings,
                    index_name="rag_document_embeddings",
                    node_label="RAGChunk",
                    text_node_property="text",
                    embedding_node_property="embedding",
                )
                logger.info("‚úÖ PolishSocietyRAG: Neo4j Vector Store po≈ÇƒÖczony (pr√≥ba %d/%d)", attempt, max_retries)
                return vector_store

            except Exception as exc:
                if attempt < max_retries:
                    logger.warning(
                        "‚ö†Ô∏è  PolishSocietyRAG: Neo4j Vector Store - pr√≥ba %d/%d failed: %s. Retry za %.1fs...",
                        attempt, max_retries, str(exc)[:100], delay
                    )
                    time.sleep(delay)
                    delay = min(delay * 1.5, 10.0)  # Exponential backoff (cap at 10s)
                else:
                    logger.error(
                        "‚ùå PolishSocietyRAG: Neo4j Vector Store - wszystkie %d pr√≥b failed",
                        max_retries,
                        exc_info=True
                    )
                    return None

        return None

    @property
    def rag_doc_service(self) -> RAGDocumentService:
        """LeniwƒÖ inicjalizacja RAGDocumentService dla dostƒôpu do graph context."""
        if self._rag_doc_service is None:
            self._rag_doc_service = RAGDocumentService()
        return self._rag_doc_service

    async def _ensure_fulltext_index(self) -> None:
        """Tworzy indeks fulltext w Neo4j na potrzeby wyszukiwania keywordowego."""

        if not self.vector_store:
            return

        try:
            driver = self.vector_store._driver

            def check_and_create_index() -> None:
                with driver.session() as session:
                    def ensure(tx) -> None:
                        indexes = [record["name"] for record in tx.run("SHOW INDEXES")]
                        if "rag_fulltext_index" not in indexes:
                            tx.run(
                                """
                                CREATE FULLTEXT INDEX rag_fulltext_index IF NOT EXISTS
                                FOR (n:RAGChunk)
                                ON EACH [n.text]
                                """
                            )

                    session.execute_write(ensure)

            await asyncio.to_thread(check_and_create_index)
            logger.info("Fulltext index 'rag_fulltext_index' jest gotowy.")
        except Exception as exc:  # pragma: no cover - indeks nie jest krytyczny
            logger.warning("Nie uda≈Ço siƒô utworzyƒá indeksu fulltext: %s", exc)

    async def _keyword_search(self, query: str, k: int = 5) -> List[Document]:
        """Wykonuje wyszukiwanie pe≈Çnotekstowe w Neo4j i zwraca dokumenty LangChain."""

        if not self.vector_store:
            return []

        # Lazy initialization fulltext index przy pierwszym u≈ºyciu
        if settings.RAG_USE_HYBRID_SEARCH and not self._fulltext_index_initialized:
            await self._ensure_fulltext_index()
            self._fulltext_index_initialized = True

        try:
            driver = self.vector_store._driver

            def search() -> List[Document]:
                with driver.session() as session:
                    def run_query(tx):
                        result = tx.run(
                            """
                            CALL db.index.fulltext.queryNodes('rag_fulltext_index', $search_query)
                            YIELD node, score
                            RETURN node.text AS text,
                                   node.doc_id AS doc_id,
                                   node.title AS title,
                                   node.chunk_index AS chunk_index,
                                   score
                            ORDER BY score DESC
                            LIMIT $limit
                            """,
                            search_query=query,
                            limit=k,
                        )
                        documents: List[Document] = []
                        for record in result:
                            documents.append(
                                Document(
                                    page_content=record["text"],
                                    metadata={
                                        "doc_id": record["doc_id"],
                                        "title": record["title"],
                                        "chunk_index": record["chunk_index"],
                                        "keyword_score": record["score"],
                                    },
                                )
                            )
                        return documents

                    return session.execute_read(run_query)

            documents = await asyncio.to_thread(search)
            logger.info("Keyword search zwr√≥ci≈Ço %s wynik√≥w", len(documents))
            return documents
        except Exception as exc:  # pragma: no cover - fallback do vector search
            logger.warning("Keyword search nie powiod≈Ço siƒô, u≈ºywam fallbacku: %s", exc)
            return []

    def _format_graph_context(self, graph_nodes: List[Dict[str, Any]]) -> str:
        """Formatuje wƒôz≈Çy grafu do czytelnego kontekstu tekstowego dla LLM.

        Args:
            graph_nodes: Lista wƒôz≈Ç√≥w z grafu z w≈Ça≈õciwo≈õciami

        Returns:
            Sformatowany string z strukturalnƒÖ wiedzƒÖ z grafu
        """
        if not graph_nodes:
            return ""

        # Grupuj wƒôz≈Çy po typie
        indicators = [n for n in graph_nodes if n.get('type') == 'Wskaznik']
        observations = [n for n in graph_nodes if n.get('type') == 'Obserwacja']
        trends = [n for n in graph_nodes if n.get('type') == 'Trend']
        demographics = [n for n in graph_nodes if n.get('type') == 'Demografia']

        sections = []

        # Sekcja Wska≈∫niki
        if indicators:
            sections.append("üìä WSKA≈πNIKI DEMOGRAFICZNE (Wskaznik):\n")
            for ind in indicators:
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = ind.get('streszczenie') or ind.get('summary', 'Brak podsumowania')
                skala = ind.get('skala') or ind.get('magnitude', 'N/A')
                pewnosc = ind.get('pewnosc') or ind.get('confidence_level', 'N/A')
                kluczowe_fakty = ind.get('kluczowe_fakty') or ind.get('key_facts', '')
                okres_czasu = ind.get('okres_czasu') or ind.get('time_period', '')

                sections.append(f"‚Ä¢ {streszczenie}")
                if skala and skala != 'N/A':
                    sections.append(f"  Wielko≈õƒá: {skala}")
                if okres_czasu:
                    sections.append(f"  Okres: {okres_czasu}")
                sections.append(f"  Pewno≈õƒá: {pewnosc}")
                if kluczowe_fakty:
                    sections.append(f"  Kluczowe fakty: {kluczowe_fakty}")
                sections.append("")

        # Sekcja Obserwacje
        if observations:
            sections.append("\nüë• OBSERWACJE DEMOGRAFICZNE (Obserwacja):\n")
            for obs in observations:
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = obs.get('streszczenie') or obs.get('summary', 'Brak podsumowania')
                pewnosc = obs.get('pewnosc') or obs.get('confidence_level', 'N/A')
                kluczowe_fakty = obs.get('kluczowe_fakty') or obs.get('key_facts', '')
                okres_czasu = obs.get('okres_czasu') or obs.get('time_period', '')

                sections.append(f"‚Ä¢ {streszczenie}")
                sections.append(f"  Pewno≈õƒá: {pewnosc}")
                if okres_czasu:
                    sections.append(f"  Okres: {okres_czasu}")
                if kluczowe_fakty:
                    sections.append(f"  Kluczowe fakty: {kluczowe_fakty}")
                sections.append("")

        # Sekcja Trendy
        if trends:
            sections.append("\nüìà TRENDY DEMOGRAFICZNE (Trend):\n")
            for trend in trends:
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = trend.get('streszczenie') or trend.get('summary', 'Brak podsumowania')
                okres_czasu = trend.get('okres_czasu') or trend.get('time_period', 'N/A')
                kluczowe_fakty = trend.get('kluczowe_fakty') or trend.get('key_facts', '')

                sections.append(f"‚Ä¢ {streszczenie}")
                sections.append(f"  Okres: {okres_czasu}")
                if kluczowe_fakty:
                    sections.append(f"  Kluczowe fakty: {kluczowe_fakty}")
                sections.append("")

        # Sekcja Demografia
        if demographics:
            sections.append("\nüéØ GRUPY DEMOGRAFICZNE (Demografia):\n")
            for demo in demographics:
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = demo.get('streszczenie') or demo.get('summary', 'Brak podsumowania')
                pewnosc = demo.get('pewnosc') or demo.get('confidence_level', 'N/A')
                kluczowe_fakty = demo.get('kluczowe_fakty') or demo.get('key_facts', '')

                sections.append(f"‚Ä¢ {streszczenie}")
                sections.append(f"  Pewno≈õƒá: {pewnosc}")
                if kluczowe_fakty:
                    sections.append(f"  Kluczowe fakty: {kluczowe_fakty}")
                sections.append("")

        return "\n".join(sections)

    def _rerank_with_cross_encoder(
        self,
        query: str,
        candidates: List[Tuple[Document, float]],
        top_k: int = 5
    ) -> List[Tuple[Document, float]]:
        """U≈ºyj cross-encoder aby precyzyjnie re-score query-document pairs.

        Cross-encoder ma attention mechanism kt√≥ry widzi query i document razem,
        co daje lepszƒÖ precision ni≈º bi-encoder (u≈ºywany w vector search).

        Args:
            query: Query u≈ºytkownika
            candidates: Lista (Document, RRF_score) par z RRF fusion
            top_k: Liczba top wynik√≥w do zwr√≥cenia

        Returns:
            Lista (Document, rerank_score) sorted by rerank_score descending
        """
        if not self.reranker or not candidates:
            logger.info("Reranker niedostƒôpny lub brak candidates - skip reranking")
            return candidates[:top_k]

        try:
            # Przygotuj pary (query, document) dla cross-encoder
            pairs = [(query, doc.page_content[:512]) for doc, _ in candidates]
            # Limit do 512 znak√≥w dla cross-encoder (max_length)

            # Cross-encoder prediction (sync, ale szybkie ~100-200ms dla 20 docs)
            scores = self.reranker.predict(pairs)

            # Po≈ÇƒÖcz dokumenty z nowymi scores
            reranked = list(zip([doc for doc, _ in candidates], scores))

            # Sortuj po cross-encoder score (descending)
            reranked.sort(key=lambda x: x[1], reverse=True)

            logger.info(
                "Reranking completed: %s candidates ‚Üí top %s results",
                len(candidates),
                top_k
            )

            return reranked[:top_k]

        except Exception as exc:
            logger.error("Reranking failed: %s - fallback to RRF ranking", exc)
            return candidates[:top_k]

    def _find_related_graph_nodes(
        self,
        chunk_doc: Document,
        graph_nodes: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Znajd≈∫ graph nodes kt√≥re sƒÖ powiƒÖzane z danym chunkiem.

        Matching bazuje na:
        1. Wsp√≥lnych s≈Çowach kluczowych (z summary/key_facts)
        2. Dokumencie ≈∫r√≥d≈Çowym (doc_id)

        Args:
            chunk_doc: Document chunk z vector/keyword search
            graph_nodes: Lista graph nodes z get_demographic_graph_context()

        Returns:
            Lista graph nodes kt√≥re sƒÖ powiƒÖzane z chunkiem
        """
        if not graph_nodes:
            return []

        related = []
        chunk_text = chunk_doc.page_content.lower()
        chunk_doc_id = chunk_doc.metadata.get('doc_id', '')

        for node in graph_nodes:
            # Sprawd≈∫ czy node pochodzi z tego samego dokumentu
            node_doc_id = node.get('doc_id', '')
            if node_doc_id and node_doc_id == chunk_doc_id:
                related.append(node)
                continue

            # Sprawd≈∫ overlap s≈Ç√≥w kluczowych
            # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
            summary = (node.get('streszczenie') or node.get('summary', '') or '').lower()
            key_facts = (node.get('kluczowe_fakty') or node.get('key_facts', '') or '').lower()

            # Ekstraktuj s≈Çowa kluczowe (> 5 chars)
            summary_words = {w for w in summary.split() if len(w) > 5}
            key_facts_words = {w for w in key_facts.split() if len(w) > 5}
            node_keywords = summary_words | key_facts_words

            # Policz overlap
            matches = sum(1 for keyword in node_keywords if keyword in chunk_text)

            # Je≈õli >=2 matching keywords, uznaj za related
            if matches >= 2:
                related.append(node)

        return related

    def _enrich_chunk_with_graph(
        self,
        chunk_text: str,
        related_nodes: List[Dict[str, Any]]
    ) -> str:
        """Wzbogaƒá chunk o powiƒÖzane graph nodes w naturalny spos√≥b.

        Args:
            chunk_text: Oryginalny tekst chunku
            related_nodes: PowiƒÖzane graph nodes

        Returns:
            Enriched chunk text z embedded graph context
        """
        if not related_nodes:
            return chunk_text

        # Grupuj nodes po typie
        indicators = [n for n in related_nodes if n.get('type') == 'Wskaznik']
        observations = [n for n in related_nodes if n.get('type') == 'Obserwacja']
        trends = [n for n in related_nodes if n.get('type') == 'Trend']

        enrichments = []

        # Dodaj wska≈∫niki
        if indicators:
            enrichments.append("\nüí° PowiƒÖzane wska≈∫niki:")
            for ind in indicators[:2]:  # Max 2 na chunk
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = ind.get('streszczenie') or ind.get('summary', '')
                skala = ind.get('skala') or ind.get('magnitude', '')
                if streszczenie:
                    if skala:
                        enrichments.append(f"  ‚Ä¢ {streszczenie} ({skala})")
                    else:
                        enrichments.append(f"  ‚Ä¢ {streszczenie}")

        # Dodaj obserwacje
        if observations:
            enrichments.append("\nüîç PowiƒÖzane obserwacje:")
            for obs in observations[:2]:  # Max 2 na chunk
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = obs.get('streszczenie') or obs.get('summary', '')
                if streszczenie:
                    enrichments.append(f"  ‚Ä¢ {streszczenie}")

        # Dodaj trendy
        if trends:
            enrichments.append("\nüìà PowiƒÖzane trendy:")
            for trend in trends[:1]:  # Max 1 na chunk
                # Backward compatibility: u≈ºywaj nowych nazw z fallbackiem na stare
                streszczenie = trend.get('streszczenie') or trend.get('summary', '')
                okres_czasu = trend.get('okres_czasu') or trend.get('time_period', '')
                if streszczenie:
                    if okres_czasu:
                        enrichments.append(f"  ‚Ä¢ {streszczenie} ({okres_czasu})")
                    else:
                        enrichments.append(f"  ‚Ä¢ {streszczenie}")

        if enrichments:
            return chunk_text + "\n" + "\n".join(enrichments)
        else:
            return chunk_text

    async def hybrid_search(
        self,
        query: str,
        top_k: int = 5
    ) -> List[Document]:
        """Wykonuje hybrydowe wyszukiwanie (vector + keyword + RRF fusion).

        Ta metoda ≈ÇƒÖczy wyszukiwanie semantyczne (embeddingi) i pe≈Çnotekstowe (keywords)
        u≈ºywajƒÖc Reciprocal Rank Fusion do po≈ÇƒÖczenia wynik√≥w.

        Args:
            query: Zapytanie tekstowe do wyszukania
            top_k: Liczba wynik√≥w do zwr√≥cenia (domy≈õlnie 5)

        Returns:
            Lista Document obiekt√≥w posortowana po relevance score

        Raises:
            RuntimeError: Je≈õli vector store nie jest dostƒôpny
        """
        if not self.vector_store:
            raise RuntimeError("Vector store niedostƒôpny - hybrid search niemo≈ºliwy")

        logger.info("Hybrid search: query='%s...', top_k=%s", query[:50], top_k)

        try:
            # HYBRID SEARCH (Vector + Keyword)
            if settings.RAG_USE_HYBRID_SEARCH:
                # Zwiƒôkszamy k aby mieƒá wiƒôcej candidates dla reranking
                candidates_k = settings.RAG_RERANK_CANDIDATES if settings.RAG_USE_RERANKING else top_k * 2

                # Vector search
                vector_results = await self.vector_store.asimilarity_search_with_score(
                    query,
                    k=candidates_k,
                )

                # Keyword search
                keyword_results = await self._keyword_search(
                    query,
                    k=candidates_k,
                )

                # RRF fusion
                fused_results = self._rrf_fusion(
                    vector_results,
                    keyword_results,
                    k=settings.RAG_RRF_K,
                )

                # Optional reranking
                if settings.RAG_USE_RERANKING and self.reranker:
                    logger.info("Applying cross-encoder reranking")
                    final_results = self._rerank_with_cross_encoder(
                        query=query,
                        candidates=fused_results[:settings.RAG_RERANK_CANDIDATES],
                        top_k=top_k
                    )
                else:
                    final_results = fused_results[:top_k]
            else:
                # Vector-only search
                final_results = await self.vector_store.asimilarity_search_with_score(
                    query,
                    k=top_k,
                )

            # Return only Documents (strip scores)
            documents = [doc for doc, score in final_results]
            logger.info("Hybrid search returned %s documents", len(documents))
            return documents

        except Exception as exc:
            logger.error("Hybrid search failed: %s", exc, exc_info=True)
            raise RuntimeError(f"Hybrid search failed: {exc}")

    async def get_demographic_insights(
        self,
        age_group: str,
        education: str,
        location: str,
        gender: str,
    ) -> Dict[str, Any]:
        """Buduje kontekst raportowy dla wskazanego profilu demograficznego.

        ≈ÅƒÖczy trzy ≈∫r√≥d≈Ça kontekstu:
        1. **Graph RAG** - Strukturalna wiedza z grafu (Indicators, Observations, Trends)
        2. **Vector Search** - Semantyczne wyszukiwanie w embeddingach
        3. **Keyword Search** - Leksykalne wyszukiwanie fulltext (opcjonalnie)

        Returns:
            Dict z kluczami:
            - context: Pe≈Çny kontekst (graph + chunks tekstowe)
            - graph_context: Sformatowany kontekst z grafu (string)
            - graph_nodes: Surowe wƒôz≈Çy grafu (list)
            - citations: Citations z hybrid search
            - query: Query u≈ºyte do wyszukiwania
            - num_results: Liczba wynik√≥w z hybrid search
            - search_type: "hybrid+graph" | "vector_only+graph" | "hybrid" | "vector_only"
        """

        if not self.vector_store:
            logger.warning("Vector store niedostƒôpny ‚Äì zwracam pusty kontekst.")
            return {"context": "", "citations": [], "query": "", "num_results": 0}

        query = (
            f"Profil demograficzny: {gender}, wiek {age_group}, wykszta≈Çcenie {education}, "
            f"lokalizacja {location} w Polsce. Jakie sƒÖ typowe cechy, warto≈õci, zainteresowania, "
            f"style ≈ºycia oraz aspiracje dla tej grupy?"
        )

        logger.info(
            "RAG hybrid search + Graph RAG dla profilu: wiek=%s, edukacja=%s, lokalizacja=%s, p≈Çeƒá=%s",
            age_group,
            education,
            location,
            gender,
        )

        try:
            # 1. GRAPH RAG - Pobierz strukturalny kontekst z grafu wiedzy
            graph_nodes = []
            graph_context_formatted = ""
            try:
                graph_nodes = self.rag_doc_service.get_demographic_graph_context(
                    age_group=age_group,
                    location=location,
                    education=education,
                    gender=gender
                )
                if graph_nodes:
                    graph_context_formatted = self._format_graph_context(graph_nodes)
                    logger.info("Pobrano %s wƒôz≈Ç√≥w grafu z kontekstem demograficznym", len(graph_nodes))
                else:
                    logger.info("Brak wynik√≥w z graph context dla podanego profilu")
            except Exception as graph_exc:
                logger.warning("Nie uda≈Ço siƒô pobraƒá graph context: %s", graph_exc)

            # 2. HYBRID SEARCH (Vector + Keyword) - Pobierz chunki tekstowe
            if settings.RAG_USE_HYBRID_SEARCH:
                # Zwiƒôkszamy k aby mieƒá wiƒôcej candidates dla reranking
                candidates_k = settings.RAG_RERANK_CANDIDATES if settings.RAG_USE_RERANKING else settings.RAG_TOP_K * 2

                vector_results = await self.vector_store.asimilarity_search_with_score(
                    query,
                    k=candidates_k,
                )
                keyword_results = await self._keyword_search(
                    query,
                    k=candidates_k,
                )
                fused_results = self._rrf_fusion(
                    vector_results,
                    keyword_results,
                    k=settings.RAG_RRF_K,
                )

                # 2b. RERANKING (opcjonalne) - Precyzyjny re-scoring z cross-encoder
                if settings.RAG_USE_RERANKING and self.reranker:
                    logger.info("Applying cross-encoder reranking on top %s candidates", len(fused_results))
                    final_results = self._rerank_with_cross_encoder(
                        query=query,
                        candidates=fused_results[:settings.RAG_RERANK_CANDIDATES],
                        top_k=settings.RAG_TOP_K
                    )
                    search_type = "hybrid+rerank+graph" if graph_nodes else "hybrid+rerank"
                else:
                    final_results = fused_results[:settings.RAG_TOP_K]
                    search_type = "hybrid+graph" if graph_nodes else "hybrid"
            else:
                final_results = await self.vector_store.asimilarity_search_with_score(
                    query,
                    k=settings.RAG_TOP_K,
                )
                search_type = "vector_only+graph" if graph_nodes else "vector_only"

            # 3. UNIFIED CONTEXT - Wzbogaƒá chunki o powiƒÖzane graph nodes
            context_chunks: List[str] = []
            citations: List[Dict[str, Any]] = []
            enriched_chunks_count = 0

            # Dodaj graph context na poczƒÖtku (je≈õli istnieje)
            if graph_context_formatted:
                context_chunks.append("=== STRUKTURALNA WIEDZA Z GRAFU WIEDZY ===\n")
                context_chunks.append(graph_context_formatted)
                context_chunks.append("\n=== KONTEKST Z DOKUMENT√ìW (WZBOGACONY) ===\n")

            # Dodaj chunki tekstowe WZBOGACONE o powiƒÖzane graph nodes
            for doc, score in final_results:
                # Znajd≈∫ graph nodes powiƒÖzane z tym chunkiem
                related_nodes = self._find_related_graph_nodes(doc, graph_nodes)

                # Wzbogaƒá chunk je≈õli sƒÖ related nodes
                if related_nodes:
                    enriched_text = self._enrich_chunk_with_graph(
                        chunk_text=doc.page_content,
                        related_nodes=related_nodes
                    )
                    enriched_chunks_count += 1
                else:
                    enriched_text = doc.page_content

                # Truncate je≈õli za d≈Çugi
                if len(enriched_text) > 1000:
                    enriched_text = enriched_text[:1000] + "\n[...fragment obciƒôty...]"

                context_chunks.append(enriched_text)
                # Format zgodny z RAGCitation schema (app/schemas/rag.py)
                citations.append(
                    {
                        "document_title": doc.metadata.get("title", "Unknown Document"),
                        "chunk_text": doc.page_content[:500],  # Original dla citation
                        "relevance_score": float(score),
                    }
                )

            if enriched_chunks_count > 0:
                logger.info(
                    "Unified context: %s/%s chunks enriched with graph nodes",
                    enriched_chunks_count,
                    len(final_results)
                )

            context = "\n\n---\n\n".join(context_chunks)
            if len(context) > settings.RAG_MAX_CONTEXT_CHARS:
                context = context[: settings.RAG_MAX_CONTEXT_CHARS] + "\n\n[... kontekst obciƒôty]"

            return {
                "context": context,
                "graph_context": graph_context_formatted,
                "graph_nodes": graph_nodes,
                "citations": citations,
                "query": query,
                "num_results": len(final_results),
                "search_type": search_type,
            }
        except Exception as exc:  # pragma: no cover - zwracamy pusty kontekst
            logger.error("Hybrydowe wyszukiwanie nie powiod≈Ço siƒô: %s", exc, exc_info=True)
            return {"context": "", "citations": [], "query": query, "num_results": 0}

    def _rrf_fusion(
        self,
        vector_results: List[Tuple[Document, float]],
        keyword_results: List[Document],
        k: int = 60,
    ) -> List[Tuple[Document, float]]:
        """≈ÅƒÖczy wyniki vector i keyword search przy pomocy Reciprocal Rank Fusion."""

        scores: Dict[int, float] = {}
        doc_map: Dict[int, Tuple[Document, float]] = {}

        for rank, (doc, original_score) in enumerate(vector_results):
            doc_hash = hash(doc.page_content)
            scores[doc_hash] = scores.get(doc_hash, 0.0) + 1.0 / (k + rank + 1)
            doc_map[doc_hash] = (doc, original_score)

        for rank, doc in enumerate(keyword_results):
            doc_hash = hash(doc.page_content)
            scores[doc_hash] = scores.get(doc_hash, 0.0) + 1.0 / (k + rank + 1)
            if doc_hash not in doc_map:
                doc_map[doc_hash] = (doc, 0.0)

        fused = sorted(scores.items(), key=lambda item: item[1], reverse=True)
        return [(doc_map[doc_hash][0], fused_score) for doc_hash, fused_score in fused]