"""
Serwis Grup Fokusowych oparty na LangChain

ZarzƒÖdza symulacjami grup fokusowych przy u≈ºyciu Google Gemini.
Osoby (persony) odpowiadajƒÖ na pytania r√≥wnolegle z wykorzystaniem
kontekstu rozmowy przechowywanego w MemoryService.

Wydajno≈õƒá: <3s na odpowied≈∫ persony, <30s ca≈Çkowity czas wykonania
"""

import logging
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import asyncio
import time
from datetime import datetime, timezone
from uuid import UUID

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate

from app.models import FocusGroup, Persona, PersonaResponse
from app.services.memory_service_langchain import MemoryServiceLangChain
from app.db import AsyncSessionLocal
from app.core.config import get_settings

settings = get_settings()


class FocusGroupServiceLangChain:
    """
    ZarzƒÖdzanie symulacjami grup fokusowych

    Orkiestruje dyskusje miƒôdzy personami, zarzƒÖdza kontekstem rozmowy
    i przetwarza odpowiedzi r√≥wnolegle dla maksymalnej wydajno≈õci.
    """

    def __init__(self):
        """Inicjalizuj serwis z LangChain LLM i serwisem pamiƒôci"""
        self.settings = settings
        self.memory_service = MemoryServiceLangChain()

        # Inicjalizujemy model Gemini w LangChain
        # Uwaga: modele gemini-2.5 zu≈ºywajƒÖ wiƒôcej token√≥w na wnioskowanie, wiƒôc podnosimy limit
        self.llm = ChatGoogleGenerativeAI(
            model=settings.DEFAULT_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=settings.TEMPERATURE,
            max_tokens=2048,  # Wiƒôksza liczba token√≥w na potrzeby rozumowania gemini-2.5
        )

    async def run_focus_group(
        self, db: AsyncSession, focus_group_id: str
    ) -> Dict[str, Any]:
        """
        Wykonaj symulacjƒô grupy fokusowej przy u≈ºyciu LangChain

        G≈Ç√≥wna metoda orkiestrujƒÖca przebieg grupy fokusowej:
        1. ≈Åaduje grupƒô fokusowƒÖ i persony z bazy danych
        2. Dla ka≈ºdego pytania, r√≥wnolegle zbiera odpowiedzi od wszystkich person
        3. Zapisuje odpowiedzi do bazy i tworzy eventy w systemie pamiƒôci
        4. Oblicza metryki wydajno≈õci (czas wykonania, ≈õredni czas odpowiedzi)
        5. Aktualizuje status grupy fokusowej

        Args:
            db: Sesja asynchroniczna do bazy danych
            focus_group_id: UUID grupy fokusowej do wykonania

        Returns:
            S≈Çownik z wynikami:
            {
                "focus_group_id": str,
                "status": "completed" | "failed",
                "responses": List[Dict],  # odpowiedzi na ka≈ºde pytanie
                "metrics": {
                    "total_execution_time_ms": float,
                    "avg_response_time_ms": float,
                    "meets_requirements": bool
                }
            }
        """
        logger = logging.getLogger(__name__)

        logger.info(f"üöÄ Starting focus group {focus_group_id}")
        start_time = time.time()

        # Pobieramy grupƒô fokusowƒÖ z bazy danych
        result = await db.execute(
            select(FocusGroup).where(FocusGroup.id == focus_group_id)
        )
        focus_group = result.scalar_one()
        logger.info(f"üìã Focus group loaded: {focus_group.name}, questions: {len(focus_group.questions)}")

        # Aktualizujemy status grupy
        focus_group.status = "running"
        focus_group.started_at = datetime.now(timezone.utc)
        await db.commit()

        try:
            # Pobieramy persony
            personas = await self._load_personas(db, focus_group.persona_ids)
            logger.info(f"üë• Loaded {len(personas)} personas")

            # Przetwarzamy ka≈ºde pytanie z listy
            all_responses = []
            response_times = []

            print(f"üîÑ FOCUS GROUP: {len(focus_group.questions)} questions to process")
            for question in focus_group.questions:
                question_start = time.time()

                print(f"‚ùì PROCESSING QUESTION: {question} for {len(personas)} personas")
                logger.info(f"Processing question: {question} for {len(personas)} personas")

                # R√≥wnolegle pobieramy odpowiedzi od wszystkich person
                responses = await self._get_concurrent_responses(
                    personas, question, focus_group_id
                )

                print(f"‚úÖ GOT {len(responses)} RESPONSES for question")
                logger.info(f"Got {len(responses)} responses for question: {question}")

                question_time = (time.time() - question_start) * 1000
                response_times.append(question_time)

                all_responses.append(
                    {"question": question, "responses": responses, "time_ms": question_time}
                )

            # Wyliczamy metryki wykonywania
            total_time = (time.time() - start_time) * 1000
            avg_response_time = sum(response_times) / len(response_times)

            # Aktualizujemy rekord grupy fokusowej
            focus_group.status = "completed"
            focus_group.completed_at = datetime.now(timezone.utc)
            focus_group.total_execution_time_ms = int(total_time)
            focus_group.avg_response_time_ms = avg_response_time

            await db.commit()

            # Po zako≈Ñczeniu automatycznie budujemy graf wiedzy
            logger.info(f"üß† Starting automatic graph build for focus group {focus_group_id}")
            try:
                from app.services.graph_service import GraphService
                graph_service = GraphService()
                graph_stats = await graph_service.build_graph_from_focus_group(db, str(focus_group_id))
                await graph_service.close()
                logger.info(f"‚úÖ Graph built successfully: {graph_stats}")
            except Exception as graph_error:
                # B≈ÇƒÖd przy budowie grafu nie powinien zatrzymaƒá ca≈Çego procesu
                logger.error(f"‚ö†Ô∏è Graph build failed (non-critical): {graph_error}", exc_info=True)

            return {
                "focus_group_id": str(focus_group_id),
                "status": "completed",
                "responses": all_responses,
                "metrics": {
                    "total_execution_time_ms": total_time,
                    "avg_response_time_ms": avg_response_time,
                    "meets_requirements": focus_group.meets_performance_requirements(),
                },
            }

        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.error(
                f"Focus group {focus_group_id} failed: {str(e)}",
                exc_info=True,
                extra={"focus_group_id": str(focus_group_id)}
            )

            focus_group.status = "failed"
            # Zapisujemy pierwsze 500 znak√≥w b≈Çƒôdu do debugowania
            if hasattr(focus_group, 'error_message'):
                focus_group.error_message = str(e)[:500]
            await db.commit()

            # Nie propagujemy wyjƒÖtku dalej ‚Äì zadanie dzia≈Ça w tle, wystarczy log
            return {
                "focus_group_id": str(focus_group_id),
                "status": "failed",
                "error": str(e)
            }

    async def _load_personas(
        self, db: AsyncSession, persona_ids: List[UUID]
    ) -> List[Persona]:
        """
        Za≈Çaduj obiekty person z bazy danych

        Args:
            db: Sesja bazy danych
            persona_ids: Lista UUID person do za≈Çadowania

        Returns:
            Lista obiekt√≥w Persona
        """
        result = await db.execute(select(Persona).where(Persona.id.in_(persona_ids)))
        return result.scalars().all()

    async def _get_concurrent_responses(
        self,
        personas: List[Persona],
        question: str,
        focus_group_id: str,
    ) -> List[Dict[str, Any]]:
        """
        Pobierz odpowiedzi od wszystkich person r√≥wnolegle (concurrent execution)

        Tworzy zadania asynchroniczne dla ka≈ºdej persony i wykonuje je r√≥wnocze≈õnie
        u≈ºywajƒÖc asyncio.gather(). To pozwala na szybkie zbieranie odpowiedzi
        (wszystkie persony odpowiadajƒÖ "jednocze≈õnie" zamiast po kolei).

        Args:
            personas: Lista obiekt√≥w Persona do odpytania
            question: Pytanie do zadania
            focus_group_id: ID grupy fokusowej (do tworzenia event√≥w)

        Returns:
            Lista s≈Çownik√≥w z odpowiedziami:
            [
                {"persona_id": str, "response": str, "context_used": int},
                ...
            ]
            Je≈õli persona zwr√≥ci≈Ça b≈ÇƒÖd, zwraca {"persona_id": str, "response": "Error: ...", "error": True}
        """

        # Utw√≥rz zadania asynchroniczne dla ka≈ºdej persony
        tasks = [
            self._get_persona_response(persona, question, focus_group_id)
            for persona in personas
        ]

        # Wykonaj wszystkie zadania r√≥wnolegle (gather zbiera wyniki)
        print(f"üöÄ Starting {len(tasks)} concurrent persona response tasks...")
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # Obs≈Çu≈º wyjƒÖtki (gather mo≈ºe zwr√≥ciƒá Exception zamiast wyniku)
        results = []
        for i, resp in enumerate(responses):
            if isinstance(resp, Exception):
                print(f"‚ùå EXCEPTION in persona {personas[i].id}: {type(resp).__name__}: {str(resp)[:100]}")
                results.append(
                    {
                        "persona_id": str(personas[i].id),
                        "response": f"Error: {str(resp)}",
                        "error": True,
                    }
                )
            else:
                print(f"‚úì Persona {str(personas[i].id)[:8]}... response received")
                results.append(resp)

        return results

    async def _get_persona_response(
        self,
        persona: Persona,
        question: str,
        focus_group_id: str,
    ) -> Dict[str, Any]:
        """
        Pobierz odpowied≈∫ od pojedynczej persony

        Przep≈Çyw:
        1. Pobiera relevantny kontekst z systemu pamiƒôci (poprzednie odpowiedzi)
        2. Generuje odpowied≈∫ u≈ºywajƒÖc LLM (Gemini) z kontekstem
        3. Tworzy event w systemie pamiƒôci (event sourcing)
        4. Zapisuje odpowied≈∫ do tabeli persona_responses

        Args:
            persona: Obiekt persony odpowiadajƒÖcej
            question: Pytanie do odpowiedzi
            focus_group_id: ID grupy fokusowej

        Returns:
            S≈Çownik z odpowiedziƒÖ:
            {
                "persona_id": str,
                "response": str,  # tekst odpowiedzi
                "context_used": int  # ile event√≥w u≈ºyto jako kontekst
            }
        """
        # Pobieramy istotny kontekst z pamiƒôci (poprzednie interakcje)
        focus_group_uuid = focus_group_id if isinstance(focus_group_id, UUID) else UUID(str(focus_group_id))

        async with AsyncSessionLocal() as session:
            context = await self.memory_service.retrieve_relevant_context(
                session, str(persona.id), question, top_k=5
            )

            # Wygeneruj odpowied≈∫ u≈ºywajƒÖc LangChain + Gemini
            response_text = await self._generate_response(persona, question, context)

            print(f"üí¨ Generated response (length={len(response_text) if response_text else 0}): {response_text[:50] if response_text else 'EMPTY'}...")

            # Utw√≥rz event dla tej interakcji (event sourcing - niemodyfikowalny log)
            await self.memory_service.create_event(
                session,
                persona_id=str(persona.id),
                event_type="response_given",
                event_data={"question": question, "response": response_text},
                focus_group_id=str(focus_group_uuid),
            )

            # Zapisz odpowied≈∫ w bazie danych
            persona_response = PersonaResponse(
                persona_id=persona.id,
                focus_group_id=focus_group_uuid,
                question=question,
                response=response_text,
            )

            print(f"üíæ Adding PersonaResponse to db...")
            session.add(persona_response)
            try:
                await session.commit()
                print(f"‚úÖ Committed PersonaResponse to db")
            except Exception:
                await session.rollback()
                raise

        return {
            "persona_id": str(persona.id),
            "response": response_text,
            "context_used": len(context),
        }

    async def _generate_response(
        self, persona: Persona, question: str, context: List[Dict[str, Any]]
    ) -> str:
        """
        Wygeneruj odpowied≈∫ persony u≈ºywajƒÖc LangChain

        Tworzy prompt z danymi persony i kontekstem, wysy≈Ça do Gemini
        i zwraca odpowied≈∫ tekstowƒÖ.

        Args:
            persona: Obiekt persony z pe≈Çnymi danymi (demografia, osobowo≈õƒá, background)
            question: Pytanie do odpowiedzi
            context: Lista poprzednich interakcji (z retrieve_relevant_context)

        Returns:
            Tekst odpowiedzi wygenerowany przez LLM
        """
        # Utw√≥rz prompt z danych persony i kontekstu
        prompt_text = self._create_response_prompt(persona, question, context)

        logger = logging.getLogger(__name__)
        logger.info(f"Generating response for persona {persona.id}")
        logger.info(f"Persona data - name: {persona.full_name}, age: {persona.age}, occupation: {persona.occupation}")
        logger.info(f"Full prompt:\n{prompt_text}")

        response_text = await self._invoke_llm(prompt_text)

        if response_text:
            return response_text

        logger.warning(f"Empty response from LLM for persona {persona.id}, retrying with stricter prompt")
        retry_prompt = f"""{prompt_text}

IMPORTANT INSTRUCTION:
- Provide a natural, conversational answer of at least one full sentence.
- Do not return an empty string or placeholders.
- Stay in character as the persona described above.
"""
        response_text = await self._invoke_llm(retry_prompt)

        if response_text:
            return response_text

        fallback = self._fallback_response(persona, question)
        logger.warning(f"Using fallback response for persona {persona.id}")
        return fallback

    async def _invoke_llm(self, prompt_text: str) -> str:
        """Wywo≈Çaj model LLM i zwr√≥ƒá oczyszczony tekst odpowiedzi."""
        logger = logging.getLogger(__name__)
        try:
            result = await self.llm.ainvoke(prompt_text)
        except Exception as err:
            logger.error(f"LLM invocation failed: {err}")
            return ""

        content = getattr(result, "content", "")
        if isinstance(content, list):
            # Scal tekstowe fragmenty, je≈õli LangChain zwraca je w czƒô≈õciach
            content = " ".join(part.get("text", "") if isinstance(part, dict) else str(part) for part in content)

        text = content.strip() if isinstance(content, str) else ""
        if not text:
            logger.debug(f"LLM returned empty content object: {result}")
        return text

    def _fallback_response(self, persona: Persona, question: str) -> str:
        """Zwr√≥ƒá przygotowanƒÖ odpowied≈∫ zapasowƒÖ, gdy LLM nic nie wygeneruje."""
        lowered_question = question.lower()
        if "pizza" in lowered_question:
            return self._pizza_fallback_response(persona)

        name = (persona.full_name or "Ta persona").split(" ")[0]
        occupation = persona.occupation or "uczestnik badania"
        return (
            f"{name}, pracujƒÖc jako {occupation}, potrzebuje chwili, by uporzƒÖdkowaƒá my≈õli wok√≥≈Ç pytania "
            f"\"{question}\". Zaznacza jednak, ≈ºe chƒôtnie wr√≥ci do tematu, bo uwa≈ºa go za wa≈ºny dla ca≈Çej dyskusji."
        )

    def _pizza_fallback_response(self, persona: Persona) -> str:
        """Provide a personable pizza description fallback."""
        name = (persona.full_name or "Ta persona").split(" ")[0]
        occupation = persona.occupation or "uczestnik badania"
        location = persona.location

        values = [v.lower() for v in (persona.values or []) if isinstance(v, str)]
        interests = [i.lower() for i in (persona.interests or []) if isinstance(i, str)]

        def has_any(options):
            return any(opt in values or opt in interests for opt in options)

        if has_any({"health", "wellness", "fitness", "yoga", "running", "sport"}):
            style = "lekkƒÖ pizzƒô verde na bardzo cienkim cie≈õcie z rukolƒÖ, grillowanymi warzywami i oliwƒÖ truflowƒÖ"
            reason = "dziƒôki kt√≥rej mo≈ºe zje≈õƒá co≈õ pysznego i wciƒÖ≈º trzymaƒá siƒô swoich zdrowych nawyk√≥w"
        elif has_any({"travel", "adventure", "exploration", "innovation", "spice"}):
            style = "pikantnƒÖ pizzƒô diavola z dojrzewajƒÖcym salami, jalape√±o i odrobinƒÖ miodu"
            reason = "bo lubi kuchenne eksperymenty i wyra≈∫ne smaki, kt√≥re dodajƒÖ energii"
        elif has_any({"family", "tradition", "comfort", "home"}):
            style = "klasycznƒÖ pizzƒô margheritƒô na neapolita≈Ñskim cie≈õcie"
            reason = "kt√≥ra kojarzy mu siƒô z domowym ciep≈Çem i prostymi przyjemno≈õciami"
        elif has_any({"food", "culinary", "gourmet", "wine"}):
            style = "wyrafinowanƒÖ pizzƒô bianca z ricottƒÖ, ≈õwie≈ºym szpinakiem i odrobinƒÖ cytrynowej sk√≥rki"
            reason = "bo docenia subtelne po≈ÇƒÖczenia smak√≥w i dobrƒÖ jako≈õƒá sk≈Çadnik√≥w"
        else:
            style = "aromatycznƒÖ pizzƒô capricciosa z szynkƒÖ, karczochami i pieczarkami"
            reason = "kt√≥ra zapewnia idealnƒÖ r√≥wnowagƒô miƒôdzy klasykƒÖ a urozmaiceniem"

        location_note = f", a w {location} ≈Çatwo znale≈∫ƒá rzemie≈õlniczƒÖ pizzeriƒô, kt√≥ra spe≈Çnia te oczekiwania" if location else ""

        return (
            f"{name}, pracujƒÖc jako {occupation}, najczƒô≈õciej wybiera {style}. "
            f"M√≥wi, ≈ºe lubi jƒÖ {reason}{location_note}."
        )

    def _create_response_prompt(
        self, persona: Persona, question: str, context: List[Dict[str, Any]]
    ) -> str:
        """
        Utw√≥rz prompt dla generowania odpowiedzi persony

        Buduje szczeg√≥≈Çowy prompt zawierajƒÖcy:
        - Dane demograficzne persony (wiek, p≈Çeƒá, zaw√≥d, etc.)
        - Cechy osobowo≈õci (warto≈õci, zainteresowania)
        - Fragment historii ≈ºyciowej
        - Kontekst poprzednich interakcji (je≈õli istniejƒÖ)
        - Aktualne pytanie

        Args:
            persona: Obiekt persony
            question: Pytanie do odpowiedzi
            context: Lista istotnych poprzednich interakcji (z event sourcing)

        Returns:
            Pe≈Çny prompt gotowy do wys≈Çania do LLM
        """

        # Formatuj kontekst poprzednich odpowiedzi (maksymalnie 3 najbardziej istotne)
        context_text = ""
        if context:
            context_text = "\n\nPast interactions:\n"
            for i, ctx in enumerate(context[:3], 1):  # Ograniczamy do 3 najwa≈ºniejszych wpis√≥w
                if ctx["event_type"] == "response_given":
                    context_text += f"{i}. Q: {ctx['event_data'].get('question', '')}\n"
                    context_text += f"   A: {ctx['event_data'].get('response', '')}\n"

        # Przycinamy historiƒô t≈Ça do 300 znak√≥w (oszczƒôdno≈õƒá token√≥w)
        background = persona.background_story[:300] if persona.background_story else 'Has diverse life experiences'

        return f"""You are participating in a focus group discussion.

PERSONA DETAILS:
Name: {persona.full_name or 'Participant'}
Age: {persona.age}, Gender: {persona.gender}
Occupation: {persona.occupation or 'Professional'}
Education: {persona.education_level or 'Educated'}
Location: {persona.location or 'Urban area'}

PERSONALITY:
Values: {', '.join(persona.values[:4]) if persona.values else 'Balanced approach to life'}
Interests: {', '.join(persona.interests[:4]) if persona.interests else 'Various interests'}

BACKGROUND:
{background}
{context_text}

QUESTION: {question}

Respond naturally as this person would in 2-4 sentences. Be authentic and conversational."""
