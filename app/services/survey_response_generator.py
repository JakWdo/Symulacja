"""
Serwis Generowania Odpowiedzi na Ankiety

Generuje odpowiedzi syntetycznych person na pytania ankietowe u≈ºywajƒÖc Google Gemini.
Odpowiedzi sƒÖ zgodne z profilami psychologicznymi i demograficznymi person.

Wydajno≈õƒá: Przetwarzanie r√≥wnoleg≈Çe dla szybkiego generowania odpowiedzi
"""

import logging
import re
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import asyncio
import time
from datetime import datetime, timezone
from uuid import UUID
from collections import Counter
import statistics

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate

from app.models import Survey, Persona, SurveyResponse
from app.schemas.survey import QuestionAnalytics
from app.db import AsyncSessionLocal
from app.core.config import get_settings

settings = get_settings()


class SurveyResponseGenerator:
    """
    Generowanie odpowiedzi person na ankiety

    Wykorzystuje AI do generowania realistycznych odpowiedzi bazujƒÖc na:
    - Demografii persony (wiek, p≈Çeƒá, wykszta≈Çcenie, doch√≥d)
    - Cechach osobowo≈õci (Big Five)
    - Wymiarach kulturowych (Hofstede)
    - Historii i tle persony
    """

    def __init__(self):
        """Inicjalizuj serwis z LangChain LLM"""
        self.settings = settings

        # Inicjalizujemy model Gemini w LangChain
        self.llm = ChatGoogleGenerativeAI(
            model=settings.DEFAULT_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=settings.TEMPERATURE,
            max_tokens=1024,  # Kr√≥tsze odpowiedzi na potrzeby ankiet
        )

    async def generate_responses(
        self, db: AsyncSession, survey_id: str
    ) -> Dict[str, Any]:
        """
        Generuj odpowiedzi wszystkich person na ankietƒô

        G≈Ç√≥wna metoda orkiestrujƒÖca generowanie odpowiedzi:
        1. ≈Åaduje ankietƒô i persony projektu
        2. Dla ka≈ºdej persony generuje odpowiedzi na wszystkie pytania
        3. Zapisuje odpowiedzi do bazy danych
        4. Oblicza metryki wydajno≈õci
        5. Aktualizuje status ankiety

        Args:
            db: Sesja asynchroniczna do bazy danych
            survey_id: UUID ankiety

        Returns:
            S≈Çownik z wynikami wykonania
        """
        logger = logging.getLogger(__name__)

        logger.info(f"üìä Starting survey {survey_id}")
        start_time = time.time()

        # Pobieramy ankietƒô z bazy
        result = await db.execute(
            select(Survey).where(Survey.id == survey_id)
        )
        survey = result.scalar_one()
        logger.info(f"üìã Survey loaded: {survey.title}, questions: {len(survey.questions)}")

        # Aktualizujemy status ankiety
        survey.status = "running"
        survey.started_at = datetime.now(timezone.utc)
        await db.commit()

        try:
            # Pobieramy persony przypisane do projektu
            personas = await self._load_project_personas(db, survey.project_id)
            logger.info(f"üë• Loaded {len(personas)} personas from project")

            if len(personas) == 0:
                raise ValueError("Project has no personas. Generate personas first.")

            # Generujemy odpowiedzi r√≥wnolegle dla wszystkich person
            logger.info(f"üîÑ Generating responses for {len(personas)} personas...")
            response_times = []

            tasks = [
                self._generate_persona_survey_response(persona, survey)
                for persona in personas
            ]
            persona_results = await asyncio.gather(*tasks, return_exceptions=True)

            # Zbieramy czasy odpowiedzi (pomijajƒÖc b≈Çƒôdy)
            for result in persona_results:
                if isinstance(result, dict) and "response_time_ms" in result:
                    response_times.append(result["response_time_ms"])

            # Wyliczamy metryki wydajno≈õci
            total_time = (time.time() - start_time) * 1000
            actual_responses = len([r for r in persona_results if not isinstance(r, Exception)])
            metrics = self._calculate_metrics(actual_responses, total_time)

            if response_times:
                metrics["avg_response_time_ms"] = sum(response_times) / len(response_times)
                metrics["total_execution_time_ms"] = total_time

            # Zapisujemy metryki w rekordzie ankiety
            survey.status = "completed"
            survey.completed_at = datetime.now(timezone.utc)
            survey.total_execution_time_ms = int(metrics["total_execution_time_ms"])
            survey.avg_response_time_ms = int(metrics["avg_response_time_ms"])
            survey.actual_responses = metrics["total_responses"]

            await db.commit()

            logger.info(f"‚úÖ Survey completed: {actual_responses} responses in {total_time:.0f}ms")

            return {
                "survey_id": str(survey_id),
                "status": "completed",
                "actual_responses": actual_responses,
                "metrics": metrics,
            }

        except Exception as e:
            logger.error(f"Survey {survey_id} failed: {str(e)}", exc_info=True)

            survey.status = "failed"
            await db.commit()

            return {
                "survey_id": str(survey_id),
                "status": "failed",
                "error": str(e)
            }

    async def _load_project_personas(
        self, db: AsyncSession, project_id: UUID
    ) -> List[Persona]:
        """Za≈Çaduj wszystkie persony projektu"""
        result = await db.execute(
            select(Persona).where(
                Persona.project_id == project_id,
                Persona.is_active == True
            )
        )
        return result.scalars().all()

    def _normalize_question_type(self, question_type: Optional[str]) -> str:
        """Znormalizuj typ pytania do sp√≥jnego formatu wewnƒôtrznego."""

        if not question_type:
            return "open_ended"

        normalized = question_type.lower().replace("-", "_")
        mapping = {
            "rating_scale": "scale",
            "open_text": "open_ended",
        }
        return mapping.get(normalized, normalized)

    def _get_question_text(self, question: Dict[str, Any]) -> str:
        """Zwr√≥ƒá opis pytania niezale≈ºnie od struktury s≈Çownika."""

        return (
            question.get("text")
            or question.get("title")
            or question.get("question")
            or ""
        )

    def _create_survey_prompt(
        self, persona: Persona, question: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        Zbuduj kompletny prompt przekazywany do modelu jƒôzykowego.

        Funkcja scala kontekst persony z tre≈õciƒÖ pytania, aby model m√≥g≈Ç
        odpowiedzieƒá w spos√≥b sp√≥jny z jej profilem psychograficznym. W zale≈ºno≈õci
        od typu pytania (skala, jednokrotny wyb√≥r, wielokrotny wyb√≥r, odpowied≈∫
        otwarta) generowane sƒÖ r√≥wnie≈º szczeg√≥≈Çowe instrukcje dla modelu oraz lista
        dostƒôpnych opcji. Dziƒôki temu ka≈ºda kolejna metoda odpowiadajƒÖca za
        wygenerowanie konkretnej odpowiedzi otrzymuje ustandaryzowanƒÖ strukturƒô
        danych niezale≈ºnie od ≈∫r√≥d≈Çowego formatu pytania w bazie.

        Args:
            persona: Obiekt persony, z kt√≥rego budujemy kontekst (cechy demograficzne
                i psychograficzne wykorzystywane w promptach).
            question: S≈Çownik opisujƒÖcy pytanie ankietowe zawierajƒÖcy m.in. `id`,
                `type`, `text/title`, `description`, listƒô `options` oraz ewentualne
                parametry skali (`scaleMin`, `scaleMax`).

        Returns:
            Lista s≈Çownik√≥w w formacie oczekiwanym przez LangChain/Google Gemini,
            reprezentujƒÖca sekwencjƒô wiadomo≈õci systemowych i u≈ºytkownika tworzƒÖcych
            kontekst rozmowy.
        """

        persona_context = self._build_persona_context(persona)
        question_type = self._normalize_question_type(question.get("type"))
        question_text = self._get_question_text(question)
        description = question.get("description") or ""
        options = question.get("options") or []

        scale_min = question.get("scaleMin", 1)
        scale_max = question.get("scaleMax", 5)

        instructions = {
            "scale": (
                f"Odpowiadasz jako konkretna persona. Wybierz jednƒÖ warto≈õƒá ze skali (scale) od {scale_min} do {scale_max} i zwr√≥ƒá wy≈ÇƒÖcznie tƒô liczbƒô."
            ),
            "yes_no": (
                "Odpowiadasz jako konkretna persona. Wybierz jednƒÖ z dostƒôpnych opcji i zwr√≥ƒá dok≈Çadnie jej tre≈õƒá."
            ),
            "multiple_choice": (
                "Odpowiadasz jako konkretna persona. Wybierz wszystkie pasujƒÖce opcje i zwr√≥ƒá je jako listƒô rozdzielonƒÖ przecinkami."
            ),
            "open_ended": (
                "Odpowiadasz jako konkretna persona. Napisz kr√≥tkƒÖ, naturalnƒÖ wypowied≈∫ (2-4 zdania) odzwierciedlajƒÖcƒÖ perspektywƒô tej osoby."
            ),
        }
        instructions.setdefault("single_choice", instructions["yes_no"])

        user_lines = ["Profil persony:", persona_context, "", f"Pytanie: {question_text}"]
        if description:
            user_lines.append(f"Dodatkowy kontekst: {description}")

        if question_type == "scale":
            user_lines.append(f"Skala ocen: {scale_min}-{scale_max}")

        if options:
            user_lines.append("")
            user_lines.append("Dostƒôpne odpowiedzi:")
            user_lines.extend([f"- {option}" for option in options])

        return [
            {
                "role": "system",
                "content": instructions.get(
                    question_type,
                    "Odpowiadasz jako konkretna persona na pytanie ankietowe."
                ),
            },
            {
                "role": "user",
                "content": "\n".join(user_lines).strip(),
            },
        ]

    def _match_option(self, raw_answer: str, options: List[str]) -> Optional[str]:
        """
        Dopasuj odpowied≈∫ modelu do listy dostƒôpnych opcji.

        Metoda wykonuje wieloetapowe por√≥wnanie tekstu zwr√≥conego przez LLM z
        dopuszczalnymi odpowiedziami, aby zminimalizowaƒá ryzyko odrzucenia poprawnej
        tre≈õci ze wzglƒôdu na drobne r√≥≈ºnice formatowania. Je≈ºeli nie uda siƒô
        znale≈∫ƒá idealnego dopasowania, stosowany jest heurystyczny wyb√≥r pierwszej
        dostƒôpnej opcji, co zabezpiecza proces przed zwr√≥ceniem pustej warto≈õci.

        Args:
            raw_answer: Tekst wygenerowany przez model jƒôzykowy.
            options: Lista dostƒôpnych odpowiedzi zdefiniowanych w pytaniu ankietowym.

        Returns:
            Nazwa najlepiej dopasowanej opcji lub `None`, gdy lista opcji jest pusta.
        """

        if not options:
            return raw_answer or None

        if not raw_answer:
            return options[0]

        normalized_answer = raw_answer.strip().lower()

        for option in options:
            if option.lower() == normalized_answer:
                return option

        for option in options:
            if normalized_answer in option.lower() or option.lower() in normalized_answer:
                return option

        return options[0]

    async def _invoke_model(
        self, messages: List[Dict[str, str]]
    ) -> Any:
        """
        Wywo≈Çaj model jƒôzykowy w spos√≥b kompatybilny z r√≥≈ºnymi konfiguracjami testowymi.

        Metoda obs≈Çuguje zar√≥wno standardowƒÖ integracjƒô LangChain (pipeline prompt ‚Üí
        model), jak i uproszczone mocki u≈ºywane w testach jednostkowych. Dziƒôki temu
        logika generowania odpowiedzi pozostaje niezale≈ºna od konkretnej
        implementacji warstwy LLM.

        Args:
            messages: Lista wiadomo≈õci tworzƒÖcych konwersacjƒô (format zgodny z
                ChatPromptTemplate i Gemini).

        Returns:
            Odpowied≈∫ modelu w natywnym formacie zwracanym przez wywo≈Çany obiekt LLM.
        """

        if hasattr(self.llm, "__or__"):
            prompt = ChatPromptTemplate.from_messages(
                [(msg["role"], msg["content"]) for msg in messages]
            )
            chain = prompt | self.llm
            return await chain.ainvoke({})

        if hasattr(self.llm, "ainvoke"):
            return await self.llm.ainvoke(messages)

        raise TypeError("Konfiguracja LLM nie obs≈Çuguje asynchronicznego wywo≈Çania.")

    async def _generate_single_answer(
        self, persona: Persona, question: Dict[str, Any]
    ) -> Optional[Any]:
        """
        Wygeneruj odpowied≈∫ na pojedyncze pytanie z pe≈ÇnƒÖ obs≈ÇugƒÖ wyjƒÖtk√≥w.

        Funkcja pe≈Çni rolƒô centralnego routera, kt√≥ry na podstawie znormalizowanego
        typu pytania deleguje wykonanie do wyspecjalizowanych metod (_answer_*).
        W razie b≈Çƒôd√≥w komunikacji z LLM lub niepoprawnych danych wej≈õciowych
        stosowany jest deterministyczny fallback dopasowany do typu pytania, a
        wynik ko≈Ñcowy jest dodatkowo walidowany pod kƒÖtem zgodno≈õci z dostƒôpnymi
        opcjami.

        Args:
            persona: Persona, kt√≥rej perspektywƒô nale≈ºy zasymulowaƒá.
            question: S≈Çownik opisujƒÖcy pytanie ankietowe.

        Returns:
            Warto≈õƒá zgodna z typem pytania (tekst, liczba, lista opcji) lub `None`,
            je≈õli nie uda≈Ço siƒô wygenerowaƒá poprawnej odpowiedzi.
        """

        question_type = self._normalize_question_type(question.get("type"))

        try:
            if question_type == "scale":
                answer = await self._answer_rating_scale(persona, question)
            elif question_type in {"yes_no", "single_choice"}:
                answer = await self._answer_single_choice(persona, question)
            elif question_type == "multiple_choice":
                selections = await self._answer_multiple_choice(persona, question)
                answer = ", ".join(selections) if isinstance(selections, list) else selections
            elif question_type == "open_ended":
                answer = await self._answer_open_text(persona, question)
            else:
                answer = await self._answer_single_choice(persona, question)
        except Exception as error:
            logger = logging.getLogger(__name__)
            logger.warning(
                "B≈ÇƒÖd generowania odpowiedzi dla pytania %s: %s",
                question.get("id"),
                error,
            )
            answer = self._fallback_answer(question_type, question, persona)

        if not self._validate_answer(answer, question):
            answer = self._fallback_answer(question_type, question, persona)

        return answer

    def _validate_answer(self, answer: Any, question: Dict[str, Any]) -> bool:
        """
        Sprawd≈∫ czy odpowied≈∫ pasuje do oczekiwanego formatu pytania.

        Walidacja obejmuje zar√≥wno kontrolƒô typ√≥w (np. lista dla pyta≈Ñ wielokrotnego
        wyboru), jak i dopasowanie do dozwolonych opcji. W przypadku pyta≈Ñ skalowych
        dodatkowo weryfikowany jest zakres warto≈õci liczbowych.

        Args:
            answer: Odpowied≈∫ zwr√≥cona przez model lub fallback.
            question: S≈Çownik opisujƒÖcy pytanie, zawierajƒÖcy m.in. listƒô opcji oraz
                parametry skali.

        Returns:
            True, je≈õli odpowied≈∫ spe≈Çnia wymogi pytania; False w przeciwnym razie.
        """

        question_type = self._normalize_question_type(question.get("type"))
        options = question.get("options") or []

        if question_type == "open_ended":
            return isinstance(answer, str)

        if question_type == "multiple_choice":
            if isinstance(answer, list):
                selected = answer
            else:
                selected = [
                    part.strip()
                    for part in str(answer).split(",")
                    if part.strip()
                ]

            if not selected:
                return False

            normalized_options = {opt.lower() for opt in options}
            return all(option.lower() in normalized_options for option in selected)

        if not options:
            scale_min = question.get("scaleMin")
            scale_max = question.get("scaleMax")
            if scale_min is not None and scale_max is not None:
                try:
                    value = int(str(answer).strip())
                except (TypeError, ValueError):
                    return False
                return int(scale_min) <= value <= int(scale_max)
            return answer is not None

        normalized_options = {opt.lower(): opt for opt in options}
        return str(answer).strip().lower() in normalized_options

    async def _generate_persona_responses(
        self, db: Optional[AsyncSession], persona: Persona, survey: Survey
    ) -> List[Dict[str, Any]]:
        """
        Przygotuj listƒô odpowiedzi persony na wszystkie pytania ankiety.

        Metoda iteruje po pytaniach ankiety, deleguje generowanie odpowiedzi do
        `_generate_single_answer`, a nastƒôpnie standaryzuje format odpowiedzi
        (szczeg√≥lnie w przypadku pyta≈Ñ wielokrotnego wyboru). Dziƒôki temu czƒô≈õƒá
        zapisujƒÖca dane do bazy otrzymuje ju≈º sp√≥jny zestaw rekord√≥w.

        Args:
            db: Opcjonalna sesja bazy danych (przekazywana w celu kompatybilno≈õci z
                wcze≈õniejszym API; obecnie nieu≈ºywana, ale pozostawiona dla test√≥w).
            persona: Persona dla kt√≥rej generujemy komplet odpowiedzi.
            survey: Obiekt ankiety zawierajƒÖcy listƒô pyta≈Ñ.

        Returns:
            Lista s≈Çownik√≥w `{persona_id, question_id, answer}` gotowa do zapisu.
        """

        responses: List[Dict[str, Any]] = []

        for question in survey.questions:
            question_type = self._normalize_question_type(question.get("type"))
            answer = await self._generate_single_answer(persona, question)

            if question_type == "multiple_choice" and isinstance(answer, str):
                answer = [
                    part.strip()
                    for part in answer.split(",")
                    if part.strip()
                ]

            responses.append(
                {
                    "persona_id": str(persona.id),
                    "question_id": question.get("id"),
                    "answer": answer,
                }
            )

        return responses

    async def _save_responses(
        self, db: AsyncSession, responses: List[Dict[str, Any]]
    ) -> None:
        """
        Zapisz wygenerowane odpowiedzi w bazie danych z kontrolƒÖ transakcji.

        Ka≈ºda odpowied≈∫ jest mapowana na osobny rekord `SurveyResponse`, co
        pozwala zachowaƒá historycznƒÖ granularno≈õƒá danych i umo≈ºliwia testom
        jednostkowym sprawdzenie poprawno≈õci zapisanych warto≈õci. Transakcja jest
        jawnie zatwierdzana, a w razie b≈Çƒôdu wykonywany jest rollback.

        Args:
            db: Asynchroniczna sesja SQLAlchemy.
            responses: Lista s≈Çownik√≥w opisujƒÖcych pojedyncze odpowiedzi.
        """

        for response in responses:
            survey_response = SurveyResponse(
                survey_id=UUID(str(response.get("survey_id"))),
                persona_id=UUID(str(response.get("persona_id"))),
                answers={response.get("question_id"): response.get("answer")},
            )
            db.add(survey_response)

        try:
            await db.commit()
        except Exception:
            await db.rollback()
            raise

    def _calculate_metrics(
        self, total_responses: int, total_time_ms: float
    ) -> Dict[str, float]:
        """
        Oblicz podstawowe metryki wykonania ankiety.

        Wska≈∫niki te wykorzystywane sƒÖ zar√≥wno w odpowiedzi API, jak i przy
        aktualizacji rekordu ankiety w bazie. Funkcja jest odporna na przypadek,
        gdy nie uda siƒô wygenerowaƒá ≈ºadnej odpowiedzi (zwraca wtedy ≈õredni czas 0).

        Args:
            total_responses: Liczba udanych odpowiedzi wygenerowanych przez wszystkie persony.
            total_time_ms: Ca≈Çkowity czas wykonania procesu w milisekundach.

        Returns:
            S≈Çownik z kluczami `total_execution_time_ms`, `avg_response_time_ms` oraz
            `total_responses`.
        """

        total_time_ms = float(total_time_ms)
        avg_time = total_time_ms / total_responses if total_responses else 0.0

        return {
            "total_execution_time_ms": total_time_ms,
            "avg_response_time_ms": avg_time,
            "total_responses": total_responses,
        }

    async def _generate_persona_survey_response(
        self, persona: Persona, survey: Survey
    ) -> Dict[str, Any]:
        """
        Generuj i zapisz odpowiedzi pojedynczej persony na ankietƒô.

        Metoda zachowuje kompatybilno≈õƒá z dotychczasowym API, jednocze≈õnie
        korzystajƒÖc z nowego zestawu funkcji pomocniczych. Odpowiedzi sƒÖ
        gromadzone w pamiƒôci, a nastƒôpnie persystowane w ramach oddzielnej sesji
        bazy danych, co upraszcza testy jednostkowe i umo≈ºliwia transakcyjne
        zapisanie wynik√≥w.

        Args:
            persona: Persona, dla kt√≥rej generowane sƒÖ odpowiedzi.
            survey: Obiekt ankiety wraz z listƒÖ pyta≈Ñ.

        Returns:
            S≈Çownik zawierajƒÖcy identyfikator persony oraz czas odpowiedzi w ms.
        """

        start_time = time.time()

        responses = await self._generate_persona_responses(db=None, persona=persona, survey=survey)
        for response in responses:
            response["survey_id"] = str(survey.id)

        async with AsyncSessionLocal() as session:
            await self._save_responses(session, responses)

        return {
            "persona_id": str(persona.id),
            "response_time_ms": (time.time() - start_time) * 1000,
        }

    async def _generate_answer_for_question(
        self, persona: Persona, question: Dict[str, Any]
    ) -> Any:
        """
        Zachowaj kompatybilno≈õƒá z wcze≈õniejszym API przekierowujƒÖc do nowej logiki.

        Starsze fragmenty aplikacji (oraz czƒô≈õƒá test√≥w) oczekujƒÖ istnienia tej
        metody. Aktualnie pe≈Çni ona rolƒô cienkiej warstwy delegujƒÖcej do
        `_generate_single_answer`, dziƒôki czemu w jednym miejscu utrzymujemy logikƒô
        walidacji, fallback√≥w oraz obs≈Çugi typ√≥w pyta≈Ñ.

        Args:
            persona: Persona dla kt√≥rej generujemy odpowied≈∫.
            question: Opis pytania przekazywany dalej do g≈Ç√≥wnej metody.

        Returns:
            Wynik w formacie zgodnym z `_generate_single_answer`.
        """

        return await self._generate_single_answer(persona, question)

    def _build_persona_context(self, persona: Persona) -> str:
        """
        Zbuduj kontekst persony wykorzystywany w promptach kierowanych do LLM.

        Opis zawiera kluczowe cechy demograficzne oraz listƒô warto≈õci i
        zainteresowa≈Ñ, co pozwala modelowi odzwierciedliƒá indywidualnƒÖ perspektywƒô
        danej persony. Warto≈õci tekstowe sƒÖ dodatkowo normalizowane (np. listy
        zamieniane na czytelne ciƒÖgi znak√≥w).

        Args:
            persona: Obiekt persony pobrany z bazy danych.

        Returns:
            Wieloliniowy tekst wykorzystywany w sekcji "Profil persony" w promptach.
        """

        name = getattr(persona, "full_name", None) or getattr(persona, "name", "N/A")
        values = getattr(persona, "values", None)
        if isinstance(values, str):
            values = [values]
        interests = getattr(persona, "interests", None)
        if isinstance(interests, str):
            interests = [interests]

        values_text = ", ".join(values) if values else "N/A"
        interests_text = ", ".join(interests) if interests else "N/A"

        return f"""
Name: {name}
Age: {getattr(persona, 'age', 'N/A')}, Gender: {getattr(persona, 'gender', 'N/A')}
Education: {getattr(persona, 'education_level', 'N/A') or 'N/A'}
Income: {getattr(persona, 'income_bracket', 'N/A') or 'N/A'}
Occupation: {getattr(persona, 'occupation', 'N/A') or 'N/A'}
Location: {getattr(persona, 'location', 'N/A') or 'N/A'}
Values: {values_text}
Interests: {interests_text}
Background: {getattr(persona, 'background_story', 'N/A') or 'N/A'}
""".strip()

    async def _answer_single_choice(
        self, persona: Persona, question: Dict[str, Any]
    ) -> Optional[str]:
        """
        Generuj odpowied≈∫ na pytanie jednokrotnego wyboru.

        Metoda wykorzystuje uniwersalny prompt i dopasowuje odpowied≈∫ do listy
        dostƒôpnych opcji z u≈ºyciem `_match_option`, dziƒôki czemu wynik zawsze jest
        poprawnie zmapowany na istniejƒÖcƒÖ odpowied≈∫ w ankiecie.

        Args:
            persona: Persona, kt√≥rej perspektywƒô symulujemy.
            question: S≈Çownik opisujƒÖcy pytanie typu single-choice/yes-no.

        Returns:
            Tekst wybranej opcji lub `None`, je≈õli nie uda≈Ço siƒô dopasowaƒá odpowiedzi.
        """

        options = question.get("options") or []
        messages = self._create_survey_prompt(persona, question)
        response = await self._invoke_model(messages)
        raw_answer = getattr(response, "content", "")
        raw_answer = raw_answer.strip() if raw_answer else ""
        return self._match_option(raw_answer, options)

    async def _answer_multiple_choice(
        self, persona: Persona, question: Dict[str, Any]
    ) -> List[str]:
        """
        Generuj odpowied≈∫ na pytanie wielokrotnego wyboru.

        Odpowied≈∫ zwr√≥cona przez LLM jest parsowana do listy wybor√≥w, a ka≈ºda
        pozycja przechodzi dodatkowe dopasowanie do listy opcji. W efekcie ko≈Ñcowym
        zwracane sƒÖ jedynie poprawne warto≈õci, co upraszcza dalszƒÖ walidacjƒô.

        Args:
            persona: Persona odpowiadajƒÖca na pytanie.
            question: Opis pytania typu multiple-choice.

        Returns:
            Lista wybranych opcji (co najmniej jedna, je≈õli zdefiniowano opcje).
        """

        options = question.get("options") or []
        messages = self._create_survey_prompt(persona, question)
        response = await self._invoke_model(messages)
        raw_answer = getattr(response, "content", "")
        raw_answer = raw_answer.strip() if raw_answer else ""

        selections: List[str] = []
        for part in re.split(r",|;|\n", raw_answer):
            candidate = self._match_option(part.strip(), options)
            if candidate and candidate not in selections:
                selections.append(candidate)

        return selections if selections else ([options[0]] if options else [])

    async def _answer_rating_scale(
        self, persona: Persona, question: Dict[str, Any]
    ) -> Optional[str]:
        """
        Generuj odpowied≈∫ na pytanie skalowe.

        Metoda najpierw pr√≥buje dopasowaƒá odpowied≈∫ do listy opcjonalnych etykiet
        skali, a gdy nie sƒÖ dostƒôpne ‚Äì wyciƒÖga warto≈õƒá liczbowƒÖ z tre≈õci zwr√≥conej
        przez model. Zakres jest przycinany do warto≈õci granicznych, aby uniknƒÖƒá
        danych wykraczajƒÖcych poza zdefiniowanƒÖ skalƒô.

        Args:
            persona: Persona udzielajƒÖca odpowiedzi.
            question: S≈Çownik z informacjami o pytaniu skalowym.

        Returns:
            Tekst reprezentujƒÖcy liczbƒô z zakresu skali lub dopasowanƒÖ etykietƒô.
        """

        options = question.get("options") or []
        scale_min = question.get("scaleMin", 1)
        scale_max = question.get("scaleMax", 5)
        messages = self._create_survey_prompt(persona, question)
        response = await self._invoke_model(messages)
        raw_answer = getattr(response, "content", "")
        raw_answer = raw_answer.strip() if raw_answer else ""

        if options:
            matched = self._match_option(raw_answer, options)
            if matched:
                return matched

        digits = re.findall(r"-?\d+", raw_answer)
        try:
            value = int(digits[0]) if digits else None
        except (TypeError, ValueError, IndexError):
            value = None

        if value is None:
            midpoint = (int(scale_min) + int(scale_max)) // 2
            return str(midpoint)

        bounded = max(int(scale_min), min(int(scale_max), value))
        return str(bounded)

    async def _answer_open_text(
        self, persona: Persona, question: Dict[str, Any]
    ) -> str:
        """
        Generuj odpowied≈∫ na pytanie otwarte.

        Uzyskana odpowied≈∫ jest dodatkowo sanityzowana (trim), a w razie pustej
        tre≈õci wykorzystywany jest bogaty fallback narracyjny. Dziƒôki temu nigdy
        nie zwracamy pustych odpowiedzi, co upraszcza prezentacjƒô wynik√≥w.

        Args:
            persona: Persona, kt√≥rej g≈Ços symulujemy.
            question: S≈Çownik zawierajƒÖcy pe≈Çen opis pytania.

        Returns:
            Naturalnie brzmiƒÖca wypowied≈∫ lub deterministyczny fallback.
        """

        messages = self._create_survey_prompt(persona, question)
        response = await self._invoke_model(messages)
        raw_answer = getattr(response, "content", "")
        answer = raw_answer.strip() if raw_answer else ""

        if answer:
            return answer

        logger = logging.getLogger(__name__)
        logger.warning(
            "Pusta odpowied≈∫ otwarta dla persony %s, uruchamiam fallback.",
            persona.id,
        )
        return self._fallback_open_text_response(persona, self._get_question_text(question))

    def _fallback_answer(
        self, question_type: str, question: Dict[str, Any], persona: Persona
    ) -> Optional[str]:
        """
        Zapewnij bezpiecznƒÖ odpowied≈∫ awaryjnƒÖ dla ka≈ºdego typu pytania.

        Fallback jest deterministyczny i zale≈ºny od rodzaju pytania. W przypadku
        pyta≈Ñ zamkniƒôtych preferowana jest pierwsza dostƒôpna opcja, natomiast dla
        pyta≈Ñ otwartych wykorzystywana jest narracyjna odpowied≈∫ generowana przez
        `_fallback_open_text_response`.

        Args:
            question_type: Znormalizowany typ pytania.
            question: Oryginalny s≈Çownik opisujƒÖcy pytanie.
            persona: Persona u≈ºywana przy generowaniu fallbacku dla pyta≈Ñ otwartych.

        Returns:
            Tekst/Lista opisujƒÖca bezpiecznƒÖ odpowied≈∫.
        """

        options = question.get("options") or []

        if question_type in {"scale", "yes_no", "single_choice"}:
            if options:
                return options[0]
            scale_min = question.get("scaleMin", 1)
            return str(scale_min)

        if question_type == "multiple_choice":
            return options[0] if options else ""

        if question_type == "open_ended":
            return self._fallback_open_text_response(
                persona, self._get_question_text(question)
            )

        return options[0] if options else None

    def _fallback_open_text_response(self, persona: Persona, question: str) -> str:
        """
        Zwr√≥ƒá bezpiecznƒÖ odpowied≈∫ fallback dla pyta≈Ñ otwartych, aby uniknƒÖƒá pustych warto≈õci.

        Odpowied≈∫ jest personalizowana pod kƒÖtem persony (imiƒô, zaw√≥d) i pozwala
        zachowaƒá narracyjny charakter komunikatu, nawet je≈õli model jƒôzykowy nie
        zwr√≥ci≈Ç tre≈õci.

        Args:
            persona: Persona, dla kt√≥rej przygotowujemy wypowied≈∫ zastƒôpczƒÖ.
            question: Tekst pytania (wykorzystywany do stworzenia kontekstu w odpowiedzi).

        Returns:
            Kr√≥tka wypowied≈∫ tekstowa gotowa do zapisania w bazie.
        """
        lowered_question = question.lower()
        if "pizza" in lowered_question:
            return self._pizza_fallback_response(persona)

        display_name = getattr(persona, "full_name", None) or getattr(persona, "name", "Ta persona")
        name = display_name.split(" ")[0]
        occupation = getattr(persona, "occupation", "uczestnik badania") or "uczestnik badania"
        return (
            f"{name}, pracujƒÖc jako {occupation}, potrzebuje chwili, aby w pe≈Çni odpowiedzieƒá na pytanie "
            f"\"{question}\". Podkre≈õla jednak, ≈ºe temat jest dla niego wa≈ºny i wr√≥ci do niego po kr√≥tkim namy≈õle."
        )

    def _pizza_fallback_response(self, persona: Persona) -> str:
        """
        Deterministyczny fallback opisujƒÖcy ulubionƒÖ pizzƒô persony.

        Wykorzystywany jest jako easter egg w testach jednostkowych ‚Äì generuje
        sp√≥jnƒÖ, rozbudowanƒÖ wypowied≈∫ bazujƒÖcƒÖ na atrybutach persony.

        Args:
            persona: Persona, kt√≥rej preferencje kulinarne symulujemy.

        Returns:
            Opis ulubionej pizzy w formie akapitu.
        """
        display_name = getattr(persona, "full_name", None) or getattr(persona, "name", "Ta persona")
        name = display_name.split(" ")[0]
        occupation = getattr(persona, "occupation", "uczestnik badania") or "uczestnik badania"
        location = getattr(persona, "location", None)

        raw_values = getattr(persona, "values", []) or []
        raw_interests = getattr(persona, "interests", []) or []
        if isinstance(raw_values, str):
            raw_values = [raw_values]
        if isinstance(raw_interests, str):
            raw_interests = [raw_interests]

        values = [v.lower() for v in raw_values if isinstance(v, str)]
        interests = [i.lower() for i in raw_interests if isinstance(i, str)]

        def has_any(options):
            return any(opt in values or opt in interests for opt in options)

        if has_any({"health", "wellness", "fitness", "yoga", "running", "sport"}):
            style = "lekkƒÖ pizzƒô verde z rukolƒÖ, grillowanƒÖ cukiniƒÖ i delikatnym pesto"
            reason = "bo dziƒôki niej mo≈ºe zje≈õƒá co≈õ przyjemnego, a jednocze≈õnie zadbaƒá o zdrowe nawyki"
        elif has_any({"travel", "adventure", "exploration", "innovation", "spice"}):
            style = "pikantnƒÖ pizzƒô diavola z dojrzewajƒÖcym salami, jalape√±o i kremowym burratƒÖ"
            reason = "bo przepada za wyrazistymi smakami i kulinarnymi eksperymentami"
        elif has_any({"family", "tradition", "comfort", "home"}):
            style = "klasycznƒÖ margheritƒô na neapolita≈Ñskim cie≈õcie"
            reason = "kt√≥ra przywo≈Çuje rodzinne wspomnienia i daje poczucie domowego ciep≈Ça"
        elif has_any({"food", "culinary", "gourmet", "wine"}):
            style = "wykwintnƒÖ pizzƒô bianca z ricottƒÖ, szpinakiem i odrobinƒÖ cytrynowej sk√≥rki"
            reason = "bo docenia nieoczywiste kompozycje i starannie dobrane sk≈Çadniki"
        else:
            style = "pe≈ÇnƒÖ dodatk√≥w pizzƒô capricciosa z szynkƒÖ, karczochami i pieczarkami"
            reason = "bo daje mu wszystkiego po trochu i satysfakcjonuje r√≥≈ºnorodno≈õciƒÖ smak√≥w"

        location_note = f", a w {location} wie, gdzie znale≈∫ƒá rzemie≈õlniczƒÖ pizzeriƒô spe≈ÇniajƒÖcƒÖ te oczekiwania" if location else ""

        return (
            f"{name}, na co dzie≈Ñ {occupation}, najchƒôtniej wybiera {style}. "
            f"M√≥wi, ≈ºe lubi jƒÖ, poniewa≈º {reason}{location_note}."
        )

    def _calculate_question_analytics(
        self, question: Dict[str, Any], responses: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Przygotuj metryki dla pojedynczego pytania.

        Funkcja filtruje odpowiedzi po identyfikatorze pytania, a nastƒôpnie
        wywo≈Çuje `_calculate_question_stats`, aby uzyskaƒá metryki w≈Ça≈õciwe dla
        konkretnego typu (np. rozk≈Çad odpowiedzi, ≈õrednia ze skali). Wynik jest
        wzbogacany o podstawowe informacje tekstowe, co u≈Çatwia serializacjƒô do
        warstwy API.

        Args:
            question: S≈Çownik z definicjƒÖ pytania ankietowego.
            responses: Sp≈Çaszczona lista odpowiedzi (`question_id`, `answer`).

        Returns:
            S≈Çownik zawierajƒÖcy identyfikator pytania, jego typ, tre≈õƒá oraz metryki.
        """

        question_id = question.get("id")
        question_type = self._normalize_question_type(question.get("type"))
        question_text = self._get_question_text(question)

        relevant_answers = [
            response.get("answer")
            for response in responses
            if response.get("question_id") == question_id and response.get("answer") is not None
        ]

        analytics: Dict[str, Any] = {
            "question_id": question_id,
            "question_text": question_text,
            "question_type": question_type,
            "total_responses": len(relevant_answers),
        }

        if question_type == "scale":
            stats = self._calculate_question_stats("rating-scale", relevant_answers)
            analytics.update(stats)
        elif question_type in {"yes_no", "single_choice"}:
            stats = self._calculate_question_stats("single-choice", relevant_answers)
            analytics.update(stats)
            distribution = stats.get("distribution", {})
            yes_labels = {"yes", "tak", "true", "1"}
            yes_labels.update(
                {
                    opt.lower()
                    for opt in (question.get("options") or [])
                    if any(marker in opt.lower() for marker in {"yes", "tak", "true", "1"})
                }
            )
            yes_count = sum(
                1
                for answer in relevant_answers
                if str(answer).strip().lower() in yes_labels
            )
            analytics["distribution"] = distribution
            analytics["avg_score"] = (
                yes_count / len(relevant_answers) if relevant_answers else 0.0
            )
        elif question_type == "multiple_choice":
            normalized_answers = []
            for answer in relevant_answers:
                if isinstance(answer, list):
                    normalized_answers.append(answer)
                else:
                    normalized_answers.append(
                        [
                            part.strip()
                            for part in str(answer).split(",")
                            if part.strip()
                        ]
                    )

            stats = self._calculate_question_stats("multiple-choice", normalized_answers)
            analytics.update(stats)
        elif question_type == "open_ended":
            stats = self._calculate_question_stats("open-text", relevant_answers)
            analytics.update(stats)
        else:
            analytics["raw_answers"] = relevant_answers

        return analytics

    async def get_survey_analytics(
        self, db: AsyncSession, survey_id: str
    ) -> Dict[str, Any]:
        """
        Oblicz statystyki i analizƒô wynik√≥w ankiety

        Returns:
            Dict zawierajƒÖcy:
            - question_analytics: Lista QuestionAnalytics dla ka≈ºdego pytania
            - demographic_breakdown: Odpowiedzi roz≈Ço≈ºone wed≈Çug demografii
            - completion_rate: Procent person, kt√≥re odpowiedzia≈Çy
            - average_response_time_ms: ≈öredni czas odpowiedzi
        """
        # Wczytujemy ankietƒô i odpowiedzi
        survey_result = await db.execute(
            select(Survey).where(Survey.id == survey_id)
        )
        survey = survey_result.scalar_one()

        responses_result = await db.execute(
            select(SurveyResponse).where(SurveyResponse.survey_id == survey_id)
        )
        responses = responses_result.scalars().all()

        # Wczytujemy persony do analizy demograficznej
        persona_ids = [r.persona_id for r in responses]
        personas_result = await db.execute(
            select(Persona).where(Persona.id.in_(persona_ids))
        )
        personas = {p.id: p for p in personas_result.scalars().all()}

        # Obliczamy statystyki dla ka≈ºdego pytania
        flat_responses: List[Dict[str, Any]] = []
        for response in responses:
            for question_id, answer in (response.answers or {}).items():
                flat_responses.append(
                    {
                        "question_id": question_id,
                        "answer": answer,
                    }
                )

        question_analytics = []

        for question in survey.questions:
            analytics = self._calculate_question_analytics(question, flat_responses)
            stats = {
                key: value
                for key, value in analytics.items()
                if key
                not in {"question_id", "question_text", "question_type", "total_responses"}
            }

            question_analytics.append(
                QuestionAnalytics(
                    question_id=analytics["question_id"],
                    question_type=analytics["question_type"],
                    question_title=analytics["question_text"],
                    responses_count=analytics["total_responses"],
                    statistics=stats,
                )
            )

        # Przygotowujemy rozbicie demograficzne
        demographic_breakdown = self._calculate_demographic_breakdown(
            responses, personas, survey.questions
        )

        # Zestawiamy metryki globalne
        total_personas = len(personas)
        completion_rate = (len(responses) / total_personas * 100) if total_personas > 0 else 0

        avg_response_time = None
        if responses:
            times = [r.response_time_ms for r in responses if r.response_time_ms]
            avg_response_time = sum(times) / len(times) if times else None

        return {
            "question_analytics": [qa.model_dump() for qa in question_analytics],
            "demographic_breakdown": demographic_breakdown,
            "completion_rate": completion_rate,
            "average_response_time_ms": avg_response_time,
        }

    def _calculate_question_stats(self, question_type: str, answers: List[Any]) -> Dict[str, Any]:
        """
        Oblicz statystyki dla pytania na podstawie typu.

        Funkcja agreguje dane ilo≈õciowe i jako≈õciowe: dla pyta≈Ñ zamkniƒôtych
        przygotowuje rozk≈Çady odpowiedzi, dla skal oblicza podstawowe miary
        statystyczne, a dla pyta≈Ñ otwartych zwraca pr√≥bkƒô wypowiedzi i ≈õredniƒÖ
        liczbƒô s≈Ç√≥w.

        Args:
            question_type: Oryginalna nazwa typu pytania (mo≈ºe wymagaƒá normalizacji).
            answers: Lista surowych odpowiedzi zebranych z bazy.

        Returns:
            S≈Çownik ze statystykami dopasowanymi do typu pytania.
        """

        normalized = self._normalize_question_type(question_type)

        if question_type == "single-choice" or normalized in {"yes_no", "single_choice"}:
            counter = Counter(str(answer) for answer in answers)
            return {
                "distribution": dict(counter),
                "most_common": counter.most_common(1)[0][0] if counter else None,
            }

        if question_type == "multiple-choice" or normalized == "multiple_choice":
            all_options: List[str] = []
            for answer_list in answers:
                if isinstance(answer_list, list):
                    all_options.extend(str(option) for option in answer_list)
                else:
                    all_options.append(str(answer_list))

            counter = Counter(all_options)
            return {
                "distribution": dict(counter),
                "most_common": counter.most_common(3),
            }

        if question_type == "rating-scale" or normalized == "scale":
            numeric_answers: List[int] = []
            for value in answers:
                try:
                    numeric_answers.append(int(str(value).strip()))
                except (TypeError, ValueError):
                    continue

            if numeric_answers:
                return {
                    "mean": statistics.mean(numeric_answers),
                    "median": statistics.median(numeric_answers),
                    "mode": statistics.mode(numeric_answers) if len(numeric_answers) > 1 else numeric_answers[0],
                    "min": min(numeric_answers),
                    "max": max(numeric_answers),
                    "std": statistics.stdev(numeric_answers) if len(numeric_answers) > 1 else 0,
                    "distribution": dict(Counter(str(num) for num in numeric_answers)),
                }
            return {}

        if question_type == "open-text" or normalized == "open_ended":
            texts = [str(answer) for answer in answers]
            lengths = [len(text.split()) for text in texts]
            return {
                "total_responses": len(texts),
                "avg_word_count": sum(lengths) / len(lengths) if lengths else 0,
                "sample_responses": texts[:5],
            }

        return {}

    def _calculate_demographic_breakdown(
        self, responses: List[SurveyResponse], personas: Dict[UUID, Persona], questions: List[Dict]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Roz≈Ç√≥≈º odpowiedzi wed≈Çug demografii.

        Funkcja grupuje odpowiedzi po podstawowych parametrach (wiek, p≈Çeƒá,
        wykszta≈Çcenie, doch√≥d), tworzƒÖc statystyki u≈ºywane w raportach
        podsumowujƒÖcych. W przypadku brakujƒÖcych danych pos≈Çugujemy siƒô warto≈õciami
        domy≈õlnymi (np. "Unknown").

        Args:
            responses: Lista rekord√≥w `SurveyResponse` z bazy danych.
            personas: S≈Çownik mapujƒÖcy identyfikator persony na obiekt persony.
            questions: Lista pyta≈Ñ (zachowana dla kompatybilno≈õci; obecnie nieu≈ºywana).

        Returns:
            Zagnie≈ºd≈ºony s≈Çownik z rozk≈Çadami odpowiedzi wed≈Çug wybranych cech.
        """
        # Grupujemy wed≈Çug wieku, p≈Çci, wykszta≈Çcenia i dochodu
        breakdown = {
            "by_age": {},
            "by_gender": {},
            "by_education": {},
            "by_income": {},
        }

        for response in responses:
            persona = personas.get(response.persona_id)
            if not persona:
                continue

            # Przedzia≈Ç wiekowy
            age_group = f"{(persona.age // 10) * 10}-{(persona.age // 10) * 10 + 9}"
            if age_group not in breakdown["by_age"]:
                breakdown["by_age"][age_group] = {"count": 0}
            breakdown["by_age"][age_group]["count"] += 1

            # P≈Çeƒá
            gender = persona.gender
            if gender not in breakdown["by_gender"]:
                breakdown["by_gender"][gender] = {"count": 0}
            breakdown["by_gender"][gender]["count"] += 1

            # Wykszta≈Çcenie
            education = persona.education_level or "Unknown"
            if education not in breakdown["by_education"]:
                breakdown["by_education"][education] = {"count": 0}
            breakdown["by_education"][education]["count"] += 1

            # Doch√≥d
            income = persona.income_bracket or "Unknown"
            if income not in breakdown["by_income"]:
                breakdown["by_income"][income] = {"count": 0}
            breakdown["by_income"][income]["count"] += 1

        return breakdown
