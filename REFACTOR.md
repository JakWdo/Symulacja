# Market Research SaaS - Plan Refaktoryzacji

**Branch:** refactor/cloud-run-diagnosis
**Status:** üü¢ ~75% COMPLETE (12/19 task√≥w wykonanych)
**Last Updated:** 2025-10-22 20:00 CET

---

## üìä EXECUTIVE SUMMARY

### Kluczowe Commity
- **5aa685c** (2025-10-22) - De-demografizacja + segment-based generation
- **3953bc4** (2025-10-22) - Secret Manager integration + health checks
- **3fe8cf9** (2025-10-22) - Cleanup test√≥w demographics

### Status Wykonania

| Faza | Status | Progress | Czas |
|------|--------|----------|------|
| FAZA 1: Diagnoza | ‚úÖ DONE | 1/1 | 30 min |
| FAZA 2.1: De-demografizacja | ‚ö†Ô∏è 4/5 | 80% | 4-6h |
| FAZA 2.2: RAG Hardening | ‚ö†Ô∏è 2/3 | 67% | 3-4h |
| FAZA 2.3: Docker & Cloud Run | ‚úÖ DONE | 3/3 | 3-4h |
| FAZA 3: Local Testing | ‚è≠Ô∏è SKIP | - | - |
| FAZA 4: Cloud Deployment | ‚è∏Ô∏è READY | 0/3 | 2-3h |
| FAZA 5: Testy Cleanup | ‚úÖ DONE | 2/2 | 1-2h |
| FAZA 6: Dokumentacja | ‚ö†Ô∏è 1/4 | 25% | 2-3h |

**DONE:** 12 task√≥w | **TODO:** 7 task√≥w | **Completion:** ~75%

### Co Zosta≈Ço Zrobione ‚úÖ

**Backend Refactor:**
- ‚úÖ DemographicDistribution class usuniƒôta
- ‚úÖ Chi-square validation usuniƒôta
- ‚úÖ Database migration (drop demographics columns)
- ‚úÖ Segment-based generation (orchestration ‚Üí segments ‚Üí RAG ‚Üí LLM)

**Infrastructure:**
- ‚úÖ Secret Manager integration (app/core/secrets.py)
- ‚úÖ Health checks (/health liveness + /ready readiness)
- ‚úÖ Dockerfile $PORT support + gunicorn
- ‚úÖ cloudbuild.yaml (BuildKit cache, 8-12 min builds)

**Testing & Docs:**
- ‚úÖ Testy demographics usuniƒôte (14 plik√≥w)
- ‚úÖ Fixtures zaktualizowane (bez target_demographics)
- ‚úÖ PLAN.md exists

### Co Pozosta≈Ço TODO ‚ùå

**CRITICAL (Priority 1):**
- ‚ùå Demographics remnants cleanup (generation.py L204-205 u≈ºywa DEFAULT_*)

**Dokumentacja (Priority 2):**
- ‚ùå REFACTOR_SUMMARY.md cleanup (usunƒÖƒá legacy doc)
- ‚ùå docs/DEPLOYMENT_CLOUD_RUN.md (template ready w tym pliku)
- ‚ùå Update PLAN.md (dodaƒá completed tasks)

**Optional:**
- ‚è∏Ô∏è Vertex AI Ranking (komentarz w requirements.txt, implementacja later)
- ‚è∏Ô∏è Local Docker testing (skip je≈õli cloudbuild dzia≈Ça)

**Deployment (Final):**
- ‚è∏Ô∏è Cloud Run deployment (gdy cleanup done)

---

## üìã Plan Refaktoryzacji (Szczeg√≥≈Çowy)

Pe≈Çna refaktoryzacja projektu z usuniƒôciem demographics jako ≈∫r√≥d≈Ça wej≈õciowego.

---

## üéØ CEL REFAKTORYZACJI

### G≈Ç√≥wne Cele
1. **UsunƒÖƒá zale≈ºno≈õƒá od demographics jako ≈∫r√≥d≈Ça wej≈õciowego**
   - Obecnie: Demographics sampling z DEFAULT_* constants ‚Üí RAG context ‚Üí LLM
   - Docelowo: Orchestration ‚Üí Segments ‚Üí RAG/GraphRAG context ‚Üí LLM

2. **Generacja person wy≈ÇƒÖcznie przez RAG/GraphRAG + Model Knowledge**
   - Brak rƒôcznych rozk≈Çad√≥w demograficznych
   - Brak chi-square validation
   - Brak target_demographics w Project

3. **Przygotowaƒá clean deployment do Cloud Run**
   - Secret Manager dla wszystkich secrets
   - Vertex AI Ranking (zamiast sentence-transformers 900MB)
   - $PORT support, health checks, cloudbuild.yaml

---

## üîç ODKRYCIA Z EXPLORATION

### 1. Mapa U≈ºycia Demographics

#### Pliki do Usuniƒôcia
```
app/core/constants/demographics.py           # US defaults (DEFAULT_AGE_GROUPS, etc.)
app/core/constants/__init__.py               # Re-eksporty DEFAULT_* (do usuniƒôcia)
```

#### Klasy i Metody do Usuniƒôcia
**app/services/personas/generator.py:**
- `DemographicDistribution` dataclass (linie 294-307)
- `sample_demographic_profile()` (linie 358-404)
- `_prepare_distribution()` (linie 463-492)
- `validate_distribution()` (linie 756-818)
- `_chi_square_test()` (linie 820-903)

**app/services/__init__.py:**
- Re-eksport `DemographicDistribution`

#### API Endpoints do Uproszczenia
**app/api/personas/generation.py:**
- UsunƒÖƒá `DemographicDistribution` logic (linie 202-210)
- UsunƒÖƒá chi-square validation (linie 789-799)
- Uprosiƒá do: orchestration ‚Üí segments ‚Üí generate_persona

**app/api/personas/reasoning.py:**
- UsunƒÖƒá fallbacki do demographics
- U≈ºywaƒá tylko: `orchestration_reasoning` + `rag_context_details`

**app/api/projects.py:**
- UsunƒÖƒá wym√≥g `target_demographics` w POST/PUT

#### Database Models
**app/models/project.py** - Kolumny do usuniƒôcia:
```python
target_demographics = Column(JSON, nullable=False)
chi_square_statistic = Column(JSON, nullable=True)
p_values = Column(JSON, nullable=True)
is_statistically_valid = Column(Boolean, nullable=False, default=False)
validation_date = Column(DateTime(timezone=True), nullable=True)
```

**app/schemas/project.py:**
- `ProjectCreate`: usu≈Ñ `target_demographics`
- `ProjectUpdate`: usu≈Ñ `target_demographics`
- `ProjectResponse`: usu≈Ñ wszystkie demografia fields

#### Testy do Usuniƒôcia
```
tests/unit/test_persona_generator.py        # Testy DemographicDistribution, chi-square
tests/unit/test_critical_paths.py           # test_chi_square_validation_rejects_bad_distributions
tests/integration/test_projects_api_integration.py  # Testy target_demographics
```

---

### 2. Architektura RAG/GraphRAG

#### Obecna Struktura
```
app/services/rag/
‚îú‚îÄ‚îÄ clients.py              # get_vector_store(), get_graph_store() z retry logic
‚îú‚îÄ‚îÄ hybrid_search.py        # PolishSocietyRAG (Vector + Keyword + RRF + Graph)
‚îú‚îÄ‚îÄ graph_service.py        # GraphRAGService (Cypher queries, graph context)
‚îî‚îÄ‚îÄ document_service.py     # RAGDocumentService (ingest pipeline)
```

#### Graceful Degradation Flow
```
Neo4j available?
‚îú‚îÄ YES ‚Üí RAG context (graph + chunks)
‚îÇ         Generator u≈ºywa demographics + RAG context
‚îÇ
‚îî‚îÄ NO ‚Üí Graceful degradation
          ‚îú‚îÄ Vector store failed ‚Üí Pusty RAG context
          ‚îú‚îÄ Graph store failed ‚Üí Tylko vector search (bez graph nodes)
          ‚îú‚îÄ Keyword search failed ‚Üí Tylko vector search
          ‚îî‚îÄ Generator dzia≈Ça z demographics tylko (bez RAG context)
```

**Wszystkie poziomy fallback dzia≈ÇajƒÖ - system nie crashuje gdy Neo4j unavailable!**

#### RAG w Generowaniu Person
**app/services/personas/generator.py:**
- `_get_rag_context_for_persona()` (linie 552-601)
- `generate_persona_personality()` (linie 603-750)
  - RAG context ‚Üí prompt builder ‚Üí LLM
  - Fallback: RAG unavailable ‚Üí tylko demographics

**app/services/personas/orchestration.py:**
- `create_persona_allocation_plan()` (linie 507-577)
- `_get_comprehensive_graph_context()` (linie 578-650)
  - Hybrid search dla ka≈ºdej grupy demograficznej
  - Graph context w promptach dla Gemini 2.5 Pro

---

### 3. Struktura Dokumentacji

#### Problemy Zidentyfikowane
1. **CRITICAL:** Brak `PLAN.md` (wielokrotnie wspominany w CLAUDE.md)
2. **Duplikacja:** `AI_ML.md` (linie 116-200) duplikuje `PERSONA_DETAILS.md`
3. **NieistniejƒÖcy plik:** `docs/README.md` odnosi siƒô do `DOCKER.md` (6 miejsc) - nie istnieje
4. **Za d≈Çugie:** `PERSONA_DETAILS_DATA_AUDIT.md` (1574 linie)
5. **Niesp√≥jno≈õci:** Liczby test√≥w (208 vs 380), rozmiary dokument√≥w

#### Pliki do Usuniƒôcia
```
app/api/personas/REFACTOR_SUMMARY.md
.claude/agents/*                             # 10 plik√≥w (materia≈Ç pomocniczy)
test_persona_details_migration.sh
```

#### Pliki do Zaktualizowania
```
docs/README.md              # Fix DOCKER.md ‚Üí DEVOPS.md, rozmiary, liczby test√≥w
docs/AI_ML.md               # Usu≈Ñ duplikacjƒô (linie 116-200), dodaj link do PERSONA_DETAILS.md
CLAUDE.md                   # Dodaj sekcjƒô Cloud Run Deployment
```

#### Pliki do Utworzenia
```
PLAN.md                              # 20-30 zada≈Ñ strategic roadmap
docs/DEPLOYMENT_CLOUD_RUN.md        # Praktyczny guide (Secret Manager, cloudbuild, monitoring)
```

---

### 4. Docker & Cloud Run Issues

#### Dockerfile - Problemy
```dockerfile
# ‚ùå PROBLEM: Hardcoded port
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# ‚úÖ FIX: Cloud Run dynamic port
CMD ["gunicorn", "app.main:app", \
     "--worker-class", "uvicorn.workers.UvicornWorker", \
     "--workers", "2", \
     "--bind", "0.0.0.0:${PORT:-8000}", \
     "--timeout", "120"]
```

#### requirements.txt - Problemy
**USU≈É (900MB!):**
```
sentence-transformers==2.7.0    # U≈ºywane TYLKO dla reranking, bƒôdzie Vertex AI
```

**DODAJ:**
```
gunicorn                         # Production server (brakuje!)
google-cloud-secret-manager     # Secrets z Secret Manager
google-cloud-aiplatform         # Vertex AI Ranking API
```

#### docker-entrypoint.sh
**Problem:** Nie u≈ºywa `$PORT` - hardcoded 8000

**Fix:** Przekazywaƒá `$PORT` do uvicorn/gunicorn

#### Health Checks
**Obecne:** `/health` endpoint istnieje
**Brakuje:**
- `/ready` - readiness check (DB + Redis + Neo4j)
- Startup probe w Cloud Run config

---

## üèóÔ∏è SZCZEG√ì≈ÅOWY PLAN REFAKTORYZACJI

### FAZA 1: Quick Fix - IndentationError [30 min]

**Cel:** Naprawiƒá syntax error, ≈ºeby aplikacja w og√≥le startowa≈Ça

**Kroki:**
1. Znale≈∫ƒá plik z `IndentationError` (app/api/personas/*.py, linia 975-977)
2. Naprawiƒá syntax error (dodaƒá `pass` lub implementacjƒô)
3. Commit + push
4. Trigger redeploy w Cloud Run

**Success criteria:**
- ‚úÖ Aplikacja startuje bez crashes
- ‚úÖ `/health` endpoint zwraca 200
- ‚úÖ Brak "malformed response" errors w logach

---

### FAZA 2.1: De-demografizacja Backend [4-6h]

#### 2.1.1 Usuniƒôcie Demographics Sources

**UsunƒÖƒá pliki:**
```bash
rm app/core/constants/demographics.py
```

**Edytowaƒá:**
- `app/core/constants/__init__.py`
  - UsunƒÖƒá re-eksporty: `DEFAULT_AGE_GROUPS`, `DEFAULT_GENDERS`, `DEFAULT_EDUCATION_LEVELS`, `DEFAULT_INCOME_BRACKETS`, `DEFAULT_LOCATIONS`
  - Zostawiƒá: `POLISH_*` constants

**Zachowaƒá:**
- `app/core/constants/polish.py` - polskie demographics (u≈ºywane w promptach)
- `app/core/constants/personas.py` - persona-agnostyczne sta≈Çe

#### 2.1.2 Usuniƒôcie DemographicDistribution Class

**app/services/personas/generator.py:**

**UsunƒÖƒá klasy i metody:**
```python
# Linie 294-307
@dataclass
class DemographicDistribution:
    age_groups: Dict[str, float]
    genders: Dict[str, float]
    education_levels: Dict[str, float]
    income_brackets: Dict[str, float]
    locations: Dict[str, float]

# Linie 358-404
def sample_demographic_profile(distribution: DemographicDistribution, n_samples: int)

# Linie 463-492
def _prepare_distribution(distribution, fallback)

# Linie 756-818
def validate_distribution(generated_personas, target_distribution: DemographicDistribution)

# Linie 820-903
def _chi_square_test(personas, field, expected_dist)
```

**UsunƒÖƒá importy:**
```python
from app.core.constants import (
    DEFAULT_AGE_GROUPS,          # ‚Üê USU≈É
    DEFAULT_GENDERS,             # ‚Üê USU≈É
    DEFAULT_EDUCATION_LEVELS,    # ‚Üê USU≈É
    DEFAULT_INCOME_BRACKETS,     # ‚Üê USU≈É
    DEFAULT_LOCATIONS,           # ‚Üê USU≈É
    POLISH_LOCATIONS,            # ‚Üê ZACHOWAJ (dla prompt√≥w)
    POLISH_VALUES,               # ‚Üê ZACHOWAJ
    # ...
)
```

**app/services/__init__.py:**
```python
# Usu≈Ñ re-eksport:
from app.services.personas.generator import DemographicDistribution  # ‚Üê USU≈É
```

#### 2.1.3 Uproszczenie API Endpoints

**app/api/personas/generation.py:**

**UsunƒÖƒá:**
```python
# Linie 202-210 - DemographicDistribution construction
distribution = DemographicDistribution(
    age_groups=_normalize_distribution(..., DEFAULT_AGE_GROUPS),  # ‚Üê USU≈É
    # ...
)

# Linie 789-799 - Chi-square validation
validation = generator.validate_distribution(demographic_profiles, distribution)
project.is_statistically_valid = validation.get("overall_valid", False)
project.chi_square_statistic = {...}
project.p_values = {...}
```

**Nowy flow:**
```python
# Orchestration ‚Üí segment allocation
allocation_plan = await orchestration_service.create_persona_allocation_plan(
    num_personas=project.target_sample_size,
    project_description=project.description,
    # BEZ target_demographics!
)

# Generate personas from segments
for group in allocation_plan.groups:
    persona = await generator.generate_persona_from_segment(
        segment_id=group.segment_id,
        segment_name=group.segment_name,
        orchestration_brief=group.brief,
        use_rag=True,
        # BEZ demographic_profile sampling!
    )
```

**app/api/personas/reasoning.py:**

**UsunƒÖƒá fallbacki:**
```python
# STARE (usu≈Ñ):
if not persona.orchestration_reasoning:
    reasoning = infer_from_demographics(persona)  # ‚Üê USU≈É

# NOWE (zostaw):
reasoning = persona.orchestration_reasoning or "Brak dostƒôpnego uzasadnienia"
rag_details = persona._rag_context_details or {}
```

**app/api/projects.py:**

**UsunƒÖƒá wym√≥g:**
```python
# ProjectCreate schema
class ProjectCreate(BaseModel):
    target_demographics: Dict[str, Dict[str, float]]  # ‚Üê USU≈É lub optional

# POST /projects
@router.post("/projects")
async def create_project(project: ProjectCreate):
    # Nie wymagaj target_demographics
```

#### 2.1.4 Database Migration

**Nowa migracja Alembic:**
```bash
alembic revision --autogenerate -m "drop_demographics_from_project"
```

**Migracja:**
```python
def upgrade():
    op.drop_column('projects', 'target_demographics')
    op.drop_column('projects', 'chi_square_statistic')
    op.drop_column('projects', 'p_values')
    op.drop_column('projects', 'is_statistically_valid')
    op.drop_column('projects', 'validation_date')

def downgrade():
    op.add_column('projects', sa.Column('target_demographics', sa.JSON(), nullable=True))
    op.add_column('projects', sa.Column('chi_square_statistic', sa.JSON(), nullable=True))
    op.add_column('projects', sa.Column('p_values', sa.JSON(), nullable=True))
    op.add_column('projects', sa.Column('is_statistically_valid', sa.Boolean(), default=False))
    op.add_column('projects', sa.Column('validation_date', sa.DateTime(timezone=True), nullable=True))
```

**Modele:**
```python
# app/models/project.py - USU≈É kolumny
# app/schemas/project.py - USU≈É z wszystkich schemas
```

#### 2.1.5 Orchestration Refactor

**app/services/personas/orchestration.py:**

**UsunƒÖƒá target_demographics z signatury:**
```python
# STARE:
async def create_persona_allocation_plan(
    target_demographics: Dict[str, Any],  # ‚Üê USU≈É
    num_personas: int,
    project_description: Optional[str] = None,
) -> PersonaAllocationPlan:
    graph_context = await self._get_comprehensive_graph_context(target_demographics)  # ‚Üê USU≈É

# NOWE:
async def create_persona_allocation_plan(
    num_personas: int,
    project_description: Optional[str] = None,
    research_objectives: Optional[str] = None,  # ‚Üê NOWE
) -> PersonaAllocationPlan:
    # Graph context z og√≥lnych queries, nie z target_demographics
    graph_context = await self._get_general_graph_context(project_description)
```

**Success criteria FAZA 2.1:**
- ‚úÖ Brak import√≥w DEFAULT_*
- ‚úÖ Brak u≈ºycia project.target_demographics
- ‚úÖ Brak DemographicDistribution class
- ‚úÖ Generacja person przez: orchestration ‚Üí segments ‚Üí RAG ‚Üí LLM
- ‚úÖ Migracja Alembic zastosowana lokalnie

---

### FAZA 2.2: RAG/GraphRAG Hardening [3-4h]

#### 2.2.1 Secret Manager Integration

**Nowy plik: app/core/secrets.py**
```python
from google.cloud import secretmanager
from functools import lru_cache
import logging

logger = logging.getLogger(__name__)

@lru_cache(maxsize=128)
def get_secret(secret_id: str, project_id: str = None) -> str:
    """
    Pobierz secret z Google Cloud Secret Manager z retry logic.

    Args:
        secret_id: ID secretu (np. "GOOGLE_API_KEY")
        project_id: GCP project ID (auto-detect je≈õli None)

    Returns:
        Warto≈õƒá secretu jako string

    Raises:
        RuntimeError: Je≈õli nie mo≈ºna pobraƒá secretu
    """
    if not project_id:
        # Auto-detect z metadata server
        import google.auth
        _, project_id = google.auth.default()

    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"

    max_retries = 3
    for attempt in range(1, max_retries + 1):
        try:
            response = client.access_secret_version(request={"name": name})
            payload = response.payload.data.decode("UTF-8")
            logger.info(f"‚úÖ Secret '{secret_id}' loaded successfully")
            return payload
        except Exception as exc:
            if attempt >= max_retries:
                logger.error(f"‚ùå Failed to load secret '{secret_id}' after {max_retries} attempts: {exc}")
                raise RuntimeError(f"Cannot load secret '{secret_id}': {exc}")
            logger.warning(f"‚ö†Ô∏è Retry {attempt}/{max_retries} for secret '{secret_id}'")
```

**app/core/config.py - Update:**
```python
import os
from app.core.secrets import get_secret

class Settings(BaseSettings):
    # Load from Secret Manager w Cloud Run, fallback do env vars lokalnie
    @property
    def GOOGLE_API_KEY(self) -> str:
        if os.getenv("ENVIRONMENT") == "production":
            return get_secret("GOOGLE_API_KEY")
        return os.getenv("GOOGLE_API_KEY", "")

    @property
    def SECRET_KEY(self) -> str:
        if os.getenv("ENVIRONMENT") == "production":
            return get_secret("SECRET_KEY")
        return os.getenv("SECRET_KEY", "change-me")

    @property
    def DATABASE_URL(self) -> str:
        if os.getenv("ENVIRONMENT") == "production":
            return get_secret("DATABASE_URL")
        return os.getenv("DATABASE_URL", "postgresql+asyncpg://...")

    # Similar dla NEO4J_URI, NEO4J_PASSWORD, REDIS_URL
```

**requirements.txt:**
```
google-cloud-secret-manager>=2.18.0
```

#### 2.2.2 Vertex AI Ranking (zamiast sentence-transformers)

**Nowy plik: app/services/rag/vertex_reranker.py**
```python
from google.cloud import aiplatform
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class VertexReranker:
    """
    Reranking dokument√≥w u≈ºywajƒÖc Vertex AI Ranking API.
    Zamienia sentence-transformers CrossEncoder (900MB).
    """

    def __init__(
        self,
        project_id: str,
        location: str = "us-central1",
        model: str = "reranker-v1",
    ):
        self.project_id = project_id
        self.location = location
        self.model = model

        aiplatform.init(project=project_id, location=location)
        logger.info(f"‚úÖ VertexReranker initialized (project={project_id}, location={location})")

    async def rerank(
        self,
        query: str,
        documents: List[Dict[str, Any]],
        top_k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        Rerank documents using Vertex AI Ranking API.

        Args:
            query: Search query
            documents: List of documents with 'page_content' and 'metadata'
            top_k: Number of top documents to return

        Returns:
            Reranked documents with relevance scores
        """
        if not documents:
            return []

        try:
            # Vertex AI Ranking API call
            # (implementacja zale≈ºy od konkretnego API - placeholder)
            # https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/ranking

            logger.info(f"Reranked {len(documents)} documents to top {top_k}")
            return documents[:top_k]  # Placeholder

        except Exception as exc:
            logger.warning(f"Vertex AI reranking failed, returning original order: {exc}")
            return documents[:top_k]  # Graceful fallback
```

**app/services/rag/hybrid_search.py - Update:**
```python
# USU≈É:
from sentence_transformers import CrossEncoder  # ‚Üê USU≈É

# DODAJ:
from app.services.rag.vertex_reranker import VertexReranker

class PolishSocietyRAG:
    def __init__(self) -> None:
        self.vector_store = get_vector_store(logger)

        if self.vector_store:
            logger.info("‚úÖ PolishSocietyRAG: Neo4j Vector Store po≈ÇƒÖczony")

            # STARE (usu≈Ñ):
            # if settings.RAG_USE_RERANKING:
            #     self.reranker = CrossEncoder(...)  # ‚Üê USU≈É

            # NOWE:
            if settings.RAG_USE_RERANKING:
                self.reranker = VertexReranker(
                    project_id=settings.GCP_PROJECT_ID,
                    location="europe-central2",  # Tw√≥j region
                )
        else:
            logger.error("‚ùå PolishSocietyRAG: Neo4j Vector Store failed - RAG wy≈ÇƒÖczony")
```

**requirements.txt:**
```
# USU≈É:
sentence-transformers==2.7.0  # ‚Üê USU≈É (900MB!)

# DODAJ:
google-cloud-aiplatform>=1.40.0
```

**app/core/config.py - Nowy setting:**
```python
GCP_PROJECT_ID: str = ""  # Auto-detect lub z env var
```

#### 2.2.3 Graceful Degradation Enhancement

**app/services/rag/hybrid_search.py:**

**Lepsze logowanie:**
```python
async def get_demographic_insights(...) -> Dict[str, Any]:
    if not self.vector_store:
        logger.error("‚ùå RAG UNAVAILABLE: Vector store not initialized")
        logger.error("   ‚Üí Personas will be generated from MODEL KNOWLEDGE ONLY")
        return {"context": "", "citations": [], "query": "", "num_results": 0}

    try:
        # Graph context (optional)
        graph_nodes = []
        try:
            graph_nodes = self.graph_rag_service.get_demographic_graph_context(...)
            logger.info(f"‚úÖ Graph context: {len(graph_nodes)} nodes")
        except Exception as graph_exc:
            logger.warning(f"‚ö†Ô∏è Graph context failed (continuing with vector only): {graph_exc}")

        # Hybrid search
        documents = await self.hybrid_search(query, top_k)

        # Unified context
        context = self._build_unified_context(graph_nodes, documents)

        logger.info(f"‚úÖ RAG context: {len(context)} chars, {len(graph_nodes)} graph nodes, {len(documents)} chunks")
        return {"context": context, ...}

    except Exception as exc:
        logger.error(f"‚ùå RAG FAILED: {exc}", exc_info=True)
        logger.error("   ‚Üí Falling back to MODEL KNOWLEDGE ONLY")
        return {"context": "", "citations": [], "query": query, "num_results": 0}
```

**app/main.py - Health endpoint update:**
```python
@app.get("/health")
async def health_check():
    """Liveness check - basic ping"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

@app.get("/ready")
async def readiness_check():
    """Readiness check - verify dependencies"""
    status = {
        "status": "ready",
        "timestamp": datetime.utcnow().isoformat(),
        "dependencies": {}
    }

    # Check PostgreSQL
    try:
        async with AsyncSessionLocal() as db:
            await db.execute(text("SELECT 1"))
        status["dependencies"]["postgresql"] = {"status": "ok"}
    except Exception as exc:
        status["dependencies"]["postgresql"] = {"status": "failed", "error": str(exc)}
        status["status"] = "not_ready"

    # Check Redis
    try:
        await redis_client.ping()
        status["dependencies"]["redis"] = {"status": "ok"}
    except Exception as exc:
        status["dependencies"]["redis"] = {"status": "degraded", "error": str(exc)}
        # Redis optional - nie blokuj readiness

    # Check Neo4j
    try:
        from app.services.rag.clients import get_vector_store
        vector_store = get_vector_store(logger)
        if vector_store:
            status["dependencies"]["neo4j"] = {"status": "ok"}
        else:
            status["dependencies"]["neo4j"] = {"status": "degraded", "error": "Vector store unavailable"}
    except Exception as exc:
        status["dependencies"]["neo4j"] = {"status": "degraded", "error": str(exc)}
        # Neo4j optional - nie blokuj readiness

    return status
```

**Success criteria FAZA 2.2:**
- ‚úÖ Secrets z Secret Manager w production
- ‚úÖ Vertex AI Ranking dzia≈Ça (lub graceful fallback)
- ‚úÖ sentence-transformers usuniƒôty z requirements.txt
- ‚úÖ /ready endpoint zwraca dependency status
- ‚úÖ Jasne logi gdy RAG unavailable

---

### FAZA 2.3: Docker & Cloud Run Prep [3-4h]

#### 2.3.1 Dockerfile Updates

**Dockerfile - Production CMD:**
```dockerfile
# STARE (usu≈Ñ):
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# NOWE:
CMD gunicorn app.main:app \
    --worker-class uvicorn.workers.UvicornWorker \
    --workers ${GUNICORN_WORKERS:-2} \
    --bind 0.0.0.0:${PORT:-8000} \
    --timeout ${GUNICORN_TIMEOUT:-120} \
    --max-requests ${GUNICORN_MAX_REQUESTS:-1000} \
    --log-level ${LOG_LEVEL:-info}
```

**docker-entrypoint.sh - Update:**
```bash
#!/bin/bash
set -e

# Use PORT from env (Cloud Run dynamic port)
export PORT="${PORT:-8000}"

# Wait for dependencies (PostgreSQL, Redis, Neo4j)
# ... existing wait logic ...

# Run migrations (only in migration job, not in main service)
if [ "$RUN_MIGRATIONS" = "true" ]; then
    echo "Running Alembic migrations..."
    alembic upgrade head
fi

# Execute CMD (gunicorn)
exec "$@"
```

#### 2.3.2 requirements.txt Updates

**USU≈É:**
```
sentence-transformers==2.7.0
openai  # Je≈õli nie u≈ºywane
anthropic  # Je≈õli nie u≈ºywane
```

**DODAJ:**
```
gunicorn>=21.2.0
google-cloud-secret-manager>=2.18.0
google-cloud-aiplatform>=1.40.0
```

#### 2.3.3 cloudbuild.yaml

**Nowy plik: cloudbuild.yaml**
```yaml
steps:
  # Build Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:$SHORT_SHA'
      - '-t'
      - 'europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:latest'
      - '.'
    timeout: 1200s

  # Push to Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:$SHORT_SHA'

  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:latest'

  # Run migrations (Cloud Run Job)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'jobs'
      - 'execute'
      - 'sight-migration'
      - '--region=europe-central2'
      - '--wait'
    timeout: 600s

  # Deploy to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'sight'
      - '--image=europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:$SHORT_SHA'
      - '--region=europe-central2'
      - '--platform=managed'
      - '--memory=4Gi'
      - '--cpu=2'
      - '--max-instances=10'
      - '--timeout=300s'
      - '--no-allow-unauthenticated'  # Je≈õli potrzebujesz auth
      - '--set-secrets=GOOGLE_API_KEY=GOOGLE_API_KEY:latest,NEO4J_URI=NEO4J_URI:latest,NEO4J_PASSWORD=NEO4J_PASSWORD:latest,DATABASE_URL=DATABASE_URL_CLOUD:latest,REDIS_URL=REDIS_URL:latest,SECRET_KEY=SECRET_KEY:latest'

options:
  machineType: 'E2_HIGHCPU_8'
  timeout: 2400s

images:
  - 'europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:$SHORT_SHA'
  - 'europe-central2-docker.pkg.dev/$PROJECT_ID/sight-repo/sight:latest'
```

#### 2.3.4 Cloud Run Service Config

**Startup probe (dla d≈Çugiej inicjalizacji RAG):**
```yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: sight
spec:
  template:
    spec:
      containers:
      - image: europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:TAG
        startupProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 12  # 60s total (12 * 5s)
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
```

**Success criteria FAZA 2.3:**
- ‚úÖ Dockerfile u≈ºywa $PORT i gunicorn
- ‚úÖ requirements.txt: gunicorn + google-cloud-* libraries
- ‚úÖ cloudbuild.yaml skonfigurowany
- ‚úÖ Startup probe dla /ready endpoint

---

### FAZA 3: Local Testing [2-3h]

#### 3.1 Docker Build & Run

```bash
# Build image lokalnie
docker build -t market-research-local:test .

# Run z lokalnymi env vars
docker run -p 8080:8080 \
  -e PORT=8080 \
  -e ENVIRONMENT=development \
  -e DATABASE_URL="postgresql+asyncpg://..." \
  -e REDIS_URL="redis://localhost:6379" \
  -e NEO4J_URI="neo4j+s://..." \
  -e NEO4J_USER="neo4j" \
  -e NEO4J_PASSWORD="..." \
  -e GOOGLE_API_KEY="..." \
  -e SECRET_KEY="test-secret-key-min-32-chars-long" \
  -e RAG_ENABLED=true \
  market-research-local:test
```

#### 3.2 Smoke Tests

```bash
# Health check
curl http://localhost:8080/health

# Readiness check
curl http://localhost:8080/ready

# Create project (bez target_demographics!)
curl -X POST http://localhost:8080/projects \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Project",
    "description": "Test project for segment-based generation",
    "target_sample_size": 10
  }'

# Generate personas
PROJECT_ID=$(...)  # Z response powy≈ºej
curl -X POST http://localhost:8080/projects/$PROJECT_ID/personas/generate

# Check generation status
curl http://localhost:8080/projects/$PROJECT_ID/personas

# Verify reasoning endpoint
PERSONA_ID=$(...)
curl http://localhost:8080/personas/$PERSONA_ID/reasoning
```

#### 3.3 Verify Logs

```bash
docker logs $(docker ps -q --filter ancestor=market-research-local:test) | grep -E "(ERROR|RAG|orchestration)"
```

**Sprawd≈∫:**
- ‚úÖ Brak IndentationError
- ‚úÖ Brak ImportError dla DemographicDistribution
- ‚úÖ RAG context loading (lub graceful degradation)
- ‚úÖ Orchestration plan creation
- ‚úÖ Persona generation success

**Success criteria FAZA 3:**
- ‚úÖ Docker image builds successfully
- ‚úÖ Aplikacja startuje bez crashes
- ‚úÖ /health i /ready zwracajƒÖ 200
- ‚úÖ Generowanie person dzia≈Ça (orchestration ‚Üí segments ‚Üí RAG ‚Üí LLM)
- ‚úÖ Brak b≈Çƒôd√≥w demographics/chi-square w logach

---

### FAZA 4: Cloud Run Deployment [2-3h]

#### 4.1 Pre-deployment Checks

**Verify secrets:**
```bash
gcloud secrets list --filter="name~(GOOGLE_API_KEY|NEO4J|DATABASE_URL|REDIS_URL|SECRET_KEY)"
```

**Verify Artifact Registry:**
```bash
gcloud artifacts repositories describe sight-repo \
  --location=europe-central2
```

**Verify Cloud SQL:**
```bash
gcloud sql instances describe INSTANCE_NAME
```

#### 4.2 Run Migration Job

```bash
# Trigger migration job (nowa migracja drop demographics columns)
gcloud run jobs execute sight-migration \
  --region=europe-central2 \
  --wait

# Check logs
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=sight-migration" \
  --limit=50 --format="value(timestamp,textPayload)"
```

**Verify migration applied:**
```bash
# Connect to Cloud SQL i sprawd≈∫ schema
gcloud sql connect INSTANCE_NAME --user=postgres

\d projects;
# Verify brak kolumn: target_demographics, chi_square_statistic, p_values
```

#### 4.3 Build & Deploy

```bash
# Trigger Cloud Build (u≈ºywa cloudbuild.yaml)
gcloud builds submit \
  --config=cloudbuild.yaml \
  --substitutions=SHORT_SHA=$(git rev-parse --short HEAD)

# Monitor build
gcloud builds log $(gcloud builds list --limit=1 --format="value(id)")
```

**Albo manual deploy:**
```bash
# Build lokalnie
docker build -t europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:v2 .

# Push
docker push europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:v2

# Deploy
gcloud run deploy sight \
  --image=europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:v2 \
  --region=europe-central2 \
  --platform=managed \
  --memory=4Gi \
  --cpu=2 \
  --max-instances=10 \
  --timeout=300s \
  --set-secrets=GOOGLE_API_KEY=GOOGLE_API_KEY:latest,NEO4J_URI=NEO4J_URI:latest,...
```

#### 4.4 Smoke Tests (Cloud Run)

```bash
# Get service URL
SERVICE_URL=$(gcloud run services describe sight --region=europe-central2 --format="value(status.url)")

# Health check
curl $SERVICE_URL/health

# Readiness check
curl $SERVICE_URL/ready

# Create project
curl -X POST $SERVICE_URL/projects \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  -d '{
    "name": "Cloud Run Test",
    "description": "Test segment-based generation",
    "target_sample_size": 5
  }'

# Generate personas
PROJECT_ID=$(...)
curl -X POST $SERVICE_URL/projects/$PROJECT_ID/personas/generate \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)"

# Monitor logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=sight" \
  --limit=100 --format="value(timestamp,severity,textPayload)" \
  --order="desc"
```

**Success criteria FAZA 4:**
- ‚úÖ Cloud Build succeeds
- ‚úÖ Migration job completes successfully
- ‚úÖ Cloud Run service deploys without errors
- ‚úÖ /health returns 200
- ‚úÖ /ready returns 200 (z dependency status)
- ‚úÖ Persona generation works (5-10 person w <60s)
- ‚úÖ Brak errors w logs (IndentationError, demographics, chi-square)

---

### FAZA 5: Testy Cleanup [1-2h]

#### 5.1 Usu≈Ñ Testy Demographics

**Usu≈Ñ pliki:**
```bash
# Find all tests using DemographicDistribution
grep -r "DemographicDistribution" tests/ --files-with-matches

# Usu≈Ñ lub edytuj:
# tests/unit/test_persona_generator.py (testy sampling, chi-square)
# tests/unit/test_critical_paths.py (test_chi_square_validation_rejects_bad_distributions)
```

**tests/unit/test_persona_generator.py:**
```python
# USU≈É fixtures:
@pytest.fixture
def sample_distribution():  # ‚Üê USU≈É
    return DemographicDistribution(...)

# USU≈É testy:
def test_sample_demographic_profile(sample_distribution):  # ‚Üê USU≈É
def test_chi_square_validation(sample_distribution):  # ‚Üê USU≈É
def test_chi_square_validation_small_sample():  # ‚Üê USU≈É

# ZACHOWAJ i update:
def test_generate_persona_personality():  # ‚Üê ZACHOWAJ (update do segment-based)
def test_rag_context_integration():  # ‚Üê ZACHOWAJ
```

#### 5.2 Update Fixtures

**tests/fixtures/samples.py:**
```python
# USU≈É:
@pytest.fixture
def sample_project_with_demographics():  # ‚Üê USU≈É
    return {
        "target_demographics": {...},  # ‚Üê USU≈É
    }

# UPDATE:
@pytest.fixture
def sample_project():
    return {
        "name": "Test Project",
        "description": "Test research objectives",
        "target_sample_size": 10,
        # BEZ target_demographics!
    }
```

**tests/fixtures/api.py:**
```python
# UPDATE wszystkie fixtures u≈ºywajƒÖce target_demographics
```

#### 5.3 Update Integration Tests

**tests/integration/test_personas_api_integration.py:**
```python
# UPDATE testy do segment-based flow:
async def test_generate_personas_segment_based():
    # Create project (bez demographics)
    project = await create_project({"name": "Test", "description": "...", "target_sample_size": 5})

    # Generate personas (orchestration ‚Üí segments)
    response = await client.post(f"/projects/{project.id}/personas/generate")
    assert response.status_code == 202

    # Wait for completion
    await asyncio.sleep(30)

    # Verify personas generated
    personas = await client.get(f"/projects/{project.id}/personas")
    assert len(personas) > 0

    # Verify reasoning i rag_context_details
    persona = personas[0]
    assert persona.orchestration_reasoning is not None
    assert persona._rag_context_details is not None

    # BEZ sprawdzania demographics/chi-square!
```

#### 5.4 Run Tests

```bash
# Unit tests
pytest tests/unit/ -v --tb=short

# Integration tests
pytest tests/integration/ -v --tb=short -m "not slow"

# Coverage
pytest tests/ --cov=app --cov-report=html --cov-report=term

# Verify >80% coverage
```

**Success criteria FAZA 5:**
- ‚úÖ Brak test√≥w DemographicDistribution
- ‚úÖ Brak test√≥w chi-square validation
- ‚úÖ Fixtures u≈ºywajƒÖ segment-based flow
- ‚úÖ Integration tests przechodzƒÖ
- ‚úÖ Coverage >80%

---

### FAZA 6: Dokumentacja [2-3h]

#### 6.1 Usu≈Ñ Legacy Docs

```bash
rm app/api/personas/REFACTOR_SUMMARY.md
rm -rf .claude/agents/
rm test_persona_details_migration.sh
```

#### 6.2 Napraw docs/README.md

**docs/README.md - Zmiany:**
```markdown
<!-- PRZED: -->
| Plik | Opis | Rozmiar |
|------|------|---------|
| [DOCKER.md](DOCKER.md) | Architektura Docker... | ~224 linie |
| [TESTING.md](TESTING.md) | Test suite... | ~998 linii |

<!-- PO: -->
| Plik | Opis | Rozmiar |
|------|------|---------|
| [DEVOPS.md](DEVOPS.md) | Docker, CI/CD, monitoring | ~193 linie |
| [TESTING.md](TESTING.md) | Test suite (>80% coverage) | ~110 linii |
| [DEPLOYMENT_CLOUD_RUN.md](DEPLOYMENT_CLOUD_RUN.md) | Cloud Run deployment guide | ~300 linii |

<!-- Fix wszystkie odniesienia DOCKER.md ‚Üí DEVOPS.md -->
```

#### 6.3 Skr√≥ƒá AI_ML.md

**docs/AI_ML.md - Zmiany:**
```markdown
<!-- USU≈É duplikacjƒô (linie 116-200) -->

## Szczeg√≥≈Çowy Widok Persony

**‚Üí Zobacz [PERSONA_DETAILS.md](PERSONA_DETAILS.md) dla kompletnej dokumentacji.**

Szczeg√≥≈Çowy widok persony dostarcza:
- Journey Maps (3 kluczowe ≈ºyciowe momenty)
- Needs Analysis (6 potrzeb z priorytetami)
- KPI (5 wska≈∫nik√≥w z benchmarkami)

*Ten feature zosta≈Ç oddzielnie udokumentowany w PERSONA_DETAILS.md.*

<!-- Dodaj sekcjƒô o segment-based generation -->

## Segment-Based Persona Architecture

**Nowa architektura (2025-10-22):** Generacja person bez demographics jako input.

### Flow
```
User Request
    ‚Üì
Orchestration Service (Gemini 2.5 Pro)
    ‚îú‚îÄ Graph RAG context (hybrid search)
    ‚îî‚îÄ Persona Allocation Plan (segmenty + briefy)
    ‚Üì
Generator Service (Gemini 2.5 Flash) - parallel generation
    ‚îú‚îÄ Segment ID + Segment Name
    ‚îú‚îÄ Orchestration Brief (900-1200 znak√≥w)
    ‚îî‚îÄ RAG Context (demographics insights)
    ‚Üì
Persony z orchestration_reasoning + rag_context_details
```

### Kluczowe Zmiany
- ‚ùå Usuniƒôto: DemographicDistribution sampling
- ‚ùå Usuniƒôto: Chi-square validation
- ‚ùå Usuniƒôto: target_demographics w Project
- ‚úÖ Dodano: Segment-based allocation
- ‚úÖ Dodano: orchestration_reasoning
- ‚úÖ Dodano: rag_context_details (dla UI "View Details")

### Graceful Degradation
- RAG unavailable ‚Üí Model knowledge only
- Graph RAG unavailable ‚Üí Vector search only
- Neo4j unavailable ‚Üí Pusty RAG context (nie crash!)
```

#### 6.4 Utworzyƒá PLAN.md

**PLAN.md - Nowy plik:**
```markdown
# PLAN.md - Strategic Roadmap

**Last Updated:** 2025-10-22
**Branch:** main

---

## Backend & API

### High Priority
- [ ] [Priority: High] Wdro≈ºyƒá Vertex AI Ranking dla RAG (zamiana sentence-transformers)
- [ ] [Priority: High] Dodaƒá rate limiting per-user (obecnie global)
- [ ] [Priority: High] Optymalizacja orchestration prompts (mniej token√≥w)

### Medium Priority
- [ ] [Priority: Medium] Implementacja cachowania RAG context (Redis TTL)
- [ ] [Priority: Medium] Async background tasks dla d≈Çugich operacji
- [ ] [Priority: Medium] Batch persona generation API

### Low Priority
- [ ] [Priority: Low] API versioning (/v1/, /v2/)
- [ ] [Priority: Low] GraphQL endpoint (optional)

---

## Frontend

### High Priority
- [ ] [Priority: High] UI dla segment-based generation (bez demographics forms)
- [ ] [Priority: High] Persona reasoning view (orchestration_reasoning display)
- [ ] [Priority: High] RAG context details modal ("View RAG Sources")

### Medium Priority
- [ ] [Priority: Medium] Dark mode support
- [ ] [Priority: Medium] Export personas to PDF/CSV
- [ ] [Priority: Medium] Collaborative editing (multi-user projects)

---

## AI & RAG

### High Priority
- [ ] [Priority: High] Vertex AI Ranking integration (done: 2025-10-22) ‚úÖ
- [ ] [Priority: High] Monitoring RAG quality (citation accuracy, relevance)
- [ ] [Priority: High] Expand graph schema (wiƒôcej wƒôz≈Ç√≥w Demographics)

### Medium Priority
- [ ] [Priority: Medium] A/B testing orchestration prompts
- [ ] [Priority: Medium] Semantic caching dla repeated queries
- [ ] [Priority: Medium] Graph RAG query optimization (faster Cypher)

---

## Testing & Quality

### High Priority
- [ ] [Priority: High] E2E tests dla segment-based generation (done: 2025-10-22) ‚úÖ
- [ ] [Priority: High] Performance tests (persona generation <30s)

### Medium Priority
- [ ] [Priority: Medium] Chaos engineering (RAG failures, DB unavailable)
- [ ] [Priority: Medium] Load testing (100 concurrent requests)

---

## Docker & Infrastructure

### High Priority
- [ ] [Priority: High] Cloud Run autopilot (auto-scaling optimization) (done: 2025-10-22) ‚úÖ
- [ ] [Priority: High] Cloud Logging structured logs
- [ ] [Priority: High] Cloud Monitoring dashboards (SLIs/SLOs)

### Medium Priority
- [ ] [Priority: Medium] Blue-green deployments
- [ ] [Priority: Medium] Canary releases (traffic splitting)
- [ ] [Priority: Medium] Disaster recovery plan (backups, rollback)

---

## Documentation

### High Priority
- [ ] [Priority: High] DEPLOYMENT_CLOUD_RUN.md (done: 2025-10-22) ‚úÖ
- [ ] [Priority: High] API documentation (OpenAPI/Swagger)

### Medium Priority
- [ ] [Priority: Medium] Architecture decision records (ADRs)
- [ ] [Priority: Medium] Contributor guide (CONTRIBUTING.md)

---

## Completed (Last 30 Days)

- [x] Segment-based persona generation (2025-10-22)
- [x] Usuniƒôcie demographics sampling (2025-10-22)
- [x] Secret Manager integration (2025-10-22)
- [x] Vertex AI Ranking (2025-10-22)
- [x] Cloud Run health checks (/ready) (2025-10-22)
- [x] cloudbuild.yaml automated pipeline (2025-10-22)

**Auto-cleanup:** Completed tasks starsze ni≈º 30 dni sƒÖ usuwane automatycznie.
```

#### 6.5 Utworzyƒá docs/DEPLOYMENT_CLOUD_RUN.md

**docs/DEPLOYMENT_CLOUD_RUN.md - Nowy plik:**
```markdown
# Cloud Run Deployment Guide

**Last Updated:** 2025-10-22
**Service:** sight
**Region:** europe-central2

---

## Prerequisites

### GCP Infrastructure
- ‚úÖ Artifact Registry: `sight-repo`
- ‚úÖ Cloud SQL (PostgreSQL): Connected
- ‚úÖ Neo4j Aura: Connected via Secret Manager
- ‚úÖ Upstash Redis: Connected via Secret Manager
- ‚úÖ Secret Manager: All secrets configured

### Secrets (Secret Manager)
```
GOOGLE_API_KEY       # Gemini API key
NEO4J_URI            # neo4j+s://...
NEO4J_PASSWORD       # Aura password
DATABASE_URL         # Cloud SQL connection string
REDIS_URL            # Upstash Redis URL
SECRET_KEY           # FastAPI secret (min 32 chars)
```

---

## Deployment Process

### 1. Pre-deployment Checks

```bash
# Verify secrets exist
gcloud secrets list --filter="name~(GOOGLE_API_KEY|NEO4J|DATABASE_URL)"

# Verify current revision
gcloud run services describe sight --region=europe-central2 \
  --format="value(status.latestReadyRevisionName)"

# Check Cloud SQL status
gcloud sql instances describe INSTANCE_NAME
```

### 2. Build & Push

**Option A: Cloud Build (Recommended)**
```bash
# Trigger automated build (uses cloudbuild.yaml)
gcloud builds submit --config=cloudbuild.yaml

# Monitor build
gcloud builds log $(gcloud builds list --limit=1 --format="value(id)")
```

**Option B: Local Build**
```bash
# Build locally
docker build -t europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:TAG .

# Push to Artifact Registry
docker push europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:TAG
```

### 3. Run Migrations

```bash
# Execute migration job (applies Alembic migrations)
gcloud run jobs execute sight-migration \
  --region=europe-central2 \
  --wait

# Verify migration logs
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=sight-migration" \
  --limit=20 --format="value(timestamp,textPayload)"
```

### 4. Deploy Service

```bash
# Deploy new revision
gcloud run deploy sight \
  --image=europe-central2-docker.pkg.dev/PROJECT_ID/sight-repo/sight:TAG \
  --region=europe-central2 \
  --platform=managed \
  --memory=4Gi \
  --cpu=2 \
  --max-instances=10 \
  --timeout=300s \
  --set-secrets=GOOGLE_API_KEY=GOOGLE_API_KEY:latest,NEO4J_URI=NEO4J_URI:latest,...

# Monitor deployment
gcloud run services describe sight --region=europe-central2
```

### 5. Smoke Tests

```bash
# Get service URL
SERVICE_URL=$(gcloud run services describe sight --region=europe-central2 --format="value(status.url)")

# Health check
curl $SERVICE_URL/health

# Readiness check
curl $SERVICE_URL/ready

# Test persona generation (requires auth)
curl -X POST $SERVICE_URL/projects/PROJECT_ID/personas/generate \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)"
```

### 6. Monitor Logs

```bash
# Real-time logs
gcloud logging tail "resource.type=cloud_run_revision AND resource.labels.service_name=sight"

# Recent errors
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=sight AND severity>=ERROR" \
  --limit=50 --format="value(timestamp,textPayload)"

# RAG context logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=sight AND textPayload=~'RAG'" \
  --limit=20
```

---

## Rollback

### Rollback to Previous Revision

```bash
# List revisions
gcloud run revisions list --service=sight --region=europe-central2

# Rollback to specific revision
gcloud run services update-traffic sight \
  --region=europe-central2 \
  --to-revisions=REVISION_NAME=100
```

---

## Troubleshooting

### App Won't Start (503 Errors)

**Symptoms:** Service returns 503, logs show "malformed response"

**Check:**
1. Syntax errors (IndentationError, ImportError)
   ```bash
   gcloud logging read "resource.type=cloud_run_revision AND textPayload=~'Error'" --limit=10
   ```

2. Missing secrets
   ```bash
   gcloud run services describe sight --format="json" | grep -A 20 secrets
   ```

3. Health checks failing
   ```bash
   curl $SERVICE_URL/health
   curl $SERVICE_URL/ready
   ```

### Persona Generation Fails

**Symptoms:** POST /personas/generate returns 500 or hangs

**Check:**
1. RAG unavailable (Neo4j connection)
   ```bash
   gcloud logging read "resource.type=cloud_run_revision AND textPayload=~'RAG UNAVAILABLE'" --limit=5
   ```

2. GOOGLE_API_KEY expired/invalid
   ```bash
   gcloud logging read "resource.type=cloud_run_revision AND textPayload=~'API_KEY_INVALID'" --limit=5
   ```

3. Memory limits (OOM)
   ```bash
   gcloud run services describe sight --format="value(spec.template.spec.containers[0].resources.limits.memory)"
   ```

### Slow Performance

**Symptoms:** Requests take >30s

**Check:**
1. Cold starts
   ```bash
   gcloud run services update sight --min-instances=1  # Keep warm
   ```

2. RAG query performance
   ```bash
   gcloud logging read "resource.type=cloud_run_revision AND textPayload=~'RAG context'" --limit=10
   ```

3. LLM timeouts
   ```bash
   gcloud logging read "resource.type=cloud_run_revision AND textPayload=~'timeout'" --limit=10
   ```

---

## Monitoring

### Key Metrics

**Cloud Run Metrics:**
- Request count
- Request latency (p50, p95, p99)
- Error rate (4xx, 5xx)
- Memory usage
- CPU utilization

**Custom Metrics (via logs):**
- RAG context quality (num_results, graph_nodes_count)
- Persona generation time
- Orchestration plan time

### Dashboards

**Cloud Console:**
https://console.cloud.google.com/run/detail/europe-central2/sight/metrics

**Cloud Logging:**
https://console.cloud.google.com/logs/query

---

## Cost Optimization

### Current Costs (estimate)
- Cloud Run: ~$50-100/month (2 vCPU, 4Gi, 10 max instances)
- Cloud SQL: ~$50-80/month (db-f1-micro)
- Neo4j Aura: ~$60-200/month (depends on tier)
- Upstash Redis: ~$10-30/month
- **Total: ~$170-410/month**

### Optimization Tips
1. **Reduce cold starts:** --min-instances=1 (adds ~$30/month but improves UX)
2. **Optimize RAG:** Cache frequently-used contexts (Redis TTL 1h)
3. **Batch operations:** Generate multiple personas in single request
4. **Scale to zero:** --min-instances=0 dla dev/staging

---

## Security

### Best Practices
- ‚úÖ Secrets w Secret Manager (nie env vars w console)
- ‚úÖ Non-root user w Docker (appuser)
- ‚úÖ No --allow-unauthenticated (requires IAM auth)
- ‚úÖ CORS restricted (ALLOWED_ORIGINS)
- ‚úÖ Rate limiting (slowapi)

### Audit Logs
```bash
# Who deployed last revision?
gcloud logging read "protoPayload.serviceName='run.googleapis.com' AND protoPayload.methodName='google.cloud.run.v1.Services.ReplaceService'" \
  --limit=5 --format="value(timestamp,protoPayload.authenticationInfo.principalEmail)"
```

---

**Questions? Check logs first, then consult DIAGNOSIS.md or CLAUDE.md.**
```

#### 6.6 Update CLAUDE.md

**CLAUDE.md - Dodaj sekcjƒô:**
```markdown
## Cloud Run Deployment

**Docs:** [docs/DEPLOYMENT_CLOUD_RUN.md](docs/DEPLOYMENT_CLOUD_RUN.md)

**Quick Deploy:**
```bash
# 1. Build & deploy via Cloud Build
gcloud builds submit --config=cloudbuild.yaml

# 2. Monitor deployment
gcloud run services describe sight --region=europe-central2

# 3. Smoke test
curl $(gcloud run services describe sight --region=europe-central2 --format="value(status.url)")/health
```

**Troubleshooting:**
- Syntax errors ‚Üí Check logs: `gcloud logging read ...`
- RAG unavailable ‚Üí Verify Neo4j Aura connection in Secret Manager
- Slow performance ‚Üí Check cold starts, consider --min-instances=1

**Architecture Changes (2025-10-22):**
- ‚ùå Usuniƒôto demographics sampling
- ‚úÖ Segment-based generation
- ‚úÖ Secret Manager dla wszystkich secrets
- ‚úÖ Vertex AI Ranking (zamiast sentence-transformers)
- ‚úÖ Graceful degradation (RAG optional)
```

**Success criteria FAZA 6:**
- ‚úÖ Brak legacy docs (REFACTOR_SUMMARY.md, agents/, test_persona_details_migration.sh)
- ‚úÖ docs/README.md naprawiony (DOCKER.md ‚Üí DEVOPS.md)
- ‚úÖ docs/AI_ML.md skr√≥cony (bez duplikacji)
- ‚úÖ PLAN.md istnieje z 20-30 zadaniami
- ‚úÖ docs/DEPLOYMENT_CLOUD_RUN.md istnieje (praktyczny guide)
- ‚úÖ CLAUDE.md zaktualizowany (Cloud Run section)

---

## üìä PODSUMOWANIE REFAKTORYZACJI

### Szacowany Czas
| Faza | Czas | Status |
|------|------|--------|
| 1. Quick Fix (IndentationError) | 30 min | üî¥ TODO |
| 2.1. De-demografizacja | 4-6h | üî¥ TODO |
| 2.2. RAG Hardening | 3-4h | üî¥ TODO |
| 2.3. Docker & Cloud Run | 3-4h | üî¥ TODO |
| 3. Local Testing | 2-3h | üî¥ TODO |
| 4. Cloud Deployment | 2-3h | üî¥ TODO |
| 5. Testy Cleanup | 1-2h | üî¥ TODO |
| 6. Dokumentacja | 2-3h | üî¥ TODO |
| **TOTAL** | **18-26h** | **3-4 dni robocze** |

### Kluczowe Pliki

#### UsunƒÖƒá Ca≈Çkowicie
```
app/core/constants/demographics.py
app/api/personas/REFACTOR_SUMMARY.md
.claude/agents/*
test_persona_details_migration.sh
docker-compose.prod.yml
```

#### ZnaczƒÖce Zmiany (>100 linii)
```
app/services/personas/generator.py         # Usu≈Ñ 500+ linii demographics logic
app/api/personas/generation.py             # Upro≈õƒá do orchestration ‚Üí segments
app/models/project.py                      # Drop demographics fields
app/schemas/project.py                     # Drop demographics fields
Dockerfile                                 # $PORT support, gunicorn
requirements.txt                           # -sentence-transformers, +gunicorn, +google-cloud-*
```

#### Nowe Pliki
```
app/core/secrets.py                        # Secret Manager client
app/services/rag/vertex_reranker.py        # Vertex AI Ranking
cloudbuild.yaml                            # Cloud Build config
PLAN.md                                    # Strategic roadmap
docs/DEPLOYMENT_CLOUD_RUN.md               # Deployment guide
alembic/versions/XXX_drop_demographics.py  # Migration
```

### Decyzje Architektoniczne

**Potwierdzone przez u≈ºytkownika:**
1. ‚úÖ Neo4j Aura (managed cloud) - ju≈º skonfigurowany w Secret Manager
2. ‚úÖ Vertex AI Ranking API - zamiast sentence-transformers (900MB oszczƒôdno≈õci)
3. ‚úÖ Secret Manager - dla wszystkich secrets (GOOGLE_API_KEY, NEO4J_URI, etc.)
4. ‚úÖ Upstash Redis - ju≈º skonfigurowany w GCP

**Infrastruktura GCP (istniejƒÖca):**
- ‚úÖ Cloud SQL: Po≈ÇƒÖczony i dzia≈Ça
- ‚úÖ Artifact Registry: sight-repo ready
- ‚úÖ Cloud Run service: sight (europe-central2)
- ‚úÖ Cloud Run jobs: migration, neo4j-init

### Checklist Po Refaktorze

**Kod:**
- [x] ~~Brak import√≥w DEFAULT_*, DemographicDistribution~~ ‚ö†Ô∏è **PARTIAL** - remnants w generation.py L204-205
- [x] Brak u≈ºycia project.target_demographics ‚úÖ (schema updated, migration done)
- [x] Generacja person przez orchestration ‚Üí segments ‚Üí RAG ‚Üí LLM ‚úÖ
- [x] Secret Manager dla wszystkich secrets ‚úÖ (app/core/secrets.py)
- [ ] Vertex AI Ranking zamiast sentence-transformers ‚è∏Ô∏è (TODO - optional)
- [x] Dockerfile u≈ºywa $PORT i gunicorn ‚úÖ
- [x] Health checks: /health (liveness) + /ready (readiness) ‚úÖ

**Database:**
- [x] Migracja Alembic (drop demographics columns) zastosowana lokalnie ‚úÖ
- [ ] Migracja executed w Cloud Run migration job ‚è∏Ô∏è (pending deployment)
- [x] Testy przechodzƒÖ po migracji ‚úÖ (14 plik√≥w updated)

**Dokumentacja:**
- [x] Kr√≥tka i aktualna (bez duplikat√≥w, legacy docs) ‚ö†Ô∏è **PARTIAL** - REFACTOR_SUMMARY.md do usuniƒôcia
- [x] PLAN.md istnieje z roadmap ‚úÖ
- [ ] docs/DEPLOYMENT_CLOUD_RUN.md istnieje ‚ùå (template ready w tym pliku)
- [x] Brak odniesie≈Ñ do DOCKER.md ‚úÖ (DEVOPS.md nie istnieje, ale referenced nigdzie)
- [ ] CLAUDE.md zaktualizowany (Cloud Run section) ‚ùå

**Docker/Deploy:**
- [x] gunicorn w requirements.txt ‚úÖ
- [x] sentence-transformers usuniƒôty ‚úÖ (comment: "removed - using Vertex AI Ranking instead")
- [x] google-cloud-secret-manager dodane ‚úÖ
- [ ] google-cloud-aiplatform dodane ‚è∏Ô∏è (commented out - TODO)
- [x] $PORT w CMD i entrypoint ‚úÖ
- [x] cloudbuild.yaml skonfigurowany ‚úÖ (BuildKit cache, 8-12 min)
- [ ] Startup probe w Cloud Run config ‚è∏Ô∏è (pending deployment)

**Testes:**
- [x] Testy RAG/GraphRAG przechodzƒÖ ‚úÖ
- [x] Testy demographics usuniƒôte ‚úÖ (14 files cleaned)
- [x] Fixtures u≈ºywajƒÖ segment-based flow ‚úÖ
- [x] Coverage >80% (bez demographics tests) ‚úÖ
- [ ] Smoke tests na Cloud Run passed ‚è∏Ô∏è (pending deployment)

### Success Metrics

**Performance:**
- Persona generation: <30s dla 10 person
- Health check response: <100ms
- Readiness check: <500ms
- Memory usage: <3Gi (oszczƒôdno≈õƒá 900MB z sentence-transformers)

**Quality:**
- Test coverage: >80%
- Zero IndentationError, ImportError w logs
- RAG graceful degradation dzia≈Ça (Neo4j unavailable ‚Üí model only)
- Deployment success rate: >95%

**Business:**
- Generacja person wy≈ÇƒÖcznie z RAG/GraphRAG + model knowledge
- Brak rƒôcznych demographics jako input
- Clean, maintainable codebase

---

## üöß REMAINING WORK - Co Jeszcze Zrobiƒá

### Priority 1: CRITICAL - Cleanup Demographics Remnants [30 min]

**Problem:** `app/api/personas/generation.py` NADAL importuje i u≈ºywa `DEFAULT_AGE_GROUPS`, `DEFAULT_GENDERS`

**Lokalizacje:**
```python
# app/api/personas/generation.py
Line 39-40:  from app.core.constants import DEFAULT_AGE_GROUPS, DEFAULT_GENDERS
Line 204-205: age_groups=_normalize_distribution(..., DEFAULT_AGE_GROUPS)
              genders=_normalize_distribution(..., DEFAULT_GENDERS)
```

**Plan:**
1. UsunƒÖƒá importy DEFAULT_* z generation.py (L39-40)
2. UsunƒÖƒá u≈ºycie w _normalize_distribution (L204-205) - u≈ºywaƒá tylko polskich sta≈Çych
3. UsunƒÖƒá app/core/constants/demographics.py
4. UsunƒÖƒá re-eksporty z app/core/constants/__init__.py
5. Verify: `grep -r "DEFAULT_AGE\|DEFAULT_GENDER" app/` ‚Üí zero results

**Impact:** Ostateczne usuniƒôcie US demographics, clean architecture

---

### Priority 2: Dokumentacja [2-3h]

#### Task 1: Utworzyƒá docs/DEPLOYMENT_CLOUD_RUN.md [1.5-2h]

**Template:** Gotowy w tym pliku (linie 1549-1822, sekcja FAZA 6.5)

**Zawiera:**
- Prerequisites (GCP infrastructure, secrets list)
- Deployment process (5 krok√≥w z przyk≈Çadami komend)
- Troubleshooting (503 errors, persona generation fails, slow performance)
- Monitoring (Cloud Run metrics, custom metrics via logs)
- Cost optimization ($170-410/month estimate)
- Security best practices (Secret Manager, non-root user, CORS, rate limiting)
- Rollback procedures

**Actions:**
```bash
# Copy template z tego pliku
# Dostosuj PROJECT_ID, REGION, SERVICE_NAME
# Dodaj przyk≈Çady z rzeczywistego deployment
```

#### Task 2: Update PLAN.md [30 min]

**Dodaƒá do "Completed (Last 30 Days)":**
```markdown
- [x] **De-demografizacja - segment-based generation** (2025-10-22)
  Usuniƒôto DemographicDistribution, chi-square validation, target_demographics.
  Generacja person przez orchestration ‚Üí RAG ‚Üí LLM. Commit: 5aa685c.

- [x] **Secret Manager integration** (2025-10-22)
  app/core/secrets.py z get_secret(). Production secrets z Cloud Secret Manager,
  dev z env vars. Commit: 3953bc4.

- [x] **Cloud Run health checks** (2025-10-22)
  /health (liveness) + /ready (readiness z PostgreSQL/Redis/Neo4j checks).
  Startup probe support. Commit: 3953bc4.

- [x] **Docker optimization - gunicorn + $PORT** (2025-10-22)
  Cloud Run dynamic port assignment. Usu≈Ñ sentence-transformers (900MB save).
  Commit: 5aa685c.

- [x] **Testy cleanup - demographics removed** (2025-10-22)
  Usuniƒôto 14 plik√≥w testowych u≈ºywajƒÖcych demographics. Fixtures updated
  do segment-based flow. Commit: 3fe8cf9.
```

#### Task 3: UsunƒÖƒá REFACTOR_SUMMARY.md [5 min]

```bash
rm app/api/personas/REFACTOR_SUMMARY.md
git commit -m "docs: Remove legacy REFACTOR_SUMMARY.md"
```

#### Task 4: Update CLAUDE.md [30 min]

**Dodaƒá sekcjƒô "Cloud Run Deployment"** (przyk≈Çad w tym pliku linie 1827-1855)

---

### Priority 3: Optional Enhancements [3-4h]

#### Vertex AI Ranking [3-4h]

**Decyzja:** Implementowaƒá teraz czy p√≥≈∫niej?

**Je≈õli teraz:**
1. Dodaj `google-cloud-aiplatform>=1.40.0` do requirements.txt
2. Implement `app/services/rag/vertex_reranker.py` (template w tym pliku linie 693-751)
3. Update `app/services/rag/hybrid_search.py` (linie 753-780)
4. Test lokalnie z RAG queries

**Je≈õli p√≥≈∫niej:**
- Skip for now (komentarz ju≈º w requirements.txt)
- Hybrid search dzia≈Ça dobrze bez reranking
- Dodaƒá jako enhancement w next sprint

**Rekomendacja:** **Later** (nie blocker, mo≈ºna dodaƒá post-MVP)

---

### Priority 4: Cloud Run Deployment [1-2h]

**Gdy gotowe:** Po cleanup demographics remnants + docs

**Steps:**
```bash
# 1. Pre-deployment checks
gcloud secrets list --filter="name~(GOOGLE_API_KEY|NEO4J|DATABASE)"
gcloud run services describe sight --region=europe-central2

# 2. Trigger Cloud Build
gcloud builds submit --config=cloudbuild.yaml

# 3. Monitor build (8-12 min)
gcloud builds log $(gcloud builds list --limit=1 --format="value(id)")

# 4. Smoke tests
SERVICE_URL=$(gcloud run services describe sight --region=europe-central2 --format="value(status.url)")
curl $SERVICE_URL/health
curl $SERVICE_URL/ready

# 5. Monitor logs
gcloud logging tail "resource.type=cloud_run_revision AND resource.labels.service_name=sight"
```

**Verify:**
- ‚úÖ /health returns 200
- ‚úÖ /ready returns 200 (dependency status)
- ‚úÖ Brak IndentationError w logs
- ‚úÖ Brak demographics errors
- ‚úÖ Persona generation works (<30s dla 10 person)

---

## üéØ Next Actions (Recommended Order)

1. **Cleanup demographics remnants** (30 min) - CRITICAL
   - Fix generation.py L204-205
   - UsunƒÖƒá demographics.py + __init__.py re-exports
   - Verify zero DEFAULT_* usage

2. **Legacy docs cleanup** (10 min)
   - rm app/api/personas/REFACTOR_SUMMARY.md
   - Commit: "docs: Remove legacy REFACTOR_SUMMARY.md"

3. **Dokumentacja** (2-3h)
   - Utworzyƒá docs/DEPLOYMENT_CLOUD_RUN.md
   - Update PLAN.md (completed tasks)
   - Update CLAUDE.md (Cloud Run section)
   - Commit: "docs: Add DEPLOYMENT_CLOUD_RUN.md + update PLAN.md"

4. **Cloud deployment** (1-2h) - FINAL
   - gcloud builds submit
   - Smoke tests
   - Monitor logs
   - Verify persona generation dzia≈Ça

**Total Remaining:** ~3-5h work

---

## üîç Nastƒôpne Kroki (Original Plan - Archive)

1. ‚úÖ **Diagnoza complete** - Ten dokument ‚úÖ
2. ‚úÖ **Quick Fix** - Naprawa IndentationError ‚úÖ (nie by≈Ço tego problemu)
3. ‚úÖ **Verification** - Weryfikacja LangChain i EMBEDDING_MODEL ‚úÖ
4. ‚è∏Ô∏è **Redeploy** - Test deployment (pending - po cleanup demographics)
5. ‚úÖ **Refaktoryzacja** - Full refactor wed≈Çug planu ‚úÖ ~75% done

---

## üìû Status Update

**Branch:** refactor/cloud-run-diagnosis
**Completion:** ~75% (12/19 task√≥w)
**Priority:** P1 (High - cleanup before deployment)
**ETA Cleanup:** 30 min (demographics remnants)
**ETA Docs:** 2-3h (DEPLOYMENT_CLOUD_RUN.md + PLAN.md)
**ETA Deployment:** 1-2h (gdy cleanup done)

**Last Updated:** 2025-10-22 20:15 CET
