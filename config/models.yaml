# Model Registry - Multi-LLM Support with Fallback chain
#
# Fallback order:
# 1. Domain-specific override (e.g., domains.personas.orchestration)
# 2. Domain defaults (e.g., domains.personas.generation)
# 3. Global defaults (defaults.chat)
#
# Multi-LLM Support:
#   from app.services.shared.llm_router import get_llm_router, TaskComplexity
#   router = get_llm_router()
#   llm = router.get_chat_model(task_complexity=TaskComplexity.SIMPLE)
#
# Legacy (single provider):
#   from config import models
#   model_config = models.get("personas", "orchestration")
#   llm = build_chat_model(**model_config.params)

# Multi-Provider Configuration
providers:
  # Fallback chain: Gemini → OpenAI → Anthropic
  fallback_chain:
    - gemini
    - openai
    - anthropic

  # Cost routing enabled by default
  cost_routing: true

  # Provider availability (set API keys in environment)
  gemini:
    enabled: true
    models:
      flash:
        name: "gemini-2.5-flash"
        cost_input_per_1m: 0.05  # $0.05 per 1M tokens
        cost_output_per_1m: 0.15  # $0.15 per 1M tokens
      pro:
        name: "gemini-2.5-pro"
        cost_input_per_1m: 1.00  # $1 per 1M tokens
        cost_output_per_1m: 3.00  # $3 per 1M tokens

  openai:
    enabled: false  # Set to true + add OPENAI_API_KEY to enable
    models:
      gpt-4o-mini:
        name: "gpt-4o-mini"
        cost_input_per_1m: 0.15  # $0.15 per 1M tokens
        cost_output_per_1m: 0.60  # $0.60 per 1M tokens
      gpt-4o:
        name: "gpt-4o"
        cost_input_per_1m: 2.50  # $2.5 per 1M tokens
        cost_output_per_1m: 10.00  # $10 per 1M tokens

  anthropic:
    enabled: false  # Set to true + add ANTHROPIC_API_KEY to enable
    models:
      claude-3-5-haiku:
        name: "claude-3-5-haiku-20241022"
        cost_input_per_1m: 0.80  # $0.8 per 1M tokens
        cost_output_per_1m: 4.00  # $4 per 1M tokens
      claude-3-5-sonnet:
        name: "claude-3-5-sonnet-20241022"
        cost_input_per_1m: 3.00  # $3 per 1M tokens
        cost_output_per_1m: 15.00  # $15 per 1M tokens

defaults:
  chat:
    model: "gemini-2.5-flash"
    temperature: 0.7
    max_tokens: 6000
    timeout: 60
    retries: 3

domains:
  personas:
    generation:
      model: "gemini-2.5-flash"
      temperature: 0.9  # Wysoka dla kreatywnych, zróżnicowanych person
      max_tokens: 6000
      timeout: 90
      top_p: 0.95       # Dodatkowa różnorodność
      top_k: 40

    orchestration:
      model: "gemini-2.5-pro"
      temperature: 0.3  # Niższa dla analytical tasks
      max_tokens: 8000  # Wystarczająco na pełny plan + briefy
      timeout: 120      # 2 minuty dla complex reasoning

    needs:
      model: "gemini-2.5-pro"
      temperature: 0.25  # Bardzo niska dla deterministycznego JTBD analysis
      max_tokens: 4000
      timeout: 120

    jtbd:
      model: "gemini-2.5-flash"
      temperature: 0.7
      max_tokens: 4000

    segment_brief:
      model: "gemini-2.5-pro"
      temperature: 0.7  # Wyższa dla kreatywnego storytelling
      max_tokens: 6000  # Długie briefe (400-800 słów)
      timeout: 120

  focus_groups:
    discussion:
      model: "gemini-2.5-flash"
      temperature: 0.8  # Bardziej kreatywne dla naturalnych odpowiedzi
      max_tokens: 2000
      timeout: 30

    summarization:
      model: "gemini-2.5-pro"
      temperature: 0.2  # Niższa dla analytical summaries
      max_tokens: 6000
      timeout: 90

  surveys:
    response:
      model: "gemini-2.5-flash"
      temperature: 0.7
      max_tokens: 1000
      timeout: 30

  study_designer:
    conversation_extraction:
      model: "gemini-2.5-pro"  # PRO dla critical multi-step extraction
      temperature: 0.3  # Niska dla consistent JSON structured output
      max_tokens: 3000
      timeout: 45
      retries: 3
      # NOTE: Używany w v2 architecture (conversation_extractor node)
      # Zastępuje wszystkie 5 starych nodes w 1 wywołaniu

    question_generation:
      model: "gemini-2.5-flash"
      temperature: 0.8  # Wyższa dla kreatywnych follow-up questions
      max_tokens: 1000
      timeout: 30
      retries: 3

    plan_generation:
      model: "gemini-2.5-flash"
      temperature: 0.3  # Niższa dla structured output (plan w markdown)
      max_tokens: 4000
      timeout: 60
      retries: 3

  rag:
    graph:
      model: "gemini-2.5-flash"
      temperature: 0.1  # Bardzo niska dla Cypher generation
      max_tokens: 2000
      timeout: 30

    embedding:
      model: "models/gemini-embedding-001"
      # NOTE: Must include "models/" prefix for Google Generative AI

    reranker:
      model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      # HuggingFace cross-encoder model
