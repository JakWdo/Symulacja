# RAG Retrieval Configuration
#
# Konfiguracja dla systemu RAG:
# - Chunking (jak dzielić dokumenty)
# - Retrieval (jak wyszukiwać chunks)
# - Hybrid Search (vector + keyword + RRF fusion)
# - Reranking (cross-encoder precision)
#
# Usage:
#   from config import rag
#   chunk_size = rag.chunking.chunk_size
#   top_k = rag.retrieval.top_k

chunking:
  # Rozmiar chunków tekstowych (znaki)
  # Mniejsze chunki dają lepszą precyzję embeddings - jeden embedding reprezentuje bardziej focused kontekst
  chunk_size: 1000

  # Overlap między chunkami (znaki)
  # 30% overlap zapobiega rozdzielaniu ważnych informacji między chunkami
  chunk_overlap: 300

retrieval:
  # Liczba top wyników z retrieval
  # Więcej results kompensuje mniejszy rozmiar chunków, zachowując podobną ilość kontekstu
  top_k: 8

  # Maksymalna długość kontekstu RAG (znaki)
  # OPTIMIZATION: Reduced from 12000 → 8000 to reduce LLM input tokens
  # Still sufficient for TOP_K=8 chunks (8 × 1000 = 8000) + moderate graph context
  max_context_chars: 8000

  # Hybrid search: czy używać keyword search + vector search
  use_hybrid_search: true

  # Hybrid search: waga vector search (0.0-1.0, reszta to keyword)
  vector_weight: 0.7

  # RRF k parameter (wygładzanie rangi w Reciprocal Rank Fusion)
  # Niższe k (40) favoryzuje top results, wyższe k (80) traktuje równomiernie
  # k=60 to sprawdzony balans - eksperymentuj używając test_rrf_k_tuning.py
  rrf_k: 60

  # Retrieval mode dla RetrievalService
  # - "vector": Tylko vector search (najszybsze, ~500ms)
  # - "hybrid": Vector + keyword + RRF (~1000ms)
  # - "hybrid+rerank": + cross-encoder reranking (~1500ms)
  mode: "vector"

  # Threshold dla auto-switching do hybrid search
  # Jeśli vector results < N, przełącz na hybrid (quality safeguard)
  rerank_threshold: 3

  # Reranking configuration
  reranking:
    # Włącz cross-encoder dla precyzyjniejszego scoringu query-document pairs
    enabled: true

    # Liczba candidatów dla reranking (przed finalnym top_k)
    # Cross-encoder jest wolniejszy, więc rerankujemy więcej niż potrzebujemy i bierzemy top
    # OPTIMIZATION: Zmniejszono 25→15→10 (60% mniej compute vs original)
    # Cloud Run CPU: 10 candidates × 100-150ms = 1.0-1.5s (vs 2.3s dla 15)
    candidates: 10

    # Cross-encoder model dla reranking
    # FIX: Poprzedni model "mmarco-mMiniLMv2-L6-v1" nie istniał na HuggingFace
    # CURRENT: ms-marco-MiniLM-L-6-v2 - English, 6 layers, SZYBKI (~100-150ms dla 15 docs)
    # ALTERNATIVE: "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1" (multilingual, wolniejszy)
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
