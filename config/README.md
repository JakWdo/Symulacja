# Config System - Centralized Configuration

Centralny system konfiguracji dla Market Research SaaS - wszystkie prompty, modele, parametry LLM i RAG w jednym miejscu.

## üéØ Cel

**Przed:** Prompty i parametry hardcode'owane w kodzie - trudne do edycji, brak wersjonowania.
**Po:** Wszystko w `config/` - ≈Çatwa edycja, versioning, walidacja, reprodukowalno≈õƒá.

## üìÅ Struktura Katalog√≥w

```
config/
‚îú‚îÄ‚îÄ README.md                   # Ten plik
‚îú‚îÄ‚îÄ PROMPTS_INDEX.md            # üìë Katalog wszystkich 25 prompt√≥w (QUICK REFERENCE!)
‚îú‚îÄ‚îÄ models.yaml                 # Model registry (fallback chain)
‚îú‚îÄ‚îÄ pricing.yaml                # Model pricing (USD/1M tokens)
‚îú‚îÄ‚îÄ features.yaml               # Feature flags
‚îú‚îÄ‚îÄ logging.yaml                # Logging configuration
‚îú‚îÄ‚îÄ providers.yaml              # LLM providers (klucze API)
‚îú‚îÄ‚îÄ app.yaml                    # Global app settings
‚îÇ
‚îú‚îÄ‚îÄ env/                        # Environment overrides
‚îÇ   ‚îú‚îÄ‚îÄ development.yaml        # Dev env (DEBUG logs, d≈Çu≈ºsze timeouts)
‚îÇ   ‚îú‚îÄ‚îÄ staging.yaml
‚îÇ   ‚îî‚îÄ‚îÄ production.yaml         # Prod env (INFO logs, Pro models)
‚îÇ
‚îú‚îÄ‚îÄ prompts/                    # Wszystkie prompty (23 pliki)
‚îÇ   ‚îú‚îÄ‚îÄ system/                 # System prompts (9)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ market_research_expert.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ polish_society_expert.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality_control.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ personas/               # Persona prompts (6)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jtbd.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orchestration.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ segment_brief.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ focus_groups/           # Focus group prompts (2)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ persona_response.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ discussion_summary.yaml
‚îÇ   ‚îú‚îÄ‚îÄ rag/                    # RAG prompts (2)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cypher_generation.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graph_rag_answer.yaml
‚îÇ   ‚îî‚îÄ‚îÄ surveys/                # Survey prompts (4)
‚îÇ       ‚îú‚îÄ‚îÄ single_choice.yaml
‚îÇ       ‚îú‚îÄ‚îÄ multiple_choice.yaml
‚îÇ       ‚îú‚îÄ‚îÄ rating_scale.yaml
‚îÇ       ‚îî‚îÄ‚îÄ open_text.yaml
‚îÇ
‚îú‚îÄ‚îÄ rag/                        # RAG configuration
‚îÇ   ‚îú‚îÄ‚îÄ graph_transformer.yaml  # LLMGraphTransformer config
‚îÇ   ‚îú‚îÄ‚îÄ retrieval.yaml          # Chunking, hybrid search, reranking
‚îÇ   ‚îî‚îÄ‚îÄ queries/                # Pre-defined Cypher queries
‚îÇ       ‚îî‚îÄ‚îÄ demographic_context.cypher
‚îÇ
‚îú‚îÄ‚îÄ demographics/               # Demographics data
‚îÇ   ‚îú‚îÄ‚îÄ poland.yaml             # POLISH_* constants
‚îÇ   ‚îî‚îÄ‚îÄ international.yaml      # DEFAULT_* constants
‚îÇ
‚îî‚îÄ‚îÄ schema/                     # JSON Schemas dla walidacji
    ‚îú‚îÄ‚îÄ prompt.schema.json
    ‚îî‚îÄ‚îÄ model.schema.json
```

## üöÄ Quick Start

> **üìë Szukasz konkretnego promptu?** Zobacz **[PROMPTS_INDEX.md](PROMPTS_INDEX.md)** - kompletny katalog wszystkich 25 prompt√≥w z parametrami i przyk≈Çadami!

### 1. U≈ºywanie Prompt√≥w

```python
from config import prompts

# Pobierz prompt
prompt_template = prompts.get("surveys.single_choice")

# Renderuj z variables (Jinja2, custom delimiters: ${var})
rendered_messages = prompt_template.render(
    persona_context="Kobieta, 28 lat, Marketing Manager...",
    question="Jak czƒôsto korzystasz z social media?",
    description="",
    options="- Codziennie\n- Kilka razy w tygodniu\n- Rzadko"
)

# U≈ºyj w LangChain
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    (msg["role"], msg["content"]) for msg in rendered_messages
])
```

### 2. U≈ºywanie Modeli

```python
from config import models
from app.services.shared.clients import build_chat_model

# Pobierz model config (fallback chain: domain.subdomain ‚Üí domain.default ‚Üí global.default)
model_config = models.get("personas", "orchestration")

# Build LLM
llm = build_chat_model(**model_config.params)
```

### 3. U≈ºywanie RAG Config

```python
from config import rag

# Chunking
chunk_size = rag.chunking.chunk_size  # 1000
chunk_overlap = rag.chunking.chunk_overlap  # 300

# Retrieval
top_k = rag.retrieval.top_k  # 8
use_hybrid = rag.retrieval.use_hybrid_search  # True

# Graph Transformer
allowed_nodes = rag.graph_transformer.allowed_nodes  # ["Obserwacja", "Wskaznik", ...]
additional_instructions = rag.graph_transformer.additional_instructions

# Cypher queries
demographic_query = rag.queries["demographic_context"]
```

## üìù Format Prompt√≥w (YAML)

Ka≈ºdy prompt to plik YAML w `config/prompts/`:

```yaml
id: surveys.single_choice
version: "1.0.0"
description: "Generate single-choice survey response as a specific persona"
messages:
  - role: system
    content: |
      You are answering a survey question as a specific persona...
  - role: user
    content: |
      Persona Profile:
      ${persona_context}

      Question: ${question}
      ${description}

      Options:
      ${options}
```

**Kluczowe cechy:**
- **Placeholders**: `${variable}` (custom Jinja2 delimiters, bezpieczne dla Cypher)
- **Versioning**: Semantic versioning (auto-bump przy zmianie hash)
- **Hashing**: SHA256 hash content'u dla version detection
- **Walidacja**: JSON Schema validation (`scripts/config_validate.py`)

## üîß Model Configuration (models.yaml)

Fallback chain: **domain.subdomain ‚Üí domain.default ‚Üí global.default**

```yaml
defaults:
  chat:
    model: "gemini-2.5-flash"
    temperature: 0.7
    max_tokens: 6000
    timeout: 60
    retries: 3

domains:
  personas:
    generation:
      model: "gemini-2.5-flash"
      temperature: 0.8

    orchestration:
      model: "gemini-2.5-pro"    # Complex reasoning
      temperature: 0.3            # Ni≈ºsza dla analytical tasks
      max_tokens: 8000
      timeout: 120

  surveys:
    response:
      model: "gemini-2.5-flash"

  rag:
    graph:
      model: "gemini-2.5-flash"
      temperature: 0.1            # Bardzo niska dla Cypher generation
```

**Przyk≈Çad u≈ºycia:**
```python
models.get("personas", "orchestration")  # ‚Üí gemini-2.5-pro, temp=0.3
models.get("personas", "jtbd")           # ‚Üí fallback do generation ‚Üí gemini-2.5-flash, temp=0.8
models.get("surveys", "response")        # ‚Üí gemini-2.5-flash (domain default)
models.get("unknown", "unknown")         # ‚Üí fallback do defaults.chat
```

## üåç Environment Overrides

Kolejno≈õƒá precedencji (highest ‚Üí lowest):
1. **Environment variables** (e.g., `GOOGLE_API_KEY`)
2. **env/{environment}.yaml** (e.g., `env/production.yaml`)
3. **Base config** (e.g., `models.yaml`)
4. **Code defaults**

**Development** (`env/development.yaml`):
```yaml
models:
  defaults:
    chat:
      timeout: 120  # D≈Çu≈ºszy dla debugging

logging:
  level: "DEBUG"
  structured: false  # Human-readable
```

**Production** (`env/production.yaml`):
```yaml
models:
  defaults:
    chat:
      model: "gemini-2.5-pro"  # Lepszy model w prod

logging:
  level: "INFO"
  structured: true  # JSON dla Cloud Logging
```

## ‚úÖ Walidacja

```bash
# Validate wszystkie configs
python scripts/config_validate.py

# Check placeholders w promptach
python scripts/config_validate.py --check-placeholders

# Auto-bump versions gdy hash siƒô zmieni≈Ç
python scripts/config_validate.py --auto-bump
```

**Validation rules:**
- Prompty: required fields (id, version, description, messages), ID format, version semver
- Models: defaults.chat.model exists, valid temperature/timeout ranges
- Placeholders: strict mode - b≈ÇƒÖd je≈õli brakuje variable

## üí∞ Pricing Configuration (pricing.yaml)

Model pricing dla tracking koszt√≥w LLM operations:

```yaml
models:
  gemini-2.5-flash:
    input_price_per_million: 0.075   # $0.075 per 1M input tokens
    output_price_per_million: 0.30   # $0.30 per 1M output tokens

  gemini-2.5-pro:
    input_price_per_million: 1.25    # $1.25 per 1M input tokens
    output_price_per_million: 5.00   # $5.00 per 1M output tokens

budget_alerts:
  warning_threshold: 0.8   # Alert at 80% budget usage
  critical_threshold: 0.9  # Critical at 90%
```

**Usage:**
```python
from config.loader import ConfigLoader

pricing = ConfigLoader().load_yaml("pricing.yaml")
input_cost = pricing["models"]["gemini-2.5-flash"]["input_price_per_million"]

# Auto-loaded w UsageTrackingService
from app.services.dashboard.usage_tracking_service import MODEL_PRICING
```

## ‚öôÔ∏è Feature Flags (features.yaml)

Feature flags, performance targets, RAG settings:

```yaml
# Performance targets
performance:
  max_response_time_per_persona: 3
  consistency_error_threshold: 0.05
  random_seed: 42

# RAG system
rag:
  enabled: true
  chunk_size: 1000
  chunk_overlap: 300
  top_k: 8
  use_hybrid_search: true
  vector_weight: 0.7
  use_reranking: true

# Cache
cache:
  segment_brief_enabled: true
  segment_brief_ttl: 604800  # 7 days
  redis_max_connections: 50
```

**Usage:**
```python
from config.loader import ConfigLoader

features = ConfigLoader().load_yaml("features.yaml")
rag_enabled = features["rag"]["enabled"]
```

## üß™ A/B Testing Prompt√≥w

Tw√≥rz warianty prompt√≥w dla test√≥w:

**1. Utw√≥rz plik variant:**
```
config/prompts/system/market_research_expert_variant_b.yaml
```

**2. Format pliku variant:**
```yaml
id: system.market_research_expert
version: "1.1.0"
variant: "b"        # Nazwa variantu
weight: 1.0         # Waga dla weighted selection
description: "More concise expert prompt"
messages:
  - role: system
    content: |
      You are a market research expert. Be concise.
```

**3. Usage:**
```python
from config import prompts

# Random weighted selection (50/50 dla weight=1.0 ka≈ºdy)
prompt = prompts.get("system.market_research_expert")

# Lub konkretny variant
prompt = prompts.get("system.market_research_expert", variant="b")
```

**Naming convention:**
- Base prompt: `{name}.yaml`
- Variant: `{name}_variant_{x}.yaml`

## üé® Jak Dodaƒá Nowy Prompt

1. **Stw√≥rz plik YAML** w `config/prompts/{domain}/{name}.yaml`:

```yaml
id: focus_groups.moderator
version: "1.0.0"
description: "Moderator prompt for focus group discussions"
messages:
  - role: system
    content: |
      You are an experienced focus group moderator...
  - role: user
    content: |
      Discussion topic: ${topic}
      Participants: ${participant_count}
```

2. **U≈ºywaj w kodzie:**

```python
from config import prompts

prompt_template = prompts.get("focus_groups.moderator")
rendered = prompt_template.render(
    topic="AI in healthcare",
    participant_count="8"
)
```

3. **Waliduj:**

```bash
python scripts/config_validate.py
```

## üîÑ Jak Dodaƒá Nowy Model

1. **Edit `config/models.yaml`:**

```yaml
domains:
  new_domain:
    subdomain:
      model: "gemini-2.5-flash"
      temperature: 0.7
      max_tokens: 4000
```

2. **U≈ºyj w kodzie:**

```python
from config import models

model_config = models.get("new_domain", "subdomain")
llm = build_chat_model(**model_config.params)
```

## üìä Pricing Configuration

`config/pricing.yaml` - u≈ºywane w usage tracking:

```yaml
models:
  gemini-2.5-flash:
    input_price_per_million: 0.075   # $0.075 per 1M tokens
    output_price_per_million: 0.30   # $0.30 per 1M tokens

  gemini-2.5-pro:
    input_price_per_million: 1.25    # $1.25 per 1M tokens
    output_price_per_million: 5.00   # $5.00 per 1M tokens
```

## üö© Feature Flags

`config/features.yaml`:

```yaml
rag:
  enabled: true
  node_properties_enabled: true

segment_cache:
  enabled: true
  ttl_days: 7

performance:
  max_focus_group_time: 30
  random_seed: 42
```

## üîÑ Migration Guide

### Quick replacements (old ‚Üí new)

```python
# Config (old)
from app.core.config import get_settings
settings = get_settings()
model = settings.PERSONA_GENERATION_MODEL
rag_enabled = settings.RAG_ENABLED

# Config (new)
from config import models, features
model = models.get("personas", "generation").model
rag_enabled = features.rag.enabled

# Prompts (old)
from app.core.prompts.persona_prompts import JTBD_ANALYSIS_PROMPT
prompt_text = JTBD_ANALYSIS_PROMPT.format(age=25, occupation="Engineer")

# Prompts (new)
from config import prompts
messages = prompts.get("personas.jtbd").render(age=25, occupation="Engineer")

# Demographics (old)
from app.core.constants import DEFAULT_AGE_GROUPS
from app.core.demographics import POLISH_LOCATIONS

# Demographics (new)
from config import demographics
age_groups = demographics.common.age_groups
locations = demographics.poland.locations
```

### Migration Timeline

- **v1.0**: Both old and new approaches work ‚úÖ (current)
- **v1.1**: Deprecation warnings added for old imports ‚ö†Ô∏è
- **v2.0**: Old modules removed (planned)

### Benefits of Migration

1. ‚úÖ **Single Source of Truth** - All config in YAML
2. ‚úÖ **Hot Reloadable** - Update YAML without code changes
3. ‚úÖ **Type Safe** - Full type annotations and validation
4. ‚úÖ **Developer Friendly** - Clear structure, easy to find settings
5. ‚úÖ **Version Controlled** - Track config changes in git

## üìö Przyk≈Çad Migracji

**Przed** (hardcode):
```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are answering a survey..."),
    ("user", f"Question: {question}\nOptions: {options}")
])
```

**Po** (config):
```python
from config import prompts

prompt_template = prompts.get("surveys.single_choice")
rendered_messages = prompt_template.render(
    persona_context=context,
    question=question,
    options="\n".join(f"- {opt}" for opt in options)
)

prompt = ChatPromptTemplate.from_messages([
    (msg["role"], msg["content"]) for msg in rendered_messages
])
```

## üß™ Testing

**Test prompt√≥w:**
```python
from config import prompts

# Load prompt
prompt = prompts.get("surveys.single_choice")

# Check hash
assert prompt.hash == "abc123..."

# Test rendering
rendered = prompt.render(
    persona_context="Test persona",
    question="Test question",
    options="- Option 1\n- Option 2"
)

assert len(rendered) == 2  # system + user messages
```

**Test modeli:**
```python
from config import models

model_config = models.get("surveys", "response")

assert model_config.model == "gemini-2.5-flash"
assert model_config.temperature == 0.7
```

## üîë Best Practices

1. **Nie hardcode'uj prompt√≥w** - zawsze u≈ºywaj PromptRegistry
2. **Nie hardcode'uj modeli** - u≈ºywaj ModelRegistry
3. **Waliduj przed commit** - `python scripts/config_validate.py`
4. **Bump version manualnie** lub u≈ºywaj `--auto-bump`
5. **Testuj po zmianach** - upewnij siƒô ≈ºe placeholders sƒÖ poprawne
6. **Environment overrides** - r√≥≈ºne modele dla dev/staging/prod
7. **Secrets w env** - NIGDY nie commituj API keys do repo

## üìñ Architecture

```
Code
  ‚Üì
config.prompts.get("domain.name")
  ‚Üì
PromptRegistry (cache + version + hash)
  ‚Üì
Load YAML from config/prompts/{domain}/{name}.yaml
  ‚Üì
Render with Jinja2 (${var} delimiters)
  ‚Üì
Return rendered messages
  ‚Üì
Use in LangChain
```

**Fallback chain dla modeli:**
```
models.get("personas", "orchestration")
  ‚Üì
1. Try: domains.personas.orchestration  ‚úÖ Found!
2. If not: domains.personas.generation
3. If not: defaults.chat
```

## üõ†Ô∏è Troubleshooting

**Problem: `Prompt not found`**
```
Solution: Check ID format - must be "domain.name"
Example: "surveys.single_choice" (NOT "single_choice")
```

**Problem: `Missing required variable`**
```
Solution: Check placeholders - all ${var} must be provided
Use --check-placeholders to see required variables
```

**Problem: `Model not found`**
```
Solution: Models.get() will fallback to defaults.chat + WARNING
Check models.yaml for correct domain/subdomain structure
```

## üìû Support

- **Validation errors:** Run `python scripts/config_validate.py --check-placeholders`
- **Prompt issues:** Check YAML syntax in config/prompts/
- **Model issues:** Verify fallback chain in config/models.yaml
- **Environment:** Check `ENVIRONMENT` env var and env/*.yaml overrides
