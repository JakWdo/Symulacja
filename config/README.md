# Config System - Centralized Configuration

Centralny system konfiguracji dla Market Research SaaS - wszystkie prompty, modele, parametry LLM i RAG w jednym miejscu.

## ğŸ¯ Cel

**Przed:** Prompty i parametry hardcode'owane w kodzie - trudne do edycji, brak wersjonowania.
**Po:** Wszystko w `config/` - Å‚atwa edycja, versioning, walidacja, reprodukowalnoÅ›Ä‡.

## ğŸ“ Struktura KatalogÃ³w

```
config/
â”œâ”€â”€ README.md                   # Ten plik
â”œâ”€â”€ models.yaml                 # Model registry (fallback chain)
â”œâ”€â”€ pricing.yaml                # Model pricing (USD/1M tokens)
â”œâ”€â”€ features.yaml               # Feature flags
â”œâ”€â”€ logging.yaml                # Logging configuration
â”œâ”€â”€ providers.yaml              # LLM providers (klucze API)
â”œâ”€â”€ app.yaml                    # Global app settings
â”‚
â”œâ”€â”€ env/                        # Environment overrides
â”‚   â”œâ”€â”€ development.yaml        # Dev env (DEBUG logs, dÅ‚uÅ¼sze timeouts)
â”‚   â”œâ”€â”€ staging.yaml
â”‚   â””â”€â”€ production.yaml         # Prod env (INFO logs, Pro models)
â”‚
â”œâ”€â”€ prompts/                    # Wszystkie prompty (23 pliki)
â”‚   â”œâ”€â”€ system/                 # System prompts (9)
â”‚   â”‚   â”œâ”€â”€ market_research_expert.yaml
â”‚   â”‚   â”œâ”€â”€ polish_society_expert.yaml
â”‚   â”‚   â”œâ”€â”€ quality_control.yaml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ personas/               # Persona prompts (6)
â”‚   â”‚   â”œâ”€â”€ jtbd.yaml
â”‚   â”‚   â”œâ”€â”€ orchestration.yaml
â”‚   â”‚   â”œâ”€â”€ segment_brief.yaml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ focus_groups/           # Focus group prompts (2)
â”‚   â”‚   â”œâ”€â”€ persona_response.yaml
â”‚   â”‚   â””â”€â”€ discussion_summary.yaml
â”‚   â”œâ”€â”€ rag/                    # RAG prompts (2)
â”‚   â”‚   â”œâ”€â”€ cypher_generation.yaml
â”‚   â”‚   â””â”€â”€ graph_rag_answer.yaml
â”‚   â””â”€â”€ surveys/                # Survey prompts (4)
â”‚       â”œâ”€â”€ single_choice.yaml
â”‚       â”œâ”€â”€ multiple_choice.yaml
â”‚       â”œâ”€â”€ rating_scale.yaml
â”‚       â””â”€â”€ open_text.yaml
â”‚
â”œâ”€â”€ rag/                        # RAG configuration
â”‚   â”œâ”€â”€ graph_transformer.yaml  # LLMGraphTransformer config
â”‚   â”œâ”€â”€ retrieval.yaml          # Chunking, hybrid search, reranking
â”‚   â””â”€â”€ queries/                # Pre-defined Cypher queries
â”‚       â””â”€â”€ demographic_context.cypher
â”‚
â”œâ”€â”€ demographics/               # Demographics data
â”‚   â”œâ”€â”€ poland.yaml             # POLISH_* constants
â”‚   â””â”€â”€ international.yaml      # DEFAULT_* constants
â”‚
â””â”€â”€ schema/                     # JSON Schemas dla walidacji
    â”œâ”€â”€ prompt.schema.json
    â””â”€â”€ model.schema.json
```

## ğŸš€ Quick Start

### 1. UÅ¼ywanie PromptÃ³w

```python
from config import prompts

# Pobierz prompt
prompt_template = prompts.get("surveys.single_choice")

# Renderuj z variables (Jinja2, custom delimiters: ${var})
rendered_messages = prompt_template.render(
    persona_context="Kobieta, 28 lat, Marketing Manager...",
    question="Jak czÄ™sto korzystasz z social media?",
    description="",
    options="- Codziennie\n- Kilka razy w tygodniu\n- Rzadko"
)

# UÅ¼yj w LangChain
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    (msg["role"], msg["content"]) for msg in rendered_messages
])
```

### 2. UÅ¼ywanie Modeli

```python
from config import models
from app.services.shared.clients import build_chat_model

# Pobierz model config (fallback chain: domain.subdomain â†’ domain.default â†’ global.default)
model_config = models.get("personas", "orchestration")

# Build LLM
llm = build_chat_model(**model_config.params)
```

### 3. UÅ¼ywanie RAG Config

```python
from config import rag

# Chunking
chunk_size = rag.chunking.chunk_size  # 1000
chunk_overlap = rag.chunking.chunk_overlap  # 300

# Retrieval
top_k = rag.retrieval.top_k  # 8
use_hybrid = rag.retrieval.use_hybrid_search  # True

# Graph Transformer
allowed_nodes = rag.graph_transformer.allowed_nodes  # ["Obserwacja", "Wskaznik", ...]
additional_instructions = rag.graph_transformer.additional_instructions

# Cypher queries
demographic_query = rag.queries["demographic_context"]
```

## ğŸ“ Format PromptÃ³w (YAML)

KaÅ¼dy prompt to plik YAML w `config/prompts/`:

```yaml
id: surveys.single_choice
version: "1.0.0"
description: "Generate single-choice survey response as a specific persona"
messages:
  - role: system
    content: |
      You are answering a survey question as a specific persona...
  - role: user
    content: |
      Persona Profile:
      ${persona_context}

      Question: ${question}
      ${description}

      Options:
      ${options}
```

**Kluczowe cechy:**
- **Placeholders**: `${variable}` (custom Jinja2 delimiters, bezpieczne dla Cypher)
- **Versioning**: Semantic versioning (auto-bump przy zmianie hash)
- **Hashing**: SHA256 hash content'u dla version detection
- **Walidacja**: JSON Schema validation (`scripts/config_validate.py`)

## ğŸ”§ Model Configuration (models.yaml)

Fallback chain: **domain.subdomain â†’ domain.default â†’ global.default**

```yaml
defaults:
  chat:
    model: "gemini-2.5-flash"
    temperature: 0.7
    max_tokens: 6000
    timeout: 60
    retries: 3

domains:
  personas:
    generation:
      model: "gemini-2.5-flash"
      temperature: 0.8

    orchestration:
      model: "gemini-2.5-pro"    # Complex reasoning
      temperature: 0.3            # NiÅ¼sza dla analytical tasks
      max_tokens: 8000
      timeout: 120

  surveys:
    response:
      model: "gemini-2.5-flash"

  rag:
    graph:
      model: "gemini-2.5-flash"
      temperature: 0.1            # Bardzo niska dla Cypher generation
```

**PrzykÅ‚ad uÅ¼ycia:**
```python
models.get("personas", "orchestration")  # â†’ gemini-2.5-pro, temp=0.3
models.get("personas", "jtbd")           # â†’ fallback do generation â†’ gemini-2.5-flash, temp=0.8
models.get("surveys", "response")        # â†’ gemini-2.5-flash (domain default)
models.get("unknown", "unknown")         # â†’ fallback do defaults.chat
```

## ğŸŒ Environment Overrides

KolejnoÅ›Ä‡ precedencji (highest â†’ lowest):
1. **Environment variables** (e.g., `GOOGLE_API_KEY`)
2. **env/{environment}.yaml** (e.g., `env/production.yaml`)
3. **Base config** (e.g., `models.yaml`)
4. **Code defaults**

**Development** (`env/development.yaml`):
```yaml
models:
  defaults:
    chat:
      timeout: 120  # DÅ‚uÅ¼szy dla debugging

logging:
  level: "DEBUG"
  structured: false  # Human-readable
```

**Production** (`env/production.yaml`):
```yaml
models:
  defaults:
    chat:
      model: "gemini-2.5-pro"  # Lepszy model w prod

logging:
  level: "INFO"
  structured: true  # JSON dla Cloud Logging
```

## âœ… Walidacja

```bash
# Validate wszystkie configs
python scripts/config_validate.py

# Check placeholders w promptach
python scripts/config_validate.py --check-placeholders

# Auto-bump versions gdy hash siÄ™ zmieniÅ‚
python scripts/config_validate.py --auto-bump
```

**Validation rules:**
- Prompty: required fields (id, version, description, messages), ID format, version semver
- Models: defaults.chat.model exists, valid temperature/timeout ranges
- Placeholders: strict mode - bÅ‚Ä…d jeÅ›li brakuje variable

## ğŸ’° Pricing Configuration (pricing.yaml)

Model pricing dla tracking kosztÃ³w LLM operations:

```yaml
models:
  gemini-2.5-flash:
    input_price_per_million: 0.075   # $0.075 per 1M input tokens
    output_price_per_million: 0.30   # $0.30 per 1M output tokens

  gemini-2.5-pro:
    input_price_per_million: 1.25    # $1.25 per 1M input tokens
    output_price_per_million: 5.00   # $5.00 per 1M output tokens

budget_alerts:
  warning_threshold: 0.8   # Alert at 80% budget usage
  critical_threshold: 0.9  # Critical at 90%
```

**Usage:**
```python
from config.loader import ConfigLoader

pricing = ConfigLoader().load_yaml("pricing.yaml")
input_cost = pricing["models"]["gemini-2.5-flash"]["input_price_per_million"]

# Auto-loaded w UsageTrackingService
from app.services.dashboard.usage_tracking_service import MODEL_PRICING
```

## âš™ï¸ Feature Flags (features.yaml)

Feature flags, performance targets, RAG settings:

```yaml
# Performance targets
performance:
  max_response_time_per_persona: 3
  consistency_error_threshold: 0.05
  random_seed: 42

# RAG system
rag:
  enabled: true
  chunk_size: 1000
  chunk_overlap: 300
  top_k: 8
  use_hybrid_search: true
  vector_weight: 0.7
  use_reranking: true

# Cache
cache:
  segment_brief_enabled: true
  segment_brief_ttl: 604800  # 7 days
  redis_max_connections: 50
```

**Usage:**
```python
from config.loader import ConfigLoader

features = ConfigLoader().load_yaml("features.yaml")
rag_enabled = features["rag"]["enabled"]
```

## ğŸ§ª A/B Testing PromptÃ³w

TwÃ³rz warianty promptÃ³w dla testÃ³w:

**1. UtwÃ³rz plik variant:**
```
config/prompts/system/market_research_expert_variant_b.yaml
```

**2. Format pliku variant:**
```yaml
id: system.market_research_expert
version: "1.1.0"
variant: "b"        # Nazwa variantu
weight: 1.0         # Waga dla weighted selection
description: "More concise expert prompt"
messages:
  - role: system
    content: |
      You are a market research expert. Be concise.
```

**3. Usage:**
```python
from config import prompts

# Random weighted selection (50/50 dla weight=1.0 kaÅ¼dy)
prompt = prompts.get("system.market_research_expert")

# Lub konkretny variant
prompt = prompts.get("system.market_research_expert", variant="b")
```

**Naming convention:**
- Base prompt: `{name}.yaml`
- Variant: `{name}_variant_{x}.yaml`

## ğŸ¨ Jak DodaÄ‡ Nowy Prompt

1. **StwÃ³rz plik YAML** w `config/prompts/{domain}/{name}.yaml`:

```yaml
id: focus_groups.moderator
version: "1.0.0"
description: "Moderator prompt for focus group discussions"
messages:
  - role: system
    content: |
      You are an experienced focus group moderator...
  - role: user
    content: |
      Discussion topic: ${topic}
      Participants: ${participant_count}
```

2. **UÅ¼ywaj w kodzie:**

```python
from config import prompts

prompt_template = prompts.get("focus_groups.moderator")
rendered = prompt_template.render(
    topic="AI in healthcare",
    participant_count="8"
)
```

3. **Waliduj:**

```bash
python scripts/config_validate.py
```

## ğŸ”„ Jak DodaÄ‡ Nowy Model

1. **Edit `config/models.yaml`:**

```yaml
domains:
  new_domain:
    subdomain:
      model: "gemini-2.5-flash"
      temperature: 0.7
      max_tokens: 4000
```

2. **UÅ¼yj w kodzie:**

```python
from config import models

model_config = models.get("new_domain", "subdomain")
llm = build_chat_model(**model_config.params)
```

## ğŸ“Š Pricing Configuration

`config/pricing.yaml` - uÅ¼ywane w usage tracking:

```yaml
models:
  gemini-2.5-flash:
    input_price_per_million: 0.075   # $0.075 per 1M tokens
    output_price_per_million: 0.30   # $0.30 per 1M tokens

  gemini-2.5-pro:
    input_price_per_million: 1.25    # $1.25 per 1M tokens
    output_price_per_million: 5.00   # $5.00 per 1M tokens
```

## ğŸš© Feature Flags

`config/features.yaml`:

```yaml
rag:
  enabled: true
  node_properties_enabled: true

segment_cache:
  enabled: true
  ttl_days: 7

performance:
  max_focus_group_time: 30
  random_seed: 42
```

## ğŸ“š PrzykÅ‚ad Migracji

**Przed** (hardcode):
```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are answering a survey..."),
    ("user", f"Question: {question}\nOptions: {options}")
])
```

**Po** (config):
```python
from config import prompts

prompt_template = prompts.get("surveys.single_choice")
rendered_messages = prompt_template.render(
    persona_context=context,
    question=question,
    options="\n".join(f"- {opt}" for opt in options)
)

prompt = ChatPromptTemplate.from_messages([
    (msg["role"], msg["content"]) for msg in rendered_messages
])
```

## ğŸ§ª Testing

**Test promptÃ³w:**
```python
from config import prompts

# Load prompt
prompt = prompts.get("surveys.single_choice")

# Check hash
assert prompt.hash == "abc123..."

# Test rendering
rendered = prompt.render(
    persona_context="Test persona",
    question="Test question",
    options="- Option 1\n- Option 2"
)

assert len(rendered) == 2  # system + user messages
```

**Test modeli:**
```python
from config import models

model_config = models.get("surveys", "response")

assert model_config.model == "gemini-2.5-flash"
assert model_config.temperature == 0.7
```

## ğŸ”‘ Best Practices

1. **Nie hardcode'uj promptÃ³w** - zawsze uÅ¼ywaj PromptRegistry
2. **Nie hardcode'uj modeli** - uÅ¼ywaj ModelRegistry
3. **Waliduj przed commit** - `python scripts/config_validate.py`
4. **Bump version manualnie** lub uÅ¼ywaj `--auto-bump`
5. **Testuj po zmianach** - upewnij siÄ™ Å¼e placeholders sÄ… poprawne
6. **Environment overrides** - rÃ³Å¼ne modele dla dev/staging/prod
7. **Secrets w env** - NIGDY nie commituj API keys do repo

## ğŸ“– Architecture

```
Code
  â†“
config.prompts.get("domain.name")
  â†“
PromptRegistry (cache + version + hash)
  â†“
Load YAML from config/prompts/{domain}/{name}.yaml
  â†“
Render with Jinja2 (${var} delimiters)
  â†“
Return rendered messages
  â†“
Use in LangChain
```

**Fallback chain dla modeli:**
```
models.get("personas", "orchestration")
  â†“
1. Try: domains.personas.orchestration  âœ… Found!
2. If not: domains.personas.generation
3. If not: defaults.chat
```

## ğŸ› ï¸ Troubleshooting

**Problem: `Prompt not found`**
```
Solution: Check ID format - must be "domain.name"
Example: "surveys.single_choice" (NOT "single_choice")
```

**Problem: `Missing required variable`**
```
Solution: Check placeholders - all ${var} must be provided
Use --check-placeholders to see required variables
```

**Problem: `Model not found`**
```
Solution: Models.get() will fallback to defaults.chat + WARNING
Check models.yaml for correct domain/subdomain structure
```

## ğŸ“ Support

- **Validation errors:** Run `python scripts/config_validate.py --check-placeholders`
- **Prompt issues:** Check YAML syntax in config/prompts/
- **Model issues:** Verify fallback chain in config/models.yaml
- **Environment:** Check `ENVIRONMENT` env var and env/*.yaml overrides
