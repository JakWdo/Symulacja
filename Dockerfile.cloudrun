# ==============================================================================
# Dockerfile.cloudrun - Single Service (Backend + Frontend Static)
# ==============================================================================
# Multi-stage build:
# 1. Build React frontend
# 2. Install Python dependencies
# 3. Runtime with backend code + frontend static files
# ==============================================================================

# ==============================================================================
# STAGE 1: Build Frontend (React + Vite)
# ==============================================================================
# Pin base image version dla stable cache (floating tag invalidowa≈Ç cache)
# node:20-alpine mo≈ºe byƒá 20.10, 20.11, 20.12... ka≈ºda zmiana invaliduje WSZYSTKIE layers
FROM node:20.18.0-alpine3.20 AS frontend-builder

WORKDIR /app/frontend

# Copy package files for caching
COPY frontend/package*.json ./

# Install dependencies
RUN npm ci

# Copy frontend source
COPY frontend/ ./

# Build production bundle
# Output: /app/frontend/dist/
RUN npm run build

# ==============================================================================
# STAGE 2: Build Backend (Python dependencies)
# ==============================================================================
# Pin Python version (floating tag 3.11-slim invalidowa≈Ç cache przy minor updates)
FROM python:3.11.11-slim-bookworm AS backend-builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    g++ \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install gunicorn (not in requirements.txt for local dev)
RUN pip install --no-cache-dir gunicorn

# Set HuggingFace cache directory for pre-warming
# CRITICAL: Must be set before model download to ensure correct cache location
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface
ENV HF_HOME=/root/.cache/huggingface

# Pre-warm HuggingFace models (RAG reranking) - eliminates cold start overhead
# Downloads cross-encoder model to /root/.cache/huggingface (~90MB)
# Saves 3-5s on first request in production
RUN python -c "from sentence_transformers import CrossEncoder; \
    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512); \
    print(f'‚úÖ CrossEncoder model loaded successfully')" && \
    echo "üì¶ Inspecting HuggingFace cache contents:" && \
    ls -lah /root/.cache/huggingface/ && \
    du -sh /root/.cache/huggingface/ && \
    echo "‚úÖ HuggingFace CrossEncoder model pre-warmed successfully"

# Verify model files are present in cache BEFORE copying to runtime stage
# Fails fast if pre-warming didn't work (better to catch here than later)
RUN test -d /root/.cache/huggingface/models--cross-encoder--ms-marco-MiniLM-L-6-v2 && \
    test -f /root/.cache/huggingface/models--cross-encoder--ms-marco-MiniLM-L-6-v2/snapshots/*/config.json && \
    echo "‚úÖ Backend-builder verification: CrossEncoder model files confirmed" || \
    (echo "‚ùå FAILED in backend-builder: Model files missing after pre-warming!" && \
     echo "   Cache directory may exist but model download failed." && \
     echo "   Check network connectivity and HuggingFace API status." && exit 1)

# ==============================================================================
# STAGE 3: Runtime (Backend + Frontend Static)
# ==============================================================================
# Pin runtime version (musi byƒá identyczna z backend-builder dla cache consistency)
FROM python:3.11.11-slim-bookworm AS runtime

# Production environment
ARG TARGET=production
ENV TARGET=${TARGET}
ENV PYTHONPATH=/app
ENV PORT=8080

# HuggingFace cache directory (explicit path for pre-warmed models)
# CRITICAL: Must point to appuser's home for cache to work correctly
ENV TRANSFORMERS_CACHE=/home/appuser/.cache/huggingface
ENV HF_HOME=/home/appuser/.cache/huggingface

WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    wget \
    && rm -rf /var/lib/apt/lists/*

# CRITICAL: Create non-root user FIRST (before copying cache)
# This ensures /home/appuser exists when we COPY the HuggingFace cache
RUN useradd -m -u 1000 appuser

# Copy Python packages from builder
COPY --from=backend-builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=backend-builder /usr/local/bin /usr/local/bin

# Copy pre-warmed HuggingFace cache (CrossEncoder model for RAG reranking)
# CRITICAL: --chown=appuser:appuser ensures correct ownership
# Eliminates 2-3s cold start when model is first accessed
COPY --chown=appuser:appuser --from=backend-builder /root/.cache/huggingface /home/appuser/.cache/huggingface

# Verify cache is accessible and contains CrossEncoder model
# This fails the build if pre-warming didn't work (better to catch here than in production)
RUN test -d /home/appuser/.cache/huggingface && \
    test -f /home/appuser/.cache/huggingface/models--cross-encoder--ms-marco-MiniLM-L-6-v2/snapshots/*/config.json && \
    echo "‚úÖ HuggingFace cache verified: CrossEncoder model present" || \
    (echo "‚ùå Cache verification FAILED - model will be downloaded at runtime!" && exit 1)

# Copy config folder EXPLICITLY (required for config module imports)
# Services depend on config.loader for prompts and model configuration
COPY --chown=appuser:appuser config/ ./config/

# Copy backend code
COPY --chown=appuser:appuser app/ ./app/
COPY --chown=appuser:appuser scripts/ ./scripts/
COPY --chown=appuser:appuser alembic/ ./alembic/
COPY --chown=appuser:appuser alembic.ini ./

# Copy frontend dist ‚Üí static/
COPY --chown=appuser:appuser --from=frontend-builder /app/frontend/dist ./static/

# Final permissions check (ensure everything is owned by appuser)
RUN chown -R appuser:appuser /app

# Expose Cloud Run port (8080)
EXPOSE 8080

# Healthcheck dla Docker i monitoring
# Sprawdza /health endpoint co 30s
# Timeout 10s, max 3 retries, start period 40s (dla inicjalizacji RAG)
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# Labels dla metadanych (Cloud Build, Git SHA, etc.)
LABEL org.opencontainers.image.title="Sight - Market Research SaaS"
LABEL org.opencontainers.image.description="AI-powered market research platform with synthetic personas"
LABEL org.opencontainers.image.vendor="Sight"

# Switch to non-root user (MUST be before CMD for security)
USER appuser

# Start gunicorn with optimized configuration for Cloud Run
# --bind :$PORT              ‚Üí Listen on Cloud Run port (8080)
# --workers 2                ‚Üí 2 workers for better CPU utilization (Cloud Run has 1+ vCPU)
# --worker-class uvicorn...  ‚Üí ASGI worker for async FastAPI
# --timeout 300              ‚Üí 5min timeout (LLM requests + RAG can be slow)
# --graceful-timeout 30      ‚Üí Graceful shutdown for ongoing requests
# --keep-alive 5             ‚Üí Keep-alive connections
# --log-level info           ‚Üí Production logging
# --access-logfile -         ‚Üí Access logs to stdout (Cloud Logging)
# --error-logfile -          ‚Üí Error logs to stderr (Cloud Logging)
CMD exec gunicorn \
    --bind :$PORT \
    --workers 2 \
    --worker-class uvicorn.workers.UvicornWorker \
    --timeout 300 \
    --graceful-timeout 30 \
    --keep-alive 5 \
    --log-level info \
    --access-logfile - \
    --error-logfile - \
    app.main:app
