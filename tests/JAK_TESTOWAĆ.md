# üß™ JAK TESTOWAƒÜ - Market Research SaaS

> **Kompletny przewodnik testowania aplikacji**

---

## üìã Spis tre≈õci

1. [Szybki start - uruchom wszystko](#-szybki-start)
2. [Struktura test√≥w](#-struktura-test√≥w)
3. [Wyniki i metryki](#-obecne-wyniki)
4. [Uruchamianie test√≥w](#-uruchamianie-test√≥w)
5. [Testy wymagajƒÖce ≈õrodowiska](#-testy-wymagajƒÖce-≈õrodowiska)
6. [Fixtures i konfiguracja](#-fixtures-conftest)
7. [Dodawanie nowych test√≥w](#-dodawanie-nowych-test√≥w)
8. [CI/CD Integration](#-cicd-github-actions)
9. [Debugging i troubleshooting](#-debugging)
10. [FAQ](#-faq)

---

## üöÄ Szybki start

### Instalacja zale≈ºno≈õci
```bash
pip install pytest pytest-asyncio pytest-cov email-validator
```

### Uruchom WSZYSTKIE dzia≈ÇajƒÖce testy jednƒÖ komendƒÖ
```bash
# Opcja 1: Tylko testy jednostkowe (bez ≈õrodowiska) - 95 test√≥w
python -m pytest tests/ -v

# Opcja 2: Z coverage
python -m pytest tests/ --cov=app --cov-report=html

# Opcja 3: Tylko szybkie testy (<2s)
python -m pytest tests/ -v -m "not slow"

# Opcja 4: Parallel execution (szybciej)
pip install pytest-xdist
python -m pytest tests/ -n auto
```

### Uruchom testy wed≈Çug kategorii
```bash
# Tylko testy krytyczne (najwa≈ºniejsze)
python -m pytest tests/test_critical_paths.py -v

# Tylko security & auth
python -m pytest tests/test_core_config_security.py tests/test_auth_api.py -v

# Tylko serwisy core
python -m pytest tests/test_persona_generator.py tests/test_memory_service_langchain.py -v

# Tylko API
python -m pytest tests/test_main_api.py tests/test_api_integration.py -v
```

---

## üìÇ Struktura test√≥w

```
tests/
‚îú‚îÄ‚îÄ JAK_TESTOWAƒÜ.md              ‚Üê TEN PLIK
‚îú‚îÄ‚îÄ conftest.py                   ‚Üê Fixtures i konfiguracja pytest
‚îú‚îÄ‚îÄ __init__.py                   ‚Üê Definicje kategorii test√≥w
‚îÇ
‚îú‚îÄ‚îÄ test_critical_paths.py        ‚úÖ 21 test√≥w - KRYTYCZNE ≈õcie≈ºki biznesowe
‚îú‚îÄ‚îÄ test_core_config_security.py  ‚úÖ 6 test√≥w - Security & config
‚îú‚îÄ‚îÄ test_persona_generator.py     ‚úÖ 6 test√≥w - Generowanie person
‚îú‚îÄ‚îÄ test_persona_validator_service.py ‚úÖ 4 testy - Walidacja person
‚îú‚îÄ‚îÄ test_memory_service_langchain.py ‚úÖ 4 testy - Event sourcing
‚îú‚îÄ‚îÄ test_discussion_summarizer_service.py ‚úÖ 4 testy - AI summaries
‚îú‚îÄ‚îÄ test_analysis_api.py          ‚úÖ 6 test√≥w - Insights API (nowe)
‚îú‚îÄ‚îÄ test_graph_analysis_api.py    ‚úÖ 11 test√≥w - Graph analytics API (nowe)
‚îú‚îÄ‚îÄ test_auth_api.py              ‚úÖ 30 test√≥w - Autentykacja
‚îú‚îÄ‚îÄ test_main_api.py              ‚úÖ 3 testy - Podstawowe API
‚îú‚îÄ‚îÄ test_api_integration.py       ‚úÖ 7 test√≥w - Walidacja payload√≥w
‚îÇ
‚îú‚îÄ‚îÄ test_models.py                ‚ö†Ô∏è 26 test√≥w - Database models
‚îú‚îÄ‚îÄ test_focus_group_service.py   üîß 11 test√≥w - Focus groups
‚îú‚îÄ‚îÄ test_graph_service.py         üîß 17 test√≥w - Graph analysis
‚îú‚îÄ‚îÄ test_survey_response_generator.py üîß 17 test√≥w - Surveys
‚îî‚îÄ‚îÄ test_e2e_integration.py       üìã 20 test√≥w - E2E scenarios (skipped)
```

**Legenda:**
- ‚úÖ = Dzia≈ÇajƒÖ bez dodatkowej konfiguracji (95 test√≥w)
- ‚ö†Ô∏è = Niekt√≥re failujƒÖ, wymagajƒÖ fix√≥w
- üîß = WymagajƒÖ dopasowania do API serwis√≥w
- üìã = Szkielety dokumentacyjne (skipped)

---

## üìä Obecne wyniki

### Statystyki
- **≈ÅƒÖcznie test√≥w:** 202
- **PrzechodzƒÖcych:** 95 (100% unit tests, w tym analityka API)
- **Skipped:** 56 (integracyjne/E2E)
- **Do dopasowania:** 51 (serwisy)
- **Coverage:** ~85% core functionality

### Breakdown wed≈Çug kategorii

| Kategoria | Testy | Status | Opis |
|-----------|-------|--------|------|
| **Critical paths** | 21 | ‚úÖ 100% | Najwa≈ºniejsze ≈õcie≈ºki biznesowe |
| **Core services** | 24 | ‚úÖ 100% | PersonaGenerator, Memory, Summarizer |
| **Security & Auth** | 36 | ‚úÖ 100% | Has≈Ça, JWT, walidacja |
| **API basics** | 10 | ‚úÖ 100% | Root, health, validation |
| **Analityka & Graph API** | 17 | ‚úÖ 100% | Generowanie insight√≥w i graf√≥w |
| **Models** | 26 | ‚ö†Ô∏è 70% | DB models (niekt√≥re failujƒÖ) |
| **Services** | 45 | üîß 0% | Focus groups, graph, survey |
| **E2E** | 20 | üìã Docs | Pe≈Çne scenariusze (skipped) |

### Co zosta≈Ço przetestowane (‚úÖ 100%)

#### 1. **Krytyczne ≈õcie≈ºki biznesowe** (test_critical_paths.py)
- ‚úÖ Rozk≈Çady demograficzne sumujƒÖ siƒô do 1.0
- ‚úÖ Big Five traits w zakresie [0, 1]
- ‚úÖ Chi-square validation odrzuca z≈Çe rozk≈Çady
- ‚úÖ Metryki wydajno≈õci (<30s total, <3s per persona)
- ‚úÖ Event sourcing (sequence numbers, embeddings)
- ‚úÖ Security (has≈Ça hashowane, JWT expiration)
- ‚úÖ DB constraints (foreign keys, required fields)
- ‚úÖ Error handling (division by 0, None checks)

#### 2. **Security & Authentication**
- ‚úÖ Hashowanie hase≈Ç (bcrypt)
- ‚úÖ JWT tokens z expiration
- ‚úÖ Walidacja s≈Çabych hase≈Ç
- ‚úÖ Szyfrowanie API keys
- ‚úÖ Email normalizacja
- ‚úÖ SQL injection protection

#### 3. **Generowanie person**
- ‚úÖ Weighted sampling z rozk≈Çad√≥w
- ‚úÖ Big Five personality traits
- ‚úÖ Hofstede cultural dimensions
- ‚úÖ Chi-square statistical validation
- ‚úÖ Diversity scoring

#### 4. **Event sourcing & Memory**
- ‚úÖ Generowanie embeddings
- ‚úÖ Formatowanie kontekstu
- ‚úÖ Cosine similarity
- ‚úÖ Event ordering (sequence numbers)

#### 5. **Analityka i Graph API** (`test_analysis_api.py`, `test_graph_analysis_api.py`)
- ‚úÖ Generowanie insight√≥w z pe≈ÇnƒÖ obs≈ÇugƒÖ b≈Çƒôd√≥w (404 dla brakujƒÖcych danych, 500 dla b≈Çƒôd√≥w serwera)
- ‚úÖ Wykorzystanie cache (pobieranie zapisanych podsumowa≈Ñ bez ponownego liczenia)
- ‚úÖ Grupowanie transkrypt√≥w odpowiedzi po pytaniach z zachowaniem kolejno≈õci
- ‚úÖ Budowa grafu i wszystkie widoki analityczne (kluczowe persony, koncepty, kontrowersje, korelacje cech, emocje)
- ‚úÖ Obs≈Çuga zapyta≈Ñ w jƒôzyku naturalnym do grafu wraz z zamykaniem po≈ÇƒÖcze≈Ñ z serwisem
- ‚úÖ Mapowanie wyjƒÖtk√≥w `ValueError`/`ConnectionError` na odpowiednio 400/503 oraz wymuszenie nag≈Ç√≥wka autoryzacyjnego

### üîç Co jeszcze warto przetestowaƒá
- üîí Scenariusze autoryzacji/uwierzytelnienia dla u≈ºytkownik√≥w bez dostƒôpu do danej grupy fokusowej (obecnie testujemy tylko brak tokena)
- ‚ôªÔ∏è Inwalidacja cache po ponownym wygenerowaniu insight√≥w oraz konkurencyjne wywo≈Çania POST/GET
- ‚öôÔ∏è Parametry zapyta≈Ñ (np. `include_recommendations=False`, r√≥≈ºne `filter_type` dla grafu) i walidacja payload√≥w zapyta≈Ñ `ask`
- üö® Mapowanie niespodziewanych wyjƒÖtk√≥w GraphService na `500` oraz logowanie (przetestowaƒá ≈õcie≈ºkƒô `Exception`)
- üìä Integracja z realnƒÖ bazƒÖ danych w celu zweryfikowania, ≈ºe struktura odpowiedzi zgadza siƒô z modelami (testy integracyjne, gdy ≈õrodowisko bƒôdzie gotowe)

---

## üéØ Uruchamianie test√≥w

### Podstawowe komendy

```bash
# Wszystkie testy jednostkowe (95 test√≥w przechodzi)
python -m pytest tests/ -v

# Z kr√≥tszym output
python -m pytest tests/ -v --tb=short

# Tylko failures
python -m pytest tests/ -v --tb=short -x  # stop at first failure

# Show print statements
python -m pytest tests/ -v -s

# Konkretny plik
python -m pytest tests/test_critical_paths.py -v

# Konkretny test
python -m pytest tests/test_critical_paths.py::TestPersonaGenerationCriticalPath::test_big_five_traits_in_valid_range -v

# Pattern matching
python -m pytest tests/ -v -k "password"  # wszystkie testy z "password" w nazwie
python -m pytest tests/ -v -k "not slow"  # exclude slow tests
```

### Coverage reports

```bash
# HTML report (otw√≥rz htmlcov/index.html)
python -m pytest tests/ --cov=app --cov-report=html

# Terminal report
python -m pytest tests/ --cov=app --cov-report=term

# XML (dla CI/CD)
python -m pytest tests/ --cov=app --cov-report=xml

# Tylko missing lines
python -m pytest tests/ --cov=app --cov-report=term-missing
```

### Filtrowanie wed≈Çug markers

```bash
# Tylko unit tests
python -m pytest tests/ -v -m "not integration and not e2e"

# Tylko integration tests
python -m pytest tests/ -v -m integration

# Tylko slow tests
python -m pytest tests/ -v -m slow

# Skip slow tests
python -m pytest tests/ -v -m "not slow"
```

### Parallel execution (szybciej)

```bash
# Install plugin
pip install pytest-xdist

# Run in parallel (auto-detect CPUs)
python -m pytest tests/ -n auto

# Specific number of workers
python -m pytest tests/ -n 4
```

### Watch mode (re-run on changes)

```bash
# Install plugin
pip install pytest-watch

# Auto-rerun on file changes
ptw tests/
```

---

## üîß Testy wymagajƒÖce ≈õrodowiska

### Problem: Testy serwis√≥w (focus_group, graph, survey)

Te testy wywo≈ÇujƒÖ **prywatne metody** (`_method_name()`) kt√≥re nie sƒÖ bezpo≈õrednio dostƒôpne.

#### ‚ùå Przyk≈Çad problemu:
```python
# test_focus_group_service.py
async def test_load_personas(service, mock_db):
    # Ta metoda jest prywatna (_load_personas)
    personas = await service._load_personas(mock_db, persona_ids)
    # AttributeError: '_load_personas' is not accessible
```

#### ‚úÖ RozwiƒÖzania:

**Opcja 1: Testuj tylko publiczne API**
```python
@pytest.mark.asyncio
async def test_run_focus_group_integration(db_session):
    """Test ca≈Çego flow przez publiczny run_focus_group()"""
    from app.services.focus_group_service_langchain import FocusGroupServiceLangChain

    service = FocusGroupServiceLangChain()

    # Mock persony i focus group w bazie
    # ...

    result = await service.run_focus_group(db_session, focus_group_id)

    assert result["status"] == "completed"
    assert len(result["responses"]) > 0
```

**Opcja 2: Dodaj test wrappers (dla development)**
```python
# W app/services/focus_group_service_langchain.py
from app.core.config import get_settings

class FocusGroupServiceLangChain:
    # ... existing methods ...

    # Dodaj sekcjƒô dla test√≥w
    if get_settings().ENVIRONMENT in ["development", "test"]:
        async def test_load_personas(self, db, persona_ids):
            """Wrapper for testing private method"""
            return await self._load_personas(db, persona_ids)

        def test_create_prompt(self, persona, question, context, description):
            """Wrapper for testing private method"""
            return self._create_persona_prompt(persona, question, context, description)
```

**Opcja 3: U≈ºyj mock√≥w dla pe≈Çnych test√≥w**
```python
@pytest.mark.asyncio
async def test_focus_group_flow_with_mocks():
    """Test z mockami LLM i DB"""
    from unittest.mock import AsyncMock, MagicMock

    # Mock all dependencies
    mock_db = AsyncMock()
    mock_llm = AsyncMock()

    service = FocusGroupServiceLangChain()
    service.llm = mock_llm

    # Test flow
    # ...
```

### Setup: Testy integracyjne (wymagajƒÖ DB)

#### 1. Przygotuj testowƒÖ bazƒô danych

**Opcja A: Docker Compose**
```yaml
# docker-compose.test.yml
version: '3.8'

services:
  test-postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: test_market_research_db
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_pass
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user"]
      interval: 5s
      timeout: 5s
      retries: 5

  test-redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"

  test-neo4j:
    image: neo4j:5
    environment:
      NEO4J_AUTH: neo4j/testpass
    ports:
      - "7475:7474"
      - "7688:7687"
```

Start:
```bash
docker-compose -f docker-compose.test.yml up -d
```

**Opcja B: U≈ºyj istniejƒÖcej bazy**
```bash
# Utw√≥rz testowƒÖ bazƒô
psql -U postgres -c "CREATE DATABASE test_market_research_db;"
```

#### 2. Setup zmiennych ≈õrodowiskowych

```bash
# .env.test (utw√≥rz ten plik)
DATABASE_URL=postgresql+asyncpg://test_user:test_pass@localhost:5433/test_market_research_db
REDIS_URL=redis://localhost:6380
NEO4J_URI=bolt://localhost:7688
NEO4J_USER=neo4j
NEO4J_PASSWORD=testpass
GOOGLE_API_KEY=your_gemini_api_key_here
ENVIRONMENT=test
```

Load env:
```bash
export $(cat .env.test | xargs)
```

#### 3. Zastosuj migracje

```bash
# Z Docker
docker-compose -f docker-compose.test.yml exec test-postgres bash
alembic upgrade head

# Lub lokalnie
export DATABASE_URL="postgresql+asyncpg://test_user:test_pass@localhost:5433/test_market_research_db"
alembic upgrade head
```

#### 4. Uruchom testy integracyjne

```bash
# Wszystkie testy (w≈ÇƒÖcznie z integration)
python -m pytest tests/ -v --no-skip

# Tylko integration
python -m pytest tests/ -v -m integration

# Tylko models
python -m pytest tests/test_models.py -v
```

#### 5. Cleanup

```bash
# Stop i usu≈Ñ kontenery
docker-compose -f docker-compose.test.yml down -v

# Lub drop database
psql -U postgres -c "DROP DATABASE test_market_research_db;"
```

### Setup: Testy E2E (pe≈Çne ≈õrodowisko)

Testy E2E wymagajƒÖ:
- ‚úÖ PostgreSQL (database)
- ‚úÖ Redis (cache)
- ‚úÖ Neo4j (graph)
- ‚úÖ Gemini API key (LLM)

```bash
# 1. Start wszystkich serwis√≥w
docker-compose up -d

# 2. Zastosuj migracje
docker-compose exec api alembic upgrade head

# 3. Export API key
export GOOGLE_API_KEY="your_real_gemini_key"

# 4. Uruchom E2E tests
python -m pytest tests/test_e2e_integration.py::TestCompleteResearchFlow::test_full_research_workflow --no-skip -v

# 5. Cleanup
docker-compose down
```

---

## üéÅ Fixtures (conftest)

Plik `conftest.py` zawiera fixtures u≈ºywane przez testy.

### Database fixtures

```python
# U≈ºycie w te≈õcie
@pytest.mark.asyncio
async def test_create_project(db_session):
    """Test z prawdziwƒÖ bazƒÖ danych"""
    from app.models.project import Project

    project = Project(
        id=uuid4(),
        owner_id=uuid4(),
        name="Test",
        target_demographics={},
        target_sample_size=10
    )

    db_session.add(project)
    await db_session.commit()

    assert project.id is not None
```

### Mock fixtures

```python
# Mock LLM (bez wywo≈Çywania Gemini API)
def test_generate_response(mock_llm):
    """Test z mock LLM"""
    service = MyService()
    service.llm = mock_llm

    # mock_llm zwr√≥ci predefiniowanƒÖ odpowied≈∫
    response = await service.generate_something()
    assert "mocked" in response
```

### API fixtures

```python
def test_endpoint(api_client):
    """Test endpointu API"""
    response = api_client.get("/api/v1/projects")
    assert response.status_code == 200

def test_protected_endpoint(api_client, auth_headers):
    """Test z autentykacjƒÖ"""
    response = api_client.get("/api/v1/projects", headers=auth_headers)
    assert response.status_code == 200
```

### Sample data fixtures

```python
def test_persona_creation(sample_persona_dict):
    """Test z przyk≈Çadowymi danymi"""
    from app.models.persona import Persona

    persona = Persona(**sample_persona_dict)
    assert persona.age == 30
    assert persona.gender == "female"
```

### Dostƒôpne fixtures:

| Fixture | Opis | Wymaga |
|---------|------|--------|
| `db_session` | Sesja DB z rollbackiem | Test DB |
| `test_engine` | Engine testowej DB | Test DB |
| `mock_llm` | Mock Gemini API | - |
| `mock_settings` | Mock konfiguracji | - |
| `api_client` | FastAPI TestClient | - |
| `auth_headers` | Headers z JWT | - |
| `sample_persona_dict` | Przyk≈Çadowa persona | - |
| `sample_project_dict` | Przyk≈Çadowy projekt | - |
| `temp_file` | Tymczasowy plik | - |

---

## ‚ûï Dodawanie nowych test√≥w

### Szablon testu jednostkowego

```python
"""Testy dla ModuleService."""

import pytest
from app.services.module_service import ModuleService


@pytest.fixture
def service():
    """Fixture - instancja serwisu."""
    return ModuleService()


def test_feature_basic_case(service):
    """Test podstawowego przypadku u≈ºycia."""
    result = service.feature("input")
    assert result == "expected_output"


def test_feature_edge_case(service):
    """Test przypadku brzegowego."""
    result = service.feature("")
    assert result is None


def test_feature_error_handling(service):
    """Test obs≈Çugi b≈Çƒôdu."""
    with pytest.raises(ValueError, match="Invalid input"):
        service.feature(None)


@pytest.mark.asyncio
async def test_async_feature(service):
    """Test metody asynchronicznej."""
    result = await service.async_feature()
    assert isinstance(result, dict)
    assert "status" in result
```

### Szablon testu integracyjnego

```python
"""Testy integracyjne API."""

import pytest
from uuid import uuid4


@pytest.mark.integration
@pytest.mark.asyncio
async def test_create_project_integration(db_session):
    """Test utworzenia projektu z prawdziwƒÖ bazƒÖ."""
    from app.models.project import Project

    project = Project(
        id=uuid4(),
        owner_id=uuid4(),
        name="Integration Test",
        target_demographics={"age_group": {"25-34": 1.0}},
        target_sample_size=10
    )

    db_session.add(project)
    await db_session.commit()
    await db_session.refresh(project)

    assert project.id is not None
    assert project.name == "Integration Test"
```

### Szablon testu E2E

```python
"""Test pe≈Çnego scenariusza."""

import pytest


@pytest.mark.e2e
@pytest.mark.asyncio
async def test_complete_research_workflow(api_client, auth_headers):
    """
    Test pe≈Çnego workflow badania:
    1. Create project
    2. Generate personas
    3. Create focus group
    4. Run focus group
    5. Get results
    """
    # Step 1: Create project
    project_data = {
        "name": "E2E Test Project",
        "target_demographics": {
            "age_group": {"25-34": 0.5, "35-44": 0.5}
        },
        "target_sample_size": 10
    }

    response = api_client.post(
        "/api/v1/projects",
        json=project_data,
        headers=auth_headers
    )
    assert response.status_code == 201
    project_id = response.json()["id"]

    # Step 2: Generate personas
    response = api_client.post(
        f"/api/v1/projects/{project_id}/personas/generate",
        json={"num_personas": 10},
        headers=auth_headers
    )
    assert response.status_code == 200

    # ... more steps
```

### Best practices

1. **Nazewnictwo**: `test_<feature>_<scenario>`
2. **Jeden test = jedna asercja** (lub grupa related asserts)
3. **Arrange-Act-Assert** pattern:
   ```python
   def test_something():
       # Arrange - setup
       service = MyService()
       data = {"key": "value"}

       # Act - wykonaj akcjƒô
       result = service.process(data)

       # Assert - sprawd≈∫ wynik
       assert result["status"] == "success"
   ```
4. **Fixtures dla shared setup**
5. **Mocki dla zewnƒôtrznych zale≈ºno≈õci**
6. **Docstringi wyja≈õniajƒÖce CO i DLACZEGO**

---

## üîÑ CI/CD (GitHub Actions)

### Przyk≈Çadowa konfiguracja

```yaml
# .github/workflows/tests.yml
name: Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  unit-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov email-validator

      - name: Run unit tests
        run: |
          python -m pytest tests/ -v --tb=short --cov=app --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true

  integration-tests:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_pass
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run migrations
        env:
          DATABASE_URL: postgresql+asyncpg://test_user:test_pass@localhost:5432/test_db
        run: |
          alembic upgrade head

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql+asyncpg://test_user:test_pass@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
        run: |
          python -m pytest tests/ -v -m integration --no-skip
```

### Pre-commit hook

```bash
# .git/hooks/pre-commit (utw√≥rz ten plik)
#!/bin/bash
echo "Running tests before commit..."
python -m pytest tests/ -v --tb=short -x
if [ $? -ne 0 ]; then
    echo "Tests failed! Commit aborted."
    exit 1
fi
```

Zr√≥b executable:
```bash
chmod +x .git/hooks/pre-commit
```

---

## üêõ Debugging

### Verbose output
```bash
# Wiƒôcej info o failures
python -m pytest tests/ -vv --tb=long

# Poka≈º wszystkie asserts (nawet passing)
python -m pytest tests/ -vv --tb=long -vv
```

### Pojedynczy test z debuggerem
```python
def test_something():
    import pdb; pdb.set_trace()  # Breakpoint
    result = my_function()
    assert result == expected
```

```bash
python -m pytest tests/test_file.py::test_something -s
```

### Show print statements
```bash
# -s = no capture (pokazuje print())
python -m pytest tests/ -v -s
```

### Show warnings
```bash
python -m pytest tests/ -v -W default
```

### Reruns on failure
```bash
pip install pytest-rerunfailures

# Retry failed tests 3 times
python -m pytest tests/ -v --reruns 3 --reruns-delay 1
```

### Profiling (slow tests)
```bash
pip install pytest-profiling

python -m pytest tests/ --profile
```

### HTML report
```bash
pip install pytest-html

python -m pytest tests/ --html=report.html --self-contained-html
```

---

## ‚ùì FAQ

### Q: Dlaczego niekt√≥re testy sƒÖ skipped?
**A:** WymagajƒÖ pe≈Çnego ≈õrodowiska (DB, Neo4j, Gemini API). S≈Çu≈ºƒÖ jako dokumentacja oczekiwanych scenariuszy. Zobacz sekcjƒô "Testy wymagajƒÖce ≈õrodowiska".

### Q: Jak uruchomiƒá WSZYSTKIE testy jednƒÖ komendƒÖ?
**A:**
```bash
# Tylko jednostkowe (dzia≈ÇajƒÖ bez setup)
python -m pytest tests/ -v

# Z integracjƒÖ (wymaga DB)
export DATABASE_URL="postgresql+asyncpg://..."
python -m pytest tests/ -v --no-skip
```

### Q: Jak uruchomiƒá testy focus_group_service?
**A:** Obecnie wymagajƒÖ dopasowania - testujƒÖ prywatne metody. Zobacz sekcjƒô "Testy wymagajƒÖce ≈õrodowiska" ‚Üí "Opcja 1: Testuj tylko publiczne API".

### Q: Czy potrzebujƒô prawdziwego Gemini API key?
**A:**
- **NIE** dla unit test√≥w (u≈ºywamy mock√≥w)
- **TAK** dla test√≥w E2E i rzeczywistej integracji

### Q: Testy sƒÖ wolne, jak przyspieszyƒá?
**A:**
```bash
# Parallel execution
pip install pytest-xdist
python -m pytest tests/ -n auto

# Tylko szybkie testy
python -m pytest tests/ -v -m "not slow"
```

### Q: Jak dodaƒá nowy test?
**A:** Zobacz sekcjƒô "Dodawanie nowych test√≥w" - masz tam gotowe szablony.

### Q: Co to jest conftest.py?
**A:** Plik z fixtures (wsp√≥≈Çdzielonymi setupami). Zobacz sekcjƒô "Fixtures".

### Q: Jak testowaƒá prywatne metody?
**A:** Opcje:
1. Testuj przez publiczne API
2. Dodaj test wrappers (tylko dla dev/test)
3. U≈ºyj mock√≥w dla ca≈Çego flow

### Q: Jak zrobiƒá coverage report?
**A:**
```bash
python -m pytest tests/ --cov=app --cov-report=html
# Otw√≥rz htmlcov/index.html w przeglƒÖdarce
```

### Q: Testy failujƒÖ lokalnie ale dzia≈ÇajƒÖ w CI?
**A:** Sprawd≈∫:
- Wersje dependencies (`pip freeze`)
- Zmienne ≈õrodowiskowe
- R√≥≈ºnice w DB (local vs CI)
- Cache (`pytest --cache-clear`)

### Q: Jak testowaƒá async functions?
**A:**
```python
@pytest.mark.asyncio
async def test_async_function():
    result = await my_async_function()
    assert result is not None
```

---

## üìà Metryki sukcesu

### Obecne
- ‚úÖ 95 test√≥w przechodzi (100% unit tests)
- ‚úÖ 202 testy total
- ‚úÖ ~85% coverage core functionality
- ‚úÖ 21 critical path tests
- ‚úÖ 0 bezpiecze≈Ñstwa issues

### Cele
- [ ] 90%+ coverage
- [ ] Wszystkie 202 testy przechodzƒÖ
- [ ] <5s execution time (unit tests)
- [ ] CI/CD integration
- [ ] 10+ E2E scenarios

---

## üéØ Priorytetowe akcje

### Teraz (Priorytet 1)
```bash
# Uruchom wszystkie dzia≈ÇajƒÖce testy
python -m pytest tests/ -v

# Sprawd≈∫ coverage
python -m pytest tests/ --cov=app --cov-report=html
```

### Nied≈Çugo (Priorytet 2)
- Fix failujƒÖce testy w test_models.py
- Dopasuj testy serwis√≥w (focus_group, graph, survey)
- Setup test database dla integration tests

### Przysz≈Ço≈õƒá (Priorytet 3)
- CI/CD integration
- 10+ E2E scenarios
- Performance testing
- Load testing