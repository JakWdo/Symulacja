"""
Test A/B por√≥wnania konfiguracji RAG
====================================

Skrypt do testowania wp≈Çywu r√≥≈ºnych parametr√≥w RAG na jako≈õƒá retrieval:
- R√≥≈ºne rozmiary chunk√≥w (1000 vs 2000)
- R√≥≈ºne warto≈õci RRF_K (40, 60, 80)
- R√≥≈ºne TOP_K (5, 8, 10)

U≈ºycie:
    python tests/manual/test_rag_ab_comparison.py

Wymaga:
    - Uruchomiony Neo4j z zaindeksowanymi dokumentami
    - GOOGLE_API_KEY w .env
"""

import asyncio
import time
from typing import Dict, List, Any
from dataclasses import dataclass

from app.services.rag_service import PolishSocietyRAG
from app.core.config import get_settings

settings = get_settings()


@dataclass
class TestQuery:
    """Zapytanie testowe z oczekiwanym kontekstem"""
    query_text: str
    demographics: Dict[str, str]
    expected_keywords: List[str]  # S≈Çowa kluczowe kt√≥re powinny byƒá w wynikach
    description: str


# Test queries reprezentatywne dla use case
TEST_QUERIES = [
    TestQuery(
        query_text="m≈Çodzi ludzie w du≈ºych miastach z wykszta≈Çceniem wy≈ºszym",
        demographics={
            "age_group": "25-34",
            "location": "Warszawa",
            "education": "wy≈ºsze",
            "gender": "kobieta"
        },
        expected_keywords=["m≈Çod", "miasto", "uniwersytet", "studi", "wykszta≈Çcen"],
        description="M≈Çoda kobieta w stolicy z wy≈ºszym wykszta≈Çceniem"
    ),
    TestQuery(
        query_text="seniorzy w ma≈Çych miastach z podstawowym wykszta≈Çceniem",
        demographics={
            "age_group": "65+",
            "location": "Kielce",
            "education": "podstawowe",
            "gender": "mƒô≈ºczyzna"
        },
        expected_keywords=["senior", "emeryt", "starszy", "wiek", "podstawow"],
        description="Senior w ma≈Çym mie≈õcie"
    ),
    TestQuery(
        query_text="osoby w ≈õrednim wieku w ≈õrednich miastach ze ≈õrednim wykszta≈Çceniem",
        demographics={
            "age_group": "45-54",
            "location": "Bydgoszcz",
            "education": "≈õrednie",
            "gender": "kobieta"
        },
        expected_keywords=["≈õredni", "wiek", "liceum", "miasto"],
        description="Osoba w ≈õrednim wieku, ≈õrednie miasto"
    ),
    TestQuery(
        query_text="m≈Çodzi absolwenci z wielkich miast poszukujƒÖcy pracy",
        demographics={
            "age_group": "22-28",
            "location": "Krak√≥w",
            "education": "wy≈ºsze",
            "gender": "mƒô≈ºczyzna"
        },
        expected_keywords=["absolwent", "praca", "zawod", "kariera", "m≈Çod"],
        description="M≈Çody absolwent szukajƒÖcy pracy"
    ),
]


def calculate_keyword_coverage(
    context: str,
    expected_keywords: List[str]
) -> float:
    """
    Oblicz pokrycie expected keywords w kontek≈õcie

    Returns:
        Float 0.0-1.0 reprezentujƒÖcy % pokrycia keywords
    """
    context_lower = context.lower()
    matched = sum(1 for keyword in expected_keywords if keyword in context_lower)
    return matched / len(expected_keywords) if expected_keywords else 0.0


def calculate_relevance_score(
    citations: List[Dict[str, Any]],
    expected_keywords: List[str]
) -> float:
    """
    Oblicz ≈õredni relevance score dla citations

    Bazuje na:
    1. RRF/rerank scores z citations
    2. Keyword coverage w ka≈ºdym citation
    """
    if not citations:
        return 0.0

    scores = []
    for citation in citations:
        # Score z retrieval (RRF lub rerank)
        retrieval_score = citation.get('score', 0.0)

        # Keyword coverage w tym citation
        text = citation.get('text', '')
        keyword_score = calculate_keyword_coverage(text, expected_keywords)

        # Weighted average: 70% retrieval, 30% keywords
        combined = 0.7 * retrieval_score + 0.3 * keyword_score
        scores.append(combined)

    return sum(scores) / len(scores)


async def test_rag_configuration(
    test_query: TestQuery,
    config_name: str
) -> Dict[str, Any]:
    """
    Test RAG z obecnƒÖ konfiguracjƒÖ

    Args:
        test_query: Query do przetestowania
        config_name: Nazwa konfiguracji (dla logging)

    Returns:
        Dict z metrykami performance
    """
    print(f"\n{'='*60}")
    print(f"Testing config: {config_name}")
    print(f"Query: {test_query.description}")
    print(f"{'='*60}")

    rag = PolishSocietyRAG()

    # Measure latency
    start_time = time.time()

    results = await rag.get_demographic_insights(
        age_group=test_query.demographics['age_group'],
        education=test_query.demographics['education'],
        location=test_query.demographics['location'],
        gender=test_query.demographics['gender']
    )

    latency = time.time() - start_time

    # Extract metrics
    context = results.get('context', '')
    citations = results.get('citations', [])
    num_results = results.get('num_results', 0)
    search_type = results.get('search_type', 'unknown')

    # Calculate quality metrics
    keyword_coverage = calculate_keyword_coverage(context, test_query.expected_keywords)
    relevance_score = calculate_relevance_score(citations, test_query.expected_keywords)
    context_length = len(context)

    # Print results
    print(f"\nüìä Results:")
    print(f"  Search type: {search_type}")
    print(f"  Latency: {latency:.2f}s")
    print(f"  Num results: {num_results}")
    print(f"  Context length: {context_length} chars")
    print(f"  Keyword coverage: {keyword_coverage:.2%}")
    print(f"  Relevance score: {relevance_score:.3f}")

    # Show top 3 citations
    print(f"\nüìÑ Top 3 citations:")
    for i, citation in enumerate(citations[:3], 1):
        score = citation.get('score', 0.0)
        text = citation.get('text', '')[:150]
        enriched = citation.get('enriched', False)
        print(f"  {i}. Score: {score:.3f} | Enriched: {enriched}")
        print(f"     {text}...")

    return {
        'config_name': config_name,
        'query_description': test_query.description,
        'latency': latency,
        'num_results': num_results,
        'context_length': context_length,
        'keyword_coverage': keyword_coverage,
        'relevance_score': relevance_score,
        'search_type': search_type
    }


async def run_ab_test():
    """
    Uruchom A/B test dla wszystkich test queries

    Por√≥wnuje obecnƒÖ konfiguracjƒô z baseline metrics
    """
    print("="*80)
    print("RAG A/B COMPARISON TEST")
    print("="*80)
    print(f"\nüìã Current configuration:")
    print(f"  CHUNK_SIZE: {settings.RAG_CHUNK_SIZE}")
    print(f"  CHUNK_OVERLAP: {settings.RAG_CHUNK_OVERLAP}")
    print(f"  TOP_K: {settings.RAG_TOP_K}")
    print(f"  MAX_CONTEXT: {settings.RAG_MAX_CONTEXT_CHARS}")
    print(f"  RRF_K: {settings.RAG_RRF_K}")
    print(f"  RERANK_CANDIDATES: {settings.RAG_RERANK_CANDIDATES}")
    print(f"  RERANKER_MODEL: {settings.RAG_RERANKER_MODEL}")
    print(f"  USE_HYBRID_SEARCH: {settings.RAG_USE_HYBRID_SEARCH}")
    print(f"  USE_RERANKING: {settings.RAG_USE_RERANKING}")

    all_results = []

    # Test ka≈ºdego query
    for query in TEST_QUERIES:
        result = await test_rag_configuration(
            query,
            config_name="current"
        )
        all_results.append(result)

        # Pauza miƒôdzy queries
        await asyncio.sleep(1)

    # Aggregate metrics
    print("\n" + "="*80)
    print("üìä AGGREGATE METRICS")
    print("="*80)

    avg_latency = sum(r['latency'] for r in all_results) / len(all_results)
    avg_keyword_coverage = sum(r['keyword_coverage'] for r in all_results) / len(all_results)
    avg_relevance = sum(r['relevance_score'] for r in all_results) / len(all_results)
    avg_context_length = sum(r['context_length'] for r in all_results) / len(all_results)

    print(f"\n  Average latency: {avg_latency:.2f}s")
    print(f"  Average keyword coverage: {avg_keyword_coverage:.2%}")
    print(f"  Average relevance score: {avg_relevance:.3f}")
    print(f"  Average context length: {avg_context_length:.0f} chars")

    # Compare to baseline (hardcoded from previous runs)
    # TODO: Load from file or database for true A/B comparison
    print("\n" + "="*80)
    print("üìà COMPARISON TO BASELINE (OLD CONFIG)")
    print("="*80)
    print("\nBASELINE (chunk=2000, overlap=400, top_k=5):")
    print("  Average latency: ~1.2s (estimated)")
    print("  Average keyword coverage: ~65% (estimated)")
    print("  Average relevance score: ~0.45 (estimated)")
    print("  Average context length: ~4500 chars (estimated)")

    print("\nCURRENT (chunk=1000, overlap=300, top_k=8):")
    print(f"  Average latency: {avg_latency:.2f}s")
    print(f"  Average keyword coverage: {avg_keyword_coverage:.2%}")
    print(f"  Average relevance score: {avg_relevance:.3f}")
    print(f"  Average context length: {avg_context_length:.0f} chars")

    # Recommendations
    print("\n" + "="*80)
    print("üí° RECOMMENDATIONS")
    print("="*80)

    if avg_keyword_coverage > 0.70:
        print("‚úÖ Keyword coverage jest dobra (>70%)")
    else:
        print("‚ö†Ô∏è  Keyword coverage poni≈ºej 70% - rozwa≈º zmianƒô chunking strategy")

    if avg_relevance > 0.50:
        print("‚úÖ Relevance score jest dobry (>0.50)")
    else:
        print("‚ö†Ô∏è  Relevance score poni≈ºej 0.50 - rozwa≈º tuning RRF_K lub reranker")

    if avg_latency < 1.5:
        print("‚úÖ Latency jest akceptowalna (<1.5s)")
    else:
        print("‚ö†Ô∏è  Latency wysoka - rozwa≈º zmniejszenie TOP_K lub wy≈ÇƒÖczenie reranking")

    print("\n" + "="*80)
    print("üèÅ TEST COMPLETED")
    print("="*80)


if __name__ == "__main__":
    asyncio.run(run_ab_test())
