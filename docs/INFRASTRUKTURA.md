# Infrastruktura i Deployment - Sight Platform

**Ostatnia aktualizacja:** 2025-11-03
**Wersja:** 2.0
**Status:** Production-ready

---

## Spis TreÅ›ci

1. [PrzeglÄ…d Infrastruktury](#przeglÄ…d-infrastruktury)
2. [Architektura Docker](#architektura-docker)
3. [Local Development](#local-development)
4. [Cloud Run Production](#cloud-run-production)
5. [External Services](#external-services)
6. [CI/CD Pipeline](#cicd-pipeline)
7. [Monitoring & Observability](#monitoring--observability)

---

## PrzeglÄ…d Infrastruktury

Platforma Sight zostaÅ‚a zaprojektowana z myÅ›lÄ… o nowoczesnej architekturze kontenerowej, ktÃ³ra zapewnia spÃ³jnoÅ›Ä‡ miÄ™dzy Å›rodowiskiem deweloperskim a produkcyjnym. System opiera siÄ™ na piÄ™ciu kluczowych serwisach uruchamianych w kontenerach Docker: PostgreSQL z rozszerzeniem pgvector dla wektorowych operacji AI, Redis jako warstwa cache'owania i zarzÄ…dzania sesjami, Neo4j z pluginami APOC i Graph Data Science dla zaawansowanych analiz grafowych, backend FastAPI z asynchronicznym przetwarzaniem oraz frontend React z TypeScript.

Nasza infrastruktura przeszÅ‚a przez znaczÄ…ce optymalizacje w ostatnich miesiÄ…cach. UdaÅ‚o siÄ™ zmniejszyÄ‡ rozmiar obrazÃ³w Docker o 84% (z 55GB do 11GB), zredukowaÄ‡ czas budowania o 67%, oraz naprawiÄ‡ 54 CVE zwiÄ…zanych z bezpieczeÅ„stwem. Deployment zostaÅ‚ w peÅ‚ni zautomatyzowany - od push do GitHub do dziaÅ‚ajÄ…cej aplikacji w Cloud Run zajmuje obecnie 8-12 minut, z automatycznymi migracjami bazy danych i inicjalizacjÄ… indeksÃ³w Neo4j.

### Kluczowe Cele Architektury

Infrastruktura Sight realizuje cztery nadrzÄ™dne cele. Pierwszym z nich jest **consistency across environments** - kod ktÃ³ry dziaÅ‚a lokalnie musi dziaÅ‚aÄ‡ identycznie w produkcji. Dlatego uÅ¼ywamy tych samych obrazÃ³w Docker, tych samych wersji dependencies i tej samej konfiguracji sieciowej zarÃ³wno na maszynach deweloperskich jak i w Cloud Run. Drugi cel to **developer experience** - deweloper powinien mÃ³c uruchomiÄ‡ peÅ‚ny stack jednÄ… komendÄ… (`docker-compose up -d`) i natychmiast zaczÄ…Ä‡ kodowaÄ‡ z hot reload. Trzeci cel to **cost optimization** - Cloud Run scale-uje do zera instancji gdy nie ma ruchu, Redis cache redukuje wywoÅ‚ania LLM o 80%, a aggressive Docker layer caching oszczÄ™dza czas i pieniÄ…dze w CI/CD. Czwarty cel to **observability** - kaÅ¼da operacja jest logowana ze structured fields, metryki wydajnoÅ›ciowe sÄ… Å›ledzone w real-time, a alerts informujÄ… zespÃ³Å‚ o anomaliach zanim wpÅ‚ynÄ… na uÅ¼ytkownikÃ³w koÅ„cowych.

### Stack Technologiczny

**Backend Infrastructure:**
- FastAPI 0.110+ (async Python web framework)
- SQLAlchemy 2.0 (async ORM z connection pooling)
- PostgreSQL 15 z pgvector (wektorowa baza danych)
- Redis 7 (cache, rate limiting, distributed locking)
- Neo4j 5.x (grafowa baza wiedzy dla RAG)
- Uvicorn (ASGI server z multiple workers)

**Frontend Infrastructure:**
- React 18 + TypeScript (komponentowa architektura)
- Vite 5.x (ultra-fast dev server + build tool)
- TanStack Query (server state management z automatic caching)
- Zustand (lightweight UI state management)
- Nginx (static file serving w produkcji)

**Deployment Infrastructure:**
- Docker Compose (local development orchestration)
- Google Cloud Run (serverless container platform)
- Google Artifact Registry (Docker image storage)
- Google Cloud Build (CI/CD automation)
- Google Secret Manager (secure credentials storage)
- Google Cloud SQL (managed PostgreSQL w produkcji)

**External Services (Production):**
- Neo4j AuraDB Free (50,000 nodes, Europe West 1)
- Upstash Redis Free (10,000 requests/day, Europe West 1)
- Google Gemini API (LLM provider - Flash i Pro modele)

---

## Architektura Docker

### Multi-Stage Builds

KaÅ¼dy serwis wykorzystuje wieloetapowe buildy Docker, ktÃ³re drastycznie redukujÄ… rozmiar finalnych obrazÃ³w. Backend FastAPI przechodzi przez trzy etapy: builder (instalacja dependencies z pip wheel compilation), runtime (kopiowanie aplikacji i config files), oraz production (minimalistyczny obraz z tylko niezbÄ™dnymi zaleÅ¼noÅ›ciami). Frontend React rÃ³wnieÅ¼ korzysta z czterech etapÃ³w: deps (node_modules installation), builder (Vite build do /dist), dev (serwer deweloperski z hot reload), oraz prod (statyczny Nginx serwujÄ…cy zbudowane pliki).

**Dockerfile.cloudrun - Production Image:**

Produkcyjny Dockerfile Å‚Ä…czy frontend i backend w jeden obraz, co upraszcza deployment i eliminuje CORS issues. Pierwszy stage builduje frontend React uÅ¼ywajÄ…c Vite - wszystkie statyczne assety trafiajÄ… do `/dist` folderu z hash suffixes dla cache busting. Drugi stage instaluje Python dependencies uÅ¼ywajÄ…c pip z wheel compilation dla native extensions (numpy, scipy dla embeddings). Trzeci stage kopiuje zbudowany frontend do `/app/static`, instaluje backend Python code, i konfiguruje uvicorn jako entry point. Finalny obraz waÅ¼y okoÅ‚o 2.8GB (poprzednio 5.5GB przed optymalizacjami), co przekÅ‚ada siÄ™ na szybsze deploymenty i niÅ¼sze koszty network egress.

**Docker Compose - Local Development:**

Konfiguracja `docker-compose.yml` definiuje siedem serwisÃ³w (postgres, redis, neo4j, migrate, neo4j-init, api, frontend) z precyzyjnie skonfigurowanymi health checks i dependency chains. Health checks zapewniajÄ… Å¼e serwisy startujÄ… w poprawnej kolejnoÅ›ci - PostgreSQL musi byÄ‡ healthy zanim migrate uruchomi alembic, a Neo4j musi byÄ‡ healthy zanim neo4j-init utworzy indeksy. Wszystkie serwisy sÄ… poÅ‚Ä…czone do jednej sieci `backend` dla internal communication, co pozwala im komunikowaÄ‡ siÄ™ po nazwach hostÃ³w (np. `postgres:5432`, `redis:6379`) zamiast IP addresses.

### Resource Limits

KaÅ¼dy serwis ma zdefiniowane limity CPU i RAM zarÃ³wno dla Å›rodowiska deweloperskiego jak i produkcyjnego. Backend API w development wykorzystuje 1 CPU core i 512MB RAM, podczas gdy w produkcji otrzymuje 2 CPU cores i 1.5GB RAM dla lepszej wydajnoÅ›ci przy obsÅ‚udze rÃ³wnolegÅ‚ych wywoÅ‚aÅ„ LLM. Frontend w development jest ograniczony do 0.5 CPU i 256MB, co wystarcza dla hot reload, natomiast w produkcji (Nginx) potrzebuje 1 CPU i 512MB.

PostgreSQL z pgvector alokuje 1 CPU i 1GB RAM w development, ale w produkcji skaluje siÄ™ do 2 CPU i 4GB dla obsÅ‚ugi embeddings i wektorowych zapytaÅ„. Redis, bÄ™dÄ…cy in-memory database, otrzymuje 256MB w dev i 1GB w prod. Neo4j, najbardziej resource-hungry serwis, wymaga 2GB w development i aÅ¼ 8GB w produkcji dla przetwarzania zÅ‚oÅ¼onych grafÃ³w i Graph Data Science algorithms.

**docker-compose.yml - Resource Configuration:**

```yaml
services:
  api:
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 1G

  postgres:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  neo4j:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G
```

Reservations okreÅ›lajÄ… minimalny guaranteed resource allocation, podczas gdy limits definiujÄ… maksymalny burst capacity. To pozwala na elastyczne zarzÄ…dzanie resources - serwisy mogÄ… burst powyÅ¼ej reservations gdy host ma wolne zasoby, ale nigdy nie przekroczÄ… limits.

### Volume Management

Docker volumes zapewniajÄ… persistence dla danych miÄ™dzy restartami kontenerÃ³w. Mamy piÄ™Ä‡ named volumes: `postgres_data` dla bazy danych, `redis_data` dla Redis persistence (AOF enabled), `neo4j_data` i `neo4j_logs` dla Neo4j, oraz `frontend_node_modules` aby uniknÄ…Ä‡ reinstalacji node_modules przy kaÅ¼dym rebuild kontenera frontend.

Named volumes sÄ… zarzÄ…dzane przez Docker daemon i przetrwajÄ… `docker-compose down`. Aby caÅ‚kowicie wyczyÅ›ciÄ‡ Å›rodowisko (fresh start), trzeba uÅ¼yÄ‡ `docker-compose down -v` ktÃ³ry usuwa volumes, lub manualnie `docker volume rm sight_postgres_data`. Bind mounts (`./:/app` dla hot reload) montujÄ… lokalne foldery bezpoÅ›rednio do kontenera, co pozwala na natychmiastowe odbicie zmian w kodzie bez rebuildu.

### Networking

Wszystkie serwisy sÄ… poÅ‚Ä…czone do custom network `backend` (bridge driver). To zapewnia izolacjÄ™ od innych Docker containers na hoÅ›cie oraz pozwala na automatic DNS resolution - kaÅ¼dy serwis jest dostÄ™pny po nazwie (np. `postgres`, `redis`, `neo4j`) z automatycznym load balancing jeÅ›li skalujemy replicas. Port publishing (`ports: - "8000:8000"`) eksponuje serwisy na hoÅ›cie dla dostÄ™pu z lokalnej maszyny, ale internal communication uÅ¼ywa Docker network bez port mappings.

---

## Local Development

### Quick Start

Uruchomienie peÅ‚nego Å›rodowiska deweloperskiego wymaga jedynie Docker Compose. Pierwsze co trzeba zrobiÄ‡ to skonfigurowaÄ‡ zmienne Å›rodowiskowe. Kopiujemy `.env.example` do `.env` i wypeÅ‚niamy wymagane wartoÅ›ci: `GOOGLE_API_KEY` (Gemini API key z Google AI Studio), `SECRET_KEY` (32-char random hex wygenerowany przez `openssl rand -hex 32`), oraz credentials do PostgreSQL, Redis i Neo4j (defaults w docker-compose sÄ… OK dla developmentu, ale production wymaga silniejszych haseÅ‚).

**Komendy startowe:**

```bash
# 1. Konfiguracja Å›rodowiska
cp .env.example .env
# Edytuj .env - ustaw GOOGLE_API_KEY i SECRET_KEY

# 2. Uruchom wszystkie serwisy
docker-compose up -d

# 3. SprawdÅº logi startup (opcjonalnie)
docker-compose logs -f

# 4. Weryfikuj Å¼e wszystko dziaÅ‚a
curl http://localhost:8000/health
# {"status": "healthy", "postgres": "connected", "redis": "connected", "neo4j": "connected"}
```

Po pierwszym uruchomieniu migrate i neo4j-init service'y wykonajÄ… siÄ™ automatycznie dziÄ™ki dependency chain w docker-compose. Migrate uruchamia `alembic upgrade head` aby zastosowaÄ‡ wszystkie migracje bazy danych. Neo4j-init uruchamia `scripts/init_neo4j_indexes.py` ktÃ³ry tworzy trzy kluczowe indeksy: `document_id_idx` (B-tree na Document.id), `chunk_embedding_idx` (property index na Chunk.embedding), oraz `chunk_vector_idx` (vector index z 768 wymiarami dla Gemini embeddings). Te indeksy sÄ… wymagane dla systemu RAG - bez nich hybrid search failuje z `VectorIndexNotFoundError`.

### DostÄ™pne Endpointy

Aplikacja po starcie jest dostÄ™pna pod kilkoma endpointami. Backend API odpowiada na `http://localhost:8000`, z interaktywnÄ… dokumentacjÄ… Swagger UI pod `/docs` i ReDoc pod `/redoc`. Frontend dev server dziaÅ‚a na `http://localhost:5173` z hot reload - kaÅ¼da zmiana w plikach `.tsx` lub `.ts` automatycznie rebuilds i refreshuje przeglÄ…darkÄ™. Neo4j Browser, uÅ¼yteczny do debugowania grafÃ³w, znajduje siÄ™ pod `http://localhost:7474` (credentials: neo4j/dev_password_change_in_prod). PostgreSQL nasÅ‚uchuje na porcie 5433 (nie standardowy 5432, aby uniknÄ…Ä‡ konfliktÃ³w z lokalnymi instalacjami), a Redis na standardowym 6379.

**Przydatne endpointy API:**

- `GET /health` - Health check all dependencies
- `GET /startup` - Detailed startup probe (z latencjami DB connections)
- `GET /docs` - Swagger UI (interactive API docs)
- `GET /metrics` - Prometheus metrics (opcjonalny, wymaga wÅ‚Ä…czenia)
- `POST /api/v1/projects/{id}/personas/generate` - Generuj 20 person
- `POST /api/v1/focus-groups` - UtwÃ³rz grupÄ™ fokusowÄ…

### Hot Reload i Rebuilds

System jest zoptymalizowany dla developer experience. Zmiany w kodzie Python (backend) lub TypeScript/React (frontend) sÄ… natychmiast widoczne dziÄ™ki volume mounts i hot reload - nie wymaga to rebuildu kontenerÃ³w. Backend uÅ¼ywa uvicorn z flagÄ… `--reload` ktÃ³ra Å›ledzi zmiany w `app/` i automatycznie restartuje worker processes. Frontend uÅ¼ywa Vite dev server z ultra-fast HMR (Hot Module Replacement) ktÃ³ry preserves application state podczas reload.

Jedynie zmiany w `requirements.txt` lub `package.json` wymagajÄ… przebudowania odpowiedniego serwisu. Dla backendu: `docker-compose up --build -d api`, dla frontendu: `docker-compose up --build -d frontend`. Rebuild zajmuje okoÅ‚o 30-60 sekund dziÄ™ki Docker layer caching - tylko zmienione layers sÄ… rebuilowane, reszta jest reused z cache.

**Migracje bazy danych:**

Po zmianach w modelach ORM (`app/models/`) trzeba wygenerowaÄ‡ i zastosowaÄ‡ migracjÄ™ Alembic. Proces jest pÃ³Å‚automatyczny - Alembic wykrywa zmiany w SQLAlchemy models i generuje odpowiednie DDL SQL, ale naleÅ¼y zawsze przejrzeÄ‡ wygenerowanÄ… migracjÄ™ przed zastosowaniem.

```bash
# 1. Auto-generuj migracjÄ™ ze zmian w modelach
docker-compose exec api alembic revision --autogenerate -m "Add kpi_snapshot field to personas"

# 2. Przejrzyj wygenerowanÄ… migracjÄ™
cat alembic/versions/XXXX_add_kpi_snapshot_field_to_personas.py

# 3. Zastosuj migracjÄ™
docker-compose exec api alembic upgrade head

# 4. Weryfikuj schema
docker-compose exec postgres psql -U sight -d sight_db -c "\d personas"
```

**WAÅ»NE:** Alembic moÅ¼e pominÄ…Ä‡ niektÃ³re zmiany - szczegÃ³lnie indeksy, custom SQL operations, data migrations. Zawsze reviewuj wygenerowanÄ… migracjÄ™ i dodaj brakujÄ…ce operacje manualnie.

### Debugging

Gdy coÅ› nie dziaÅ‚a, kluczowe sÄ… logi. Komenda `docker-compose logs -f api` streamuje logi backendu w real-time, co pozwala Å›ledziÄ‡ requesty HTTP, bÅ‚Ä™dy Python, oraz wywoÅ‚ania LLM. Dla frontendu analogicznie `docker-compose logs -f frontend` pokazuje output Vite dev server. Dla wszystkich serwisÃ³w jednoczeÅ›nie: `docker-compose logs -f` (moÅ¼e byÄ‡ overwhelming).

**Filtrowanie logÃ³w:**

```bash
# Tylko bÅ‚Ä™dy (ERROR level)
docker-compose logs api | grep ERROR

# Ostatnie 100 linii
docker-compose logs --tail=100 api

# Konkretny timestamp range (wymaga jq)
docker-compose logs --timestamps api | grep "2025-11-03"

# Follow logs z konkretnego serwisu
docker-compose logs -f postgres redis neo4j
```

W przypadku powaÅ¼niejszych problemÃ³w moÅ¼na wejÅ›Ä‡ do wnÄ™trza kontenera przez `docker exec -it sight_api bash`. Pozwala to na inspekcjÄ™ plikÃ³w, uruchomienie shell Python (`python -c "from app.core.config import get_settings; print(get_settings())"` - wait, to juÅ¼ nie dziaÅ‚a, teraz uÅ¼ywamy `from config import app`), czy sprawdzenie zmiennych Å›rodowiskowych (`env | grep DATABASE`). Komenda `docker stats` wyÅ›wietla real-time uÅ¼ycie CPU, RAM i network dla wszystkich kontenerÃ³w - przydatne przy debugowaniu performance issues.

**Typowe problemy local development:**

**Problem 1: Port already in use**
Symptom: `Error starting userland proxy: listen tcp4 0.0.0.0:8000: bind: address already in use`
Przyczyna: Inny proces (lokalna instalacja FastAPI, Jupyter notebook) uÅ¼ywa portu 8000
RozwiÄ…zanie: `lsof -ti:8000 | xargs kill -9` lub zmieÅ„ port w docker-compose.yml

**Problem 2: PostgreSQL connection refused**
Symptom: `OperationalError: could not connect to server: Connection refused`
Przyczyna: Postgres container nie wystartowaÅ‚ (failed health check) lub DATABASE_URL ma zÅ‚e credentials
RozwiÄ…zanie: `docker-compose logs postgres` sprawdÅº logi, weryfikuj DATABASE_URL w .env

**Problem 3: Neo4j timeout**
Symptom: `ServiceUnavailable: Failed to establish connection`
Przyczyna: Neo4j jest wolny do startu (2-3 minuty przy pierwszym uruchomieniu)
RozwiÄ…zanie: Poczekaj 2-3 minuty, sprawdÅº `docker-compose logs neo4j` czy wystartowaÅ‚

**Problem 4: Frontend hot reload nie dziaÅ‚a**
Symptom: Zmiany w .tsx nie sÄ… widoczne w przeglÄ…darce
Przyczyna: Volume mount nie dziaÅ‚a (Windows WSL2 issue) lub Vite cache corruption
RozwiÄ…zanie: `docker-compose restart frontend` lub `docker-compose exec frontend rm -rf node_modules/.vite`

**Problem 5: Out of memory**
Symptom: Containers crashujÄ… z exit code 137 (OOM killed)
Przyczyna: Docker Desktop ma za maÅ‚o RAM alokowanego (default 2GB)
RozwiÄ…zanie: Docker Desktop â†’ Settings â†’ Resources â†’ Memory: zwiÄ™ksz do minimum 8GB

### Database Management

**PostgreSQL:**

```bash
# Connect do bazy
docker-compose exec postgres psql -U sight -d sight_db

# SprawdÅº tabele
\dt

# SprawdÅº schema konkretnej tabeli
\d personas

# Wykonaj query
SELECT COUNT(*) FROM personas WHERE is_active = true;

# Export danych do CSV
\copy (SELECT * FROM personas) TO '/tmp/personas.csv' CSV HEADER

# Backup caÅ‚ej bazy
docker-compose exec postgres pg_dump -U sight sight_db > backup_$(date +%Y%m%d).sql

# Restore z backupu
cat backup_20251103.sql | docker-compose exec -T postgres psql -U sight -d sight_db
```

**Redis:**

```bash
# Connect do Redis
docker-compose exec redis redis-cli

# SprawdÅº wszystkie keys (DEV ONLY - nie uÅ¼ywaj w produkcji!)
KEYS *

# SprawdÅº konkretny key
GET segment_brief:25-34:wyÅ¼sze:warszawa:kobieta

# SprawdÅº TTL (time to live)
TTL segment_brief:25-34:wyÅ¼sze:warszawa:kobieta

# Flush cache (DEV ONLY)
FLUSHALL

# SprawdÅº memory usage
INFO memory
```

**Neo4j:**

OtwÃ³rz Neo4j Browser w przeglÄ…darce: `http://localhost:7474`
Credentials: `neo4j / dev_password_change_in_prod`

```cypher
// SprawdÅº ile jest dokumentÃ³w RAG
MATCH (d:Document) RETURN count(d)

// SprawdÅº ile jest chunkÃ³w
MATCH (c:RAGChunk) RETURN count(c)

// SprawdÅº graph nodes (WskaÅºnik, Obserwacja, Trend, Demografia)
MATCH (n) WHERE n:Wskaznik OR n:Obserwacja OR n:Trend OR n:Demografia
RETURN labels(n) as type, count(n) as count

// SprawdÅº przykÅ‚adowy graph node
MATCH (w:Wskaznik) RETURN w LIMIT 5

// SprawdÅº vector index
CALL db.indexes()

// Test vector search
CALL db.index.vector.queryNodes('chunk_vector_idx', 10, [0.1, 0.2, ...])
```

---

## Cloud Run Production

### Architektura Single Service

W przeciwieÅ„stwie do tradycyjnego podejÅ›cia z osobnymi serwisami dla frontendu i backendu, Sight deployuje siÄ™ jako jedna usÅ‚uga Cloud Run. Dockerfile.cloudrun wykorzystuje multi-stage build: najpierw buduje frontend React z Vite (generujÄ…c statyczne pliki w `/dist`), nastÄ™pnie instaluje Python dependencies dla backendu, a finalny stage Å‚Ä…czy oba - FastAPI serwuje zarÃ³wno API endpoints (`/api/v1/*`) jak i statyczne pliki frontendu (`/`, `/assets/*`).

To rozwiÄ…zanie ma kilka zalet. Po pierwsze, **prostota** - jedna usÅ‚uga Cloud Run zamiast dwÃ³ch oznacza mniej konfiguracji, mniej secrets do zarzÄ…dzania, i niÅ¼sze koszty (jedna instancja zamiast dwÃ³ch). Po drugie, **brak CORS issues** - frontend i backend sÄ… pod tym samym origin, wiÄ™c nie ma potrzeby konfiguracji CORS headers ani preflight OPTIONS requests. Po trzecie, **Å‚atwiejsze routing** - Cloud Run Load Balancer kieruje caÅ‚y traffic do jednego serwisu, a FastAPI internal router dystrybuuje requesty do API vs static files.

**app/main.py - Static Files Mounting:**

```python
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles

app = FastAPI()

# API routes
app.include_router(api_router, prefix="/api/v1")

# Static files (frontend build) - MUST be after API routes
app.mount("/", StaticFiles(directory="static", html=True), name="static")
```

KolejnoÅ›Ä‡ jest krytyczna - API routes muszÄ… byÄ‡ zarejestrowane PRZED static files mount, inaczej wszystkie requesty trafiÅ‚yby do static files handler i API nie dziaÅ‚aÅ‚oby. StaticFiles z `html=True` automatycznie serwuje `index.html` dla wszystkich Å›cieÅ¼ek ktÃ³re nie pasujÄ… do API routes, co zapewnia poprawne dziaÅ‚anie React Router (client-side routing).

### Google Cloud Platform Setup

Deployment wymaga najpierw skonfigurowania GCP. Projekt `gen-lang-client-0508446677` ma wÅ‚Ä…czone piÄ™Ä‡ kluczowych API: Cloud Run (uruchamianie kontenerÃ³w), Cloud Build (CI/CD pipeline), Artifact Registry (storage dla Docker images), Secret Manager (bezpieczne przechowywanie credentials), oraz Cloud SQL Admin (zarzÄ…dzanie bazÄ… PostgreSQL).

**WÅ‚Ä…czanie API (jednorazowe):**

```bash
# Zaloguj siÄ™ do GCP
gcloud auth login

# Ustaw projekt
gcloud config set project gen-lang-client-0508446677

# WÅ‚Ä…cz wymagane API
gcloud services enable run.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable secretmanager.googleapis.com
gcloud services enable sqladmin.googleapis.com
```

Cloud SQL instancja `sight` zostaÅ‚a utworzona w regionie `europe-central2` (Warsaw) na tier `db-f1-micro` (0.6GB RAM, shared CPU) z 10GB SSD storage. To wystarczajÄ…ce dla maÅ‚ych i Å›rednich obciÄ…Å¼eÅ„ - instancja kosztuje okoÅ‚o 10-15 USD miesiÄ™cznie. Backup automatyczny wykonuje siÄ™ codziennie o 3:00 AM, z retencjÄ… 7 dni. Maintenance window ustawiony jest na niedziele o 4:00 AM, minimalizujÄ…c wpÅ‚yw na uÅ¼ytkownikÃ³w.

**Tworzenie Cloud SQL instancji (jednorazowe):**

```bash
# UtwÃ³rz Cloud SQL instancjÄ™
gcloud sql instances create sight \
    --database-version=POSTGRES_15 \
    --tier=db-f1-micro \
    --region=europe-central2 \
    --storage-type=SSD \
    --storage-size=10GB \
    --backup-start-time=03:00 \
    --maintenance-window-day=SUN \
    --maintenance-window-hour=4

# UtwÃ³rz bazÄ™ danych
gcloud sql databases create sight_db --instance=sight

# UtwÃ³rz uÅ¼ytkownika
gcloud sql users create sight --instance=sight --password=STRONG_PASSWORD_HERE

# SprawdÅº connection name (potrzebny dla DATABASE_URL_CLOUD)
gcloud sql instances describe sight --format="value(connectionName)"
# Output: gen-lang-client-0508446677:europe-central2:sight
```

**Artifact Registry:**

```bash
# UtwÃ³rz repository dla Docker images
gcloud artifacts repositories create sight-containers \
    --repository-format=docker \
    --location=europe-central2 \
    --description="Docker images for Sight platform"

# Skonfiguruj Docker authentication
gcloud auth configure-docker europe-central2-docker.pkg.dev
```

---

## External Services

### Neo4j AuraDB

OprÃ³cz Cloud SQL, aplikacja integruje siÄ™ z dwoma managed services zewnÄ™trznymi. Neo4j AuraDB Free tier (50,000 nodes, 0 USD/miesiÄ…c) hostuje graf dla systemu RAG. Instancja znajduje siÄ™ w regionie `europe-west1` (Belgium), co daje latencjÄ™ okoÅ‚o 20ms z Cloud Run w Warsaw. Connection string ma format `neo4j+s://xxxxx.databases.neo4j.io` - protokÃ³Å‚ `neo4j+s` jest wymagany dla AuraDB (nie `bolt://` jak w lokalnym Neo4j).

**Setup AuraDB (jednorazowe):**

1. Zarejestruj siÄ™ na https://neo4j.com/cloud/aura/
2. UtwÃ³rz instancjÄ™ Free tier w regionie Europe West 1
3. Zapisz connection URI i hasÅ‚o (pokazane tylko raz!)
4. Skonfiguruj IP allowlist - dodaj `0.0.0.0/0` dla Cloud Run (dynamic IPs)
5. Dodaj credentials do Secret Manager (NEO4J_URI, NEO4J_PASSWORD)

**WaÅ¼ne:** AuraDB uÅ¼ywa `neo4j+s://` (secure WebSocket) zamiast `bolt://` (binary protocol). Aplikacja automatycznie detektuje protokÃ³Å‚ i uÅ¼ywa odpowiedniego drivera.

### Upstash Redis

Upstash Redis w Free tier (10,000 requests/day) peÅ‚ni rolÄ™ cache'a dla segment briefs i KPI snapshots. Region rÃ³wnieÅ¼ `europe-west1` dla niskiej latencji. Connection string: `redis://default:PASSWORD@region.upstash.io:PORT`. Upstash automatycznie evictuje najmniej uÅ¼ywane keys gdy limit jest bliski przekroczenia (LRU eviction policy).

**Setup Upstash (jednorazowe):**

1. Zarejestruj siÄ™ na https://upstash.com/
2. UtwÃ³rz Redis database w regionie Europe West 1
3. Wybierz Free tier (10k requests/day, 256MB storage)
4. Skopiuj REST URL i konwertuj na Redis URL format
5. Dodaj REDIS_URL do Secret Manager

**Cache strategy:**

- Segment briefs: TTL 7 dni (604800s)
- Graph RAG context: TTL 7 dni
- KPI snapshots: TTL 5 minut (300s)
- Expected hit rate: 70-90% dla segment briefs, 80-95% dla graph context

### Secrets Management

Wszystkie wraÅ¼liwe dane (API keys, passwords, connection strings) sÄ… przechowywane w GCP Secret Manager, nie w zmiennych Å›rodowiskowych czy plikach .env. Mamy siedem secrets:

- `GOOGLE_API_KEY` - Gemini API key dla LLM operations
- `NEO4J_URI` i `NEO4J_PASSWORD` - credentials do AuraDB
- `REDIS_URL` - peÅ‚ny connection string do Upstash
- `DATABASE_URL_CLOUD` - PostgreSQL connection string przez Unix socket
- `POSTGRES_PASSWORD` - hasÅ‚o uÅ¼ytkownika postgres
- `SECRET_KEY` - FastAPI session signing key (32-char random hex)

**Tworzenie secrets (jednorazowe):**

```bash
# 1. Wygeneruj SECRET_KEY
openssl rand -hex 32

# 2. UtwÃ³rz secrets
echo -n "YOUR_GEMINI_API_KEY" | gcloud secrets create GOOGLE_API_KEY --data-file=-
echo -n "neo4j+s://xxxxx.databases.neo4j.io" | gcloud secrets create NEO4J_URI --data-file=-
echo -n "YOUR_NEO4J_PASSWORD" | gcloud secrets create NEO4J_PASSWORD --data-file=-
echo -n "redis://default:PASSWORD@region.upstash.io:PORT" | gcloud secrets create REDIS_URL --data-file=-
echo -n "STRONG_POSTGRES_PASSWORD" | gcloud secrets create POSTGRES_PASSWORD --data-file=-
echo -n "$(openssl rand -hex 32)" | gcloud secrets create SECRET_KEY --data-file=-

# 3. Zbuduj DATABASE_URL_CLOUD (Unix socket dla Cloud SQL)
echo -n "postgresql+asyncpg://sight:POSTGRES_PASSWORD@/sight_db?host=/cloudsql/gen-lang-client-0508446677:europe-central2:sight" | gcloud secrets create DATABASE_URL_CLOUD --data-file=-

# 4. Nadaj uprawnienia Cloud Run service account
PROJECT_NUMBER=$(gcloud projects describe gen-lang-client-0508446677 --format="value(projectNumber)")
SERVICE_ACCOUNT="${PROJECT_NUMBER}-compute@developer.gserviceaccount.com"

for SECRET in GOOGLE_API_KEY NEO4J_URI NEO4J_PASSWORD REDIS_URL DATABASE_URL_CLOUD POSTGRES_PASSWORD SECRET_KEY; do
    gcloud secrets add-iam-policy-binding $SECRET \
        --member="serviceAccount:${SERVICE_ACCOUNT}" \
        --role="roles/secretmanager.secretAccessor"
done
```

Secrets sÄ… automatycznie montowane do Cloud Run przez parametr `--set-secrets` w deploy command. Cloud Run service account ma rolÄ™ `roles/secretmanager.secretAccessor` dla kaÅ¼dego secretu. WartoÅ›ci sÄ… dostÄ™pne w kontenerze jako zmienne Å›rodowiskowe, ale nigdy nie sÄ… wyÅ›wietlane w logach czy Cloud Console UI.

**Aktualizacja secrets:**

```bash
# UtwÃ³rz nowÄ… wersjÄ™ secretu
echo -n "NEW_VALUE" | gcloud secrets versions add SECRET_NAME --data-file=-

# Cloud Run automatycznie uÅ¼yje latest version przy nastÄ™pnym deploy
# MoÅ¼na teÅ¼ wymusiÄ‡ nowÄ… rewizjÄ™ bez deploy:
gcloud run services update sight --region=europe-central2
```

---

## CI/CD Pipeline

### Overview

PeÅ‚ny deployment pipeline jest zdefiniowany w `cloudbuild.yaml` i skÅ‚ada siÄ™ z siedmiu sekwencyjnych krokÃ³w: pull cache, Docker build, push to registry, database migrations, Cloud Run deploy, Neo4j initialization, oraz smoke tests. Pipeline uruchamia siÄ™ automatycznie przy kaÅ¼dym push do branch `main` przez Cloud Build trigger podpiÄ™ty do GitHub repo `JakWdo/Symulacja`.

CaÅ‚kowity czas wykonania wynosi **8-12 minut** dla incremental builds (z cache'owaniem Docker layers), lub **20-25 minut** dla first build bez cache. Code-only changes (bez zmian w dependencies) kompletujÄ… w **5-8 minut** dziÄ™ki aggressive layer caching. Pipeline uÅ¼ywa explicit `--cache-from` oraz BuildKit inline cache dla maximum cache hit rate.

**Optimizations applied (October 2024):**

- BuildKit inline cache: zapisuje cache metadata wewnÄ…trz image layers
- Pinned base images: `node:20.18.0`, `python:3.11.11` (prevents cache invalidation)
- Multi-stage caching: kaÅ¼dy stage (frontend-builder, backend-builder, runtime) jest cache'owany osobno
- Machine type E2_HIGHCPU_8: szybszy build (8 vCPU vs 1 vCPU default)
- Parallel quality checks: usuniÄ™te z cloudbuild.yaml (przenoszone do pre-commit hooks lokalnie)

### Step 1: Pull Cache

Pipeline zaczyna siÄ™ od pobrania poprzedniego image z Artifact Registry aby uÅ¼yÄ‡ go jako cache source. UÅ¼ywamy `entrypoint: bash` z `|| true` aby uniknÄ…Ä‡ failowania na first build (no cache exists yet). Docker automatycznie uÅ¼yje downloaded image jako cache source w nastÄ™pnym kroku.

```bash
docker pull europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:latest || echo "No cache image found - first build will be slow"
```

### Step 2: Build & Push

Build Docker image z `Dockerfile.cloudrun` uÅ¼ywajÄ…c BuildKit dla lepszego cachingu. Multi-stage build zajmuje 3-5 minut dla code-only changes lub 15-20 minut dla zmian w dependencies. Image jest tagowany dwoma tagami: `latest` (zawsze wskazuje na najnowszy build) oraz `$COMMIT_SHA` (konkretny git commit dla rollback).

```bash
export DOCKER_BUILDKIT=1

docker build \
  -f Dockerfile.cloudrun \
  --cache-from europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:latest \
  --build-arg BUILDKIT_INLINE_CACHE=1 \
  -t europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:latest \
  -t europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:$COMMIT_SHA \
  .

docker push --all-tags europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight
```

Push do Artifact Registry nastÄ™puje natychmiast po build. Registry automatycznie skanuje image pod kÄ…tem CVEs i wyÅ›wietla wyniki w Cloud Console. Critical CVEs powinny byÄ‡ naprawione przed deploy do produkcji.

### Step 3: Database Migrations (CRITICAL)

**NajwaÅ¼niejszy krok caÅ‚ego pipeline.** Przed wdroÅ¼eniem nowego kodu aplikacji, schema bazy danych musi byÄ‡ up-to-date. Cloud Run Job `db-migrate` uruchamia komendÄ™ `alembic upgrade head` wewnÄ…trz tego samego Docker image ktÃ³ry bÄ™dzie deployowany.

Job ma dostÄ™p do Cloud SQL przez Unix socket (`--add-cloudsql-instances`) i uÅ¼ywa `DATABASE_URL_CLOUD` secret. JeÅ›li migracja failuje (np. syntax error w migration script, constraint violation), build siÄ™ przerywa. To zapobiega deployment broken code ktÃ³ry nie moÅ¼e siÄ™ poÅ‚Ä…czyÄ‡ z bazÄ….

```bash
# Create or update migration job
gcloud run jobs create db-migrate \
  --image=europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:latest \
  --region=europe-central2 \
  --add-cloudsql-instances=gen-lang-client-0508446677:europe-central2:sight \
  --set-secrets=DATABASE_URL=DATABASE_URL_CLOUD:latest \
  --command=alembic,upgrade,head \
  --max-retries=2 \
  --task-timeout=300

# Execute migrations
gcloud run jobs execute db-migrate --region=europe-central2 --wait

# Check exit code
if [ $? -eq 0 ]; then
  echo "âœ… Migrations completed successfully"
else
  echo "âŒ Migrations failed - aborting deployment"
  exit 1
fi
```

Migracje sÄ… wykonywane jako Cloud Run Job (nie exec w dziaÅ‚ajÄ…cym kontenerze) z dwÃ³ch powodÃ³w. Po pierwsze, **isolation** - job dziaÅ‚a w czystym environment z maksymalnie 300s timeout, bez ryzyka interference z running application. Po drugie, **retry logic** - job automatycznie retry-uje do 2 razy przy transient failures (network blips, Cloud SQL restarts).

### Step 4: Cloud Run Deploy

Deployment do Cloud Run uÅ¼ywa `gcloud run deploy` z parametrami zoptymalizowanymi dla FastAPI + LLM workload:

```bash
gcloud run deploy sight \
  --image=europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:latest \
  --region=europe-central2 \
  --platform=managed \
  --allow-unauthenticated \
  --port=8080 \
  --memory=4Gi \
  --cpu=2 \
  --cpu-boost \
  --timeout=300 \
  --min-instances=0 \
  --max-instances=5 \
  --execution-environment=gen2 \
  --add-cloudsql-instances=gen-lang-client-0508446677:europe-central2:sight \
  --set-secrets=DATABASE_URL=DATABASE_URL_CLOUD:latest,GOOGLE_API_KEY=GOOGLE_API_KEY:latest,NEO4J_PASSWORD=NEO4J_PASSWORD:latest,NEO4J_URI=NEO4J_URI:latest,POSTGRES_PASSWORD=POSTGRES_PASSWORD:latest,REDIS_URL=REDIS_URL:latest,SECRET_KEY=SECRET_KEY:latest \
  --set-env-vars=NEO4J_USER=neo4j,ENVIRONMENT=production,DEBUG=False,DEFAULT_LLM_PROVIDER=google,DEFAULT_MODEL=gemini-2.5-flash,EMBEDDING_MODEL=models/gemini-embedding-001,RAG_ENABLED=True,ORCHESTRATION_ENABLED=True
```

**Kluczowe parametry:**

- `--memory=4Gi` - wystarczajÄ…ce dla FastAPI + Redis client + Neo4j driver + sentence-transformers (~2.5GB peak)
- `--cpu=2` - dwa vCPU pozwalajÄ… na parallel processing LLM requests
- `--cpu-boost` - temporary CPU boost podczas startu kontenera (cold start optimization, reduces cold start z 10s â†’ 5s)
- `--timeout=300` - 5 minut timeout dla dÅ‚ugich LLM operations (focus groups z 20 person Ã— 4 pytania ~2 min)
- `--min-instances=0` - scale to zero gdy brak traffic (cost optimization)
- `--max-instances=5` - auto-scale do 5 instancji przy heavy load (prevents runaway costs)
- `--execution-environment=gen2` - nowszy runtime z lepszÄ… performance i security

Secrets sÄ… montowane jako environment variables (`--set-secrets=DATABASE_URL=DATABASE_URL_CLOUD:latest,...`). Cloud Run automatycznie pobiera najnowsze wersje secrets - nie trzeba manualnie aktualizowaÄ‡ deployment po zmianie secretu.

Deployment zajmuje 1-2 minuty. Cloud Run czeka aÅ¼ nowa rewizja przejdzie health check (`/health` endpoint musi zwrÃ³ciÄ‡ 200 OK przez 10 sekund), dopiero wtedy kieruje traffic. JeÅ›li health check failuje przez 4 minuty, deployment jest rollbackowany automatycznie do poprzedniej rewizji.

### Step 5: Neo4j Initialization

Po deployment aplikacji, osobny Cloud Run Job `neo4j-init` uruchamia `python scripts/init_neo4j_cloudrun.py`. Skrypt tworzy trzy kluczowe indeksy w Neo4j:

- `document_id_idx` - B-tree index na `Document.id` dla szybkich lookupÃ³w
- `chunk_embedding_idx` - property index na `Chunk.embedding` (metadata)
- `chunk_vector_idx` - **vector index** na `Chunk.embedding` z 768 wymiarami (Gemini embeddings), cosine similarity

Vector index jest wymagany dla hybrid search RAG. Bez niego queries `db.index.vector.queryNodes()` failujÄ… z `VectorIndexNotFoundError`. Tworzenie indeksu zajmuje 30-60 sekund dla typowej bazy z okoÅ‚o 10,000 chunks.

```bash
# Create or update Neo4j init job
gcloud run jobs create neo4j-init \
  --image=europe-central2-docker.pkg.dev/gen-lang-client-0508446677/sight-containers/sight:latest \
  --region=europe-central2 \
  --set-secrets=NEO4J_URI=NEO4J_URI:latest,NEO4J_PASSWORD=NEO4J_PASSWORD:latest \
  --set-env-vars=NEO4J_USER=neo4j \
  --command=python,scripts/init_neo4j_cloudrun.py \
  --max-retries=3 \
  --task-timeout=300

# Execute initialization
gcloud run jobs execute neo4j-init --region=europe-central2 --wait

# Check status (non-fatal)
if [ $? -eq 0 ]; then
  echo "âœ… Neo4j indexes initialized successfully"
else
  echo "âš ï¸ Neo4j init failed - RAG features may be limited"
fi
```

Ten krok jest **non-blocking** - jeÅ›li failuje (np. Neo4j timeout, network issues), build siÄ™ nie przerywa. Aplikacja dziaÅ‚a normalnie, ale RAG features sÄ… limited dopÃ³ki indeksy nie zostanÄ… utworzone. Background task w FastAPI retry-uje poÅ‚Ä…czenie z Neo4j co 5 minut.

### Step 6: Smoke Tests

Ostatni krok wykonuje cztery smoke tests na fresh deployowanej aplikacji:

1. **Health check** - `GET /health` musi zwrÃ³ciÄ‡ 200 OK z JSON payload `{"status": "healthy"}`
2. **Startup probe** - `GET /startup` weryfikuje poÅ‚Ä…czenia do PostgreSQL, Redis, Neo4j
3. **API docs** - `GET /docs` sprawdza czy Swagger UI jest dostÄ™pne
4. **Frontend** - `GET /` zwraca React SPA (nie 404)

```bash
# Get deployed service URL
SERVICE_URL=$(gcloud run services describe sight --region=europe-central2 --format="value(status.url)")

# Test 1: Health check
HEALTH_STATUS=$(curl -f -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/health")
if [ "$HEALTH_STATUS" != "200" ]; then
  echo "âŒ Health check FAILED"
  exit 1
fi

# Test 2: Frontend
FRONTEND_STATUS=$(curl -f -s -o /dev/null -w "%{http_code}" "$SERVICE_URL/")
if [ "$FRONTEND_STATUS" != "200" ]; then
  echo "âŒ Frontend FAILED"
  exit 1
fi

echo "ğŸ‰ Smoke tests PASSED!"
```

JeÅ›li ktÃ³rykolwiek test failuje, build jest oznaczony jako failed. To **blocking step** - informuje zespÃ³Å‚ Å¼e deployment siÄ™ nie powiÃ³dÅ‚ mimo Å¼e Cloud Run deploy sukceeded. W przyszÅ‚oÅ›ci planujemy automatic rollback do poprzedniej rewizji przy failed smoke tests.

### Monitoring Builds

Logi z kaÅ¼dego build sÄ… dostÄ™pne w Cloud Console lub przez CLI:

```bash
# Lista ostatnich buildÃ³w
gcloud builds list --limit=5

# Stream logs konkretnego build
gcloud builds log BUILD_ID --stream

# SzczegÃ³Å‚y buildu (JSON)
gcloud builds describe BUILD_ID --format=json

# Status poszczegÃ³lnych krokÃ³w
gcloud builds describe BUILD_ID --format="json" | jq '.steps[] | {id, status, timing}'
```

KaÅ¼dy krok pipeline ma assigned ID (np. `pull-cache`, `build`, `deploy`). MoÅ¼na sprawdziÄ‡ ktÃ³ry krok failowaÅ‚ i ile czasu zajÄ…Å‚. To jest przydatne do debugowania slow builds czy identyfikacji bottlenecks w pipeline.

**Setup Cloud Build trigger (jednorazowe):**

```bash
# UtwÃ³rz trigger na push do main branch
gcloud builds triggers create github \
  --name="sight-deploy-main" \
  --repo-name="Symulacja" \
  --repo-owner="JakWdo" \
  --branch-pattern="^main$" \
  --build-config="cloudbuild.yaml"
```

---

## Monitoring & Observability

### Cloud Logging

Wszystkie logi z Cloud Run sÄ… automatycznie przekazywane do Cloud Logging. Query language pozwala na precyzyjne filtrowanie.

**PrzykÅ‚ady queries:**

```bash
# Tylko bÅ‚Ä™dy z ostatnich 50 wpisÃ³w
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=sight AND severity>=ERROR" --limit=50

# LLM operations z uÅ¼yciem tokenÃ³w
gcloud logging read "resource.type=cloud_run_revision AND jsonPayload.operation=persona_generation" --limit=20 --format=json

# Slow queries (latency > 1s)
gcloud logging read "resource.type=cloud_run_revision AND jsonPayload.latency_ms>1000" --limit=20

# Requests z konkretnego user_id
gcloud logging read "resource.type=cloud_run_revision AND jsonPayload.user_id=USER_UUID" --limit=50
```

Logi zawierajÄ… structured fields: timestamp, severity (DEBUG/INFO/WARNING/ERROR/CRITICAL), textPayload (message), httpRequest (dla HTTP logs), oraz labels (custom metadata). FastAPI automatycznie loguje kaÅ¼dy request z method, path, status code i latency.

**Structured logging w aplikacji:**

```python
import logging

logger = logging.getLogger(__name__)

# Log LLM call z kontekstem
logger.info(
    "LLM generation completed",
    extra={
        "operation": "persona_generation",
        "model": "gemini-2.5-flash",
        "input_tokens": 1234,
        "output_tokens": 567,
        "latency_ms": 2800,
        "cost_usd": 0.00026,
        "user_id": str(user_id),
        "project_id": str(project_id)
    }
)
```

### Key Metrics to Track

**LLM Performance:**
- Tokens per operation (input/output)
- Cost per operation ($USD)
- Latency (p50, p90, p95, p99)
- Error rate (% failed calls)
- Retry rate (% calls requiring retry)

**RAG Performance:**
- Cache hit rate (hybrid search, graph RAG)
- Retrieval latency (vector, keyword, graph)
- Context size (chars, tokens)
- Relevance score (user feedback)

**Quality Metrics:**
- Persona quality score (0-100)
- Demographic accuracy (chi-square p-value)
- Consistency score (% personas passing checks)
- Hallucination rate (% outputs with facts not in RAG)

**Infrastructure Metrics:**
- API latency (p50, p90, p95, p99)
- Database query time (p95)
- Memory usage (MB, % of limit)
- CPU usage (%, throttling events)
- Active instances count
- Cold start frequency
- Error rate (HTTP 5xx)

### Cloud Monitoring Dashboard

Target metrics dla produkcji:

- **API latency** - P95 < 500ms dla prostych endpoints, < 3s dla LLM-powered
- **Persona generation** - 20 person < 60s (obecnie okoÅ‚o 45s dziÄ™ki parallel processing)
- **Focus group** - 20 person Ã— 4 pytania < 3 minuty (obecnie okoÅ‚o 2 min)
- **Cold start** - < 10s dla pierwszego request po scale-to-zero (dziÄ™ki `--cpu-boost`)
- **Memory usage** - < 3GB sustained (4GB limit daje buffer dla peaks)
- **Database query** - P95 < 100ms (obecnie okoÅ‚o 65ms)
- **Hybrid search** - P95 < 350ms (obecnie okoÅ‚o 280ms)

Metrics sÄ… monitorowane przez Cloud Monitoring. Dashboard pokazuje request count, latency percentiles (P50/P90/P95/P99), error rate, CPU/memory utilization, oraz active instances count.

**Setup alerting policies:**

```bash
# Alert na error rate > 1%
gcloud alpha monitoring policies create \
  --notification-channels=CHANNEL_ID \
  --display-name="High Error Rate" \
  --condition-display-name="Error rate > 1%" \
  --condition-threshold-value=0.01 \
  --condition-threshold-duration=300s

# Alert na latency P95 > 5s
gcloud alpha monitoring policies create \
  --notification-channels=CHANNEL_ID \
  --display-name="High Latency" \
  --condition-display-name="P95 latency > 5s" \
  --condition-threshold-value=5000 \
  --condition-threshold-duration=300s
```

### Common Production Issues

**Problem 1: Service timeout podczas startu**
Symptom: Cloud Run deployment sukceeded ale /health zwraca 504 Gateway Timeout
Przyczyna: Aplikacja startuje >4 minuty (Cloud Run limit)
RozwiÄ…zanie: ZwiÄ™kszyÄ‡ `--timeout` lub zoptymalizowaÄ‡ startup (lazy initialization zamiast eager)

**Problem 2: Database connection failed**
Symptom: Logi pokazujÄ… `OperationalError: could not connect to server`
Przyczyna: Å¹le skonfigurowany `DATABASE_URL_CLOUD` secret lub brak uprawnienia do Cloud SQL
RozwiÄ…zanie: ZweryfikowaÄ‡ format connection string (`postgresql+asyncpg://...?host=/cloudsql/...`) i sprawdziÄ‡ czy service account ma rolÄ™ `roles/cloudsql.client`

**Problem 3: Neo4j timeout**
Symptom: `/startup` endpoint pokazuje `"neo4j": "connection_failed"`
Przyczyna: Neo4j AuraDB wymaga `neo4j+s://` URI (nie `bolt://`), lub firewall blokuje Cloud Run IP
RozwiÄ…zanie: ZaktualizowaÄ‡ `NEO4J_URI` secret i dodaÄ‡ `0.0.0.0/0` do allowlist w AuraDB Console (Cloud Run ma dynamiczne IPs)

**Problem 4: Frontend 404**
Symptom: `GET /` zwraca 404 Not Found, API dziaÅ‚a
Przyczyna: Statyczne pliki frontendu nie zostaÅ‚y skopiowane do Docker image (bÅ‚Ä…d w Dockerfile.cloudrun)
RozwiÄ…zanie: ZweryfikowaÄ‡ `COPY --from=frontend-builder /app/dist /app/static` w Dockerfile i sprawdziÄ‡ czy `app.mount("/", StaticFiles(directory="static", html=True))` jest w main.py

**Problem 5: Slow LLM responses**
Symptom: Timeout errors przy generowaniu person lub focus groups
Przyczyna: NiewystarczajÄ…ce CPU/RAM lub Gemini API throttling
RozwiÄ…zanie: ZwiÄ™kszyÄ‡ `--cpu=4 --memory=8Gi` lub zaimplementowaÄ‡ rate limiting + request queuing

**Problem 6: Out of memory (OOM)**
Symptom: Cloud Run logs pokazujÄ… `Memory limit exceeded`, instancja crashuje
Przyczyna: Memory leak, zbyt duÅ¼e embeddings w pamiÄ™ci, lub niewystarczajÄ…cy limit
RozwiÄ…zanie: ZwiÄ™kszyÄ‡ `--memory=8Gi` lub zoptymalizowaÄ‡ memory usage (batch processing embeddings, garbage collection)

### Cost Optimization

Dla maÅ‚ego projektu (okoÅ‚o 100 users, 1000 requests/day) miesiÄ™czne koszty wynoszÄ… okoÅ‚o **16-30 USD**:

- Cloud Run (sight): $5-10 - zaleÅ¼ne od request count i compute time
- Cloud SQL (db-f1-micro): $10-15 - staÅ‚y koszt za instancjÄ™ + storage
- Neo4j AuraDB Free: $0 - darmowy tier do 50k nodes
- Upstash Redis Free: $0 - darmowy tier do 10k requests/day
- Cloud Build: $0-2 - pierwsze 120 minut/dzieÅ„ sÄ… free
- Artifact Registry: $1-3 - storage + egress

NajwiÄ™kszym kosztem jest Cloud SQL. Dla jeszcze niÅ¼szych kosztÃ³w moÅ¼na rozwaÅ¼yÄ‡ Cloud SQL Serverless (pay-per-use) lub migracjÄ™ do managed PostgreSQL od innego providera (Supabase, Neon).

**Cost optimization tips:**

1. **Cloud Run auto-scaling** - dziÄ™ki `--min-instances=0` aplikacja scale-uje do zero instancji gdy brak traffic. PÅ‚acimy tylko za actual compute time, nie za idle instances. Dla okoÅ‚o 1000 requests/day (Å›rednio 500ms/request) to okoÅ‚o 8 minut compute time dziennie = $0.20/dzieÅ„ = $6/miesiÄ…c.

2. **Gemini Flash zamiast Pro** - Flash model kosztuje $0.075/1M input tokens (Pro: $1.25 = 17x droÅ¼ej). Dla wiÄ™kszoÅ›ci operacji (generowanie person, focus group responses) Flash daje wystarczajÄ…cÄ… jakoÅ›Ä‡. Pro uÅ¼ywamy tylko dla complex analysis i summarization.

3. **Redis cache hits** - segment briefs sÄ… cache'owane w Redis na 7 dni. Cache hit rate okoÅ‚o 80% oznacza 80% mniej wywoÅ‚aÅ„ Gemini API = oszczÄ™dnoÅ›Ä‡ okoÅ‚o $15-20/miesiÄ…c dla aktywnego uÅ¼ytkowania.

4. **Docker layer caching** - Cloud Build cache'uje Docker layers miÄ™dzy buildami z explicit `--cache-from` source. JeÅ›li `requirements.txt` i `package.json` siÄ™ nie zmieniÅ‚y, instalacja dependencies jest skipped = build zajmuje 5-8 minut zamiast 20-25. Mniej compute time = niÅ¼sze koszty Cloud Build (okoÅ‚o $0.50-1.00 oszczÄ™dnoÅ›ci per build).

---

**Autorzy:** DevOps & Infrastructure Team
**Kontakt:** Slack #infrastructure
**Ostatnia aktualizacja:** 2025-11-03
