# Architektura AI/ML i RAG - Sight Platform

**Ostatnia aktualizacja:** 2025-11-03
**Wersja:** 2.1
**Autor:** AI/ML Engineering Team

---

## Spis TreÅ›ci

1. [PrzeglÄ…d Architektury](#przeglÄ…d-architektury)
2. [Model Selection Strategy](#model-selection-strategy)
3. [LLM Infrastructure](#llm-infrastructure)
4. [RAG System Architecture](#rag-system-architecture)
5. [Prompt Engineering](#prompt-engineering)
6. [Performance Optimizations](#performance-optimizations)
7. [Token Usage & Cost Management](#token-usage--cost-management)
8. [Monitoring & Observability](#monitoring--observability)

---

## PrzeglÄ…d Architektury

System AI/ML w platformie Sight wykorzystuje **Google Gemini 2.5** (Flash i Pro) do generowania realistycznych person oraz symulacji grup fokusowych. Architektura oparta jest na trzech filarach:

1. **LLM Orchestration Layer** - ZarzÄ…dzanie wywoÅ‚aniami modeli jÄ™zykowych z retry logic
2. **Hybrid RAG System** - Wyszukiwanie wektorowe + sÅ‚ownikowe + grafowe dla polskiego kontekstu
3. **Event Sourcing** - Immutable log wszystkich interakcji AI dla audytu i reprodukowalnoÅ›ci

### Architektura Wysokiego Poziomu

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SIGHT AI/ML ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Persona     â”‚    â”‚ Focus Group  â”‚    â”‚   Survey     â”‚     â”‚
â”‚  â”‚ Generation   â”‚    â”‚  Discussion  â”‚    â”‚  Response    â”‚     â”‚
â”‚  â”‚ Gemini Flash â”‚    â”‚ Gemini Flash â”‚    â”‚ Gemini Flash â”‚     â”‚
â”‚  â”‚ temp=0.9     â”‚    â”‚ temp=0.8     â”‚    â”‚ temp=0.7     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                   â”‚                    â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                             â†“                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚   LLM Abstraction Layer  â”‚                       â”‚
â”‚              â”‚  (LangChain + Retry)     â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                             â†“                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â†“                                          â†“            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Hybrid RAG â”‚                         â”‚  Graph RAG  â”‚       â”‚
â”‚  â”‚  (Vector +  â”‚                         â”‚  (Neo4j)    â”‚       â”‚
â”‚  â”‚  Keyword)   â”‚                         â”‚             â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â†“                                          â†“            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚            Usage Tracking & Cost Monitoring         â”‚       â”‚
â”‚  â”‚       (Tokens, Cost, Latency, Error Rate)           â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GÅ‚Ã³wne Komponenty

**Service Layer** (`app/services/`) - Logika biznesowa zorganizowana wedÅ‚ug domeny:
- `personas/` - Generacja person, orkiestracja, analiza JTBD
- `focus_groups/` - ZarzÄ…dzanie dyskusjami, podsumowania, pamiÄ™Ä‡ konwersacyjna
- `surveys/` - Generacja odpowiedzi na ankiety
- `rag/` - Hybrid search, graph transformations, zarzÄ…dzanie dokumentami
- `shared/` - WspÃ³Å‚dzielone klienty LLM, narzÄ™dzia

**Configuration Layer** (`config/`) - Wszystkie prompty, modele i ustawienia w YAML:
- `models.yaml` - Rejestr modeli z fallback chain
- `prompts/` - 25+ promptÃ³w zorganizowanych wedÅ‚ug domeny
- `rag/retrieval.yaml` - Konfiguracja chunking, hybrid search, reranking

---

## Model Selection Strategy

**ZASADA:** UÅ¼ywaj najtaÅ„szego modelu, ktÃ³ry speÅ‚nia wymagania jakoÅ›ciowe.

### Gemini 2.5 Flash

**Przypadki uÅ¼ycia:** 90% operacji - generacja person, odpowiedzi w grupach fokusowych, odpowiedzi ankietowe

**Parametry:**
- Temperature: 0.7-0.9 (kreatywnoÅ›Ä‡ vs spÃ³jnoÅ›Ä‡)
- Max tokens: 2000-6000
- Timeout: 30-90s

**Performance:**
- Koszt: $0.075 / 1M input tokens, $0.30 / 1M output tokens
- Latencja: 1-3s per request
- Throughput: ~20 rÃ³wnolegÅ‚ych wywoÅ‚aÅ„ (asyncio.gather)

### Gemini 2.5 Pro

**Przypadki uÅ¼ycia:** 10% operacji - orkiestracja person, analiza JTBD, podsumowania grup fokusowych

**Parametry:**
- Temperature: 0.2-0.4 (analityczne zadania)
- Max tokens: 4000-8000
- Timeout: 90-120s

**Performance:**
- Koszt: $1.25 / 1M input tokens, $5.00 / 1M output tokens (17x droÅ¼ej!)
- Latencja: 3-5s per request
- Kiedy uÅ¼yÄ‡: Complex reasoning, dÅ‚ugie konteksty (>8k tokens), wysokie wymagania jakoÅ›ciowe

### Model Registry (`config/models.yaml`)

Centralna konfiguracja z fallback chain:
1. Domain-specific override (np. `domains.personas.orchestration`)
2. Domain default (np. `domains.personas.generation`)
3. Global default (`defaults.chat`)

**PrzykÅ‚ad uÅ¼ycia:**
```python
from config import models
from app.services.shared.clients import build_chat_model

# Pobierz konfiguracjÄ™ z fallback chain
model_config = models.get("personas", "generation")
# Result: {model: "gemini-2.5-flash", temperature: 0.9, ...}

# Zbuduj model z automatic retry logic
llm = build_chat_model(**model_config.params)

# WywoÅ‚aj model (async)
response = await llm.ainvoke(messages)
```

---

## LLM Infrastructure

### LangChain Abstraction Layer

**Lokalizacja:** `app/services/shared/clients.py`

**KorzyÅ›ci:**
1. **Unified Interface** - Jedna funkcja `build_chat_model()` dla wszystkich serwisÃ³w
2. **Automatic Retry** - Exponential backoff dla rate limits (1s, 2s, 4s)
3. **Provider Flexibility** - Åatwa migracja Gemini â†’ OpenAI â†’ Anthropic
4. **Structured Outputs** - Pydantic models z walidacjÄ…
5. **Token Tracking** - Automatyczne logowanie usage_metadata

### Error Handling & Graceful Degradation

**Exponential Backoff:**
```python
# LangChain automatic retry dla ResourceExhausted (rate limits)
llm = build_chat_model(max_retries=3)
# Retry 1: 1s, Retry 2: 2s, Retry 3: 4s
```

**Graceful Degradation:**
- RAG failuje â†’ generuj personÄ™ bez kontekstu
- Graph RAG timeout â†’ uÅ¼yj tylko vector search
- LLM failuje â†’ fallback response (dla grup fokusowych)

---

## LangGraph Integration & Study Designer

### PrzeglÄ…d

**LangGraph** to framework state machine'Ã³w do orchestracji zÅ‚oÅ¼onych przepÅ‚ywÃ³w konwersacyjnych z LLM. W Sight uÅ¼ywamy LangGraph w **Study Designer Chat** - systemie konwersacyjnego projektowania badaÅ„, ktÃ³ry prowadzi uÅ¼ytkownika przez wieloetapowy proces definiowania badania UX.

**Dlaczego LangGraph?**
- **State persistence** - TypedDict state zapisywany w PostgreSQL (JSONB)
- **Conditional routing** - WÄ™zÅ‚y decydujÄ… o nastÄ™pnym kroku na podstawie danych
- **Loop-back logic** - MoÅ¼liwoÅ›Ä‡ powrotu do poprzednich etapÃ³w gdy dane niekompletne
- **Message history** - PeÅ‚na historia konwersacji user â†” assistant
- **Separation of concerns** - KaÅ¼dy node ma jednÄ… odpowiedzialnoÅ›Ä‡ (SRP)

### Architektura Study Designer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LangGraph StateGraph                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  START â†’ welcome â†’ gather_goal â”€â”€â”€â”€â”            â”‚
â”‚                      â†‘              â†“            â”‚
â”‚                      â””â”€â”€(loop)â”€â”€ define_audienceâ”‚
â”‚                                     â†“            â”‚
â”‚                              select_method      â”‚
â”‚                                     â†“            â”‚
â”‚                             configure_details   â”‚
â”‚                                     â†“            â”‚
â”‚                              generate_plan      â”‚
â”‚                                     â†“            â”‚
â”‚                              await_approval     â”‚
â”‚                                     â†“            â”‚
â”‚                                   END            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**7 Node Types:**

1. **welcome** (static) - WiadomoÅ›Ä‡ powitalna
2. **gather_goal** (LLM) - Ekstrakcja celu badania
3. **define_audience** (LLM) - Definicja grupy docelowej
4. **select_method** (LLM) - WybÃ³r metody (personas/focus_group/survey/mixed)
5. **configure_details** (LLM) - SzczegÃ³Å‚y konfiguracji
6. **generate_plan** (LLM) - Generacja planu badania (Markdown)
7. **await_approval** (static) - Oczekiwanie na zatwierdzenie

### State Schema (TypedDict)

```python
from typing import TypedDict, NotRequired, Literal

class ConversationState(TypedDict):
    session_id: str
    user_id: str
    messages: list[dict[str, str]]  # [{"role": "user|assistant|system", "content": "..."}]
    current_stage: Literal["welcome", "gather_goal", "define_audience", ...]

    # Optional fields (populated during conversation)
    study_goal: NotRequired[str | None]
    target_audience: NotRequired[dict | None]
    research_method: NotRequired[Literal["personas", "focus_group", "survey", "mixed"] | None]
    configuration: NotRequired[dict | None]
    generated_plan: NotRequired[dict | None]
    plan_approved: NotRequired[bool]
```

**Serialization:**
- State zapisywany jako JSON w `study_designer_sessions.conversation_state` (JSONB column)
- Datetime i UUID konwertowane do string przed zapisem
- Deserializacja odtwarza TypedDict z DB JSON

### Node Implementation Pattern

**PrzykÅ‚ad: gather_goal node**

```python
async def gather_goal_node(state: ConversationState) -> ConversationState:
    """Ekstraktuje cel badania z wiadomoÅ›ci uÅ¼ytkownika."""

    # 1. Pobierz ostatniÄ… wiadomoÅ›Ä‡ uÅ¼ytkownika
    user_message = get_last_user_message(state)
    if not user_message:
        return state  # Brak wiadomoÅ›ci - zostaÅ„ w obecnym stage

    # 2. Przygotuj prompt z kontekstem
    template = prompts.get("study_designer.gather_goal")
    prompt_text = template.render(user_message=user_message)

    # 3. WywoÅ‚aj LLM (Gemini 2.5 Flash, temp=0.8)
    model_config = models.get("study_designer", "question_generation")
    llm = build_chat_model(**model_config.params)
    response = await llm.ainvoke(prompt_text)

    # 4. Parsuj strukturowany JSON output
    llm_output = parse_llm_json_response(response.content)
    # Expected: {goal_extracted: bool, goal: str|null, confidence: str,
    #            follow_up_question: str|null, assistant_message: str}

    # 5. Zaktualizuj state na podstawie wyniku
    if llm_output.get("goal_extracted") and llm_output.get("goal"):
        state["study_goal"] = llm_output["goal"]
        state["current_stage"] = "define_audience"  # Sukces - idÅº dalej
    else:
        state["current_stage"] = "gather_goal"  # Loop-back - zadaj kolejne pytanie

    # 6. Dodaj odpowiedÅº asystenta do historii
    state["messages"].append({
        "role": "assistant",
        "content": llm_output["assistant_message"]
    })

    return state
```

**Kluczowe wzorce:**
- **Conditional stage transition** - Node ustawia `current_stage` aby kontrolowaÄ‡ routing
- **Loop-back pattern** - JeÅ›li dane niekompletne, node pozostawia stage bez zmian
- **Structured JSON output** - LLM zwraca JSON z predefiniowanymi polami
- **State mutation** - Node modyfikuje state in-place i zwraca zaktualizowany

### LLM Configuration per Stage

**config/models.yaml:**

```yaml
domains:
  study_designer:
    question_generation:  # gather_goal, define_audience, select_method
      model: "gemini-2.5-flash"
      temperature: 0.8  # WyÅ¼sza dla kreatywnych follow-up questions
      max_tokens: 2000
      timeout: 30
      retries: 3

    plan_generation:  # generate_plan
      model: "gemini-2.5-flash"
      temperature: 0.3  # NiÅ¼sza dla strukturowanego outputu
      max_tokens: 6000
      timeout: 60
      retries: 3
```

**Dlaczego rÃ³Å¼ne temperatury?**
- **Wysoka (0.7-0.8):** Pytania dostosowane do kontekstu, kreatywne follow-upy
- **Åšrednia (0.5-0.6):** WybÃ³r opcji z wyjaÅ›nieniem
- **Niska (0.3-0.4):** Strukturowany output (plan Markdown, estymacje)

### Prompts (Jinja2 Templates)

**5 PromptÃ³w Study Designer** w `config/prompts/study_designer/`:

**1. gather_goal.yaml** - Ekstrakcja celu badania
```yaml
id: study_designer.gather_goal
version: "1.0.0"
messages:
  - role: system
    content: |
      JesteÅ› ekspertem UX researcher. Twoim zadaniem jest zrozumienie
      celu badania poprzez analizÄ™ odpowiedzi i zadawanie pytaÅ„.

      FORMAT ODPOWIEDZI - ZAWSZE JSON:
      {
        "goal_extracted": true/false,
        "goal": "PeÅ‚ny wyekstraktowany cel lub null",
        "confidence": "high"|"medium"|"low",
        "follow_up_question": "Pytanie do uÅ¼ytkownika lub null",
        "assistant_message": "PeÅ‚na odpowiedÅº dla uÅ¼ytkownika"
      }

      KRYTERIA SUKCESU:
      - Cel konkretny (nie "zrobiÄ‡ badanie" ale "zrozumieÄ‡ porzucanie koszyka")
      - Cel mierzalny (moÅ¼na zaprojektowaÄ‡ badanie wokÃ³Å‚ niego)
      - Cel biznesowy (rozwiÄ…zuje problem lub odpowiada na pytanie)

  - role: user
    content: ${user_message}
```

**2. define_audience.yaml** - Definicja grupy docelowej
```yaml
id: study_designer.define_audience
version: "1.0.0"
messages:
  - role: system
    content: |
      Ekstraktuj demografiÄ™ grupy docelowej. Pytaj o:
      - Wiek (range lub konkretne grupy)
      - PÅ‚eÄ‡ (jeÅ›li istotne)
      - Lokalizacja (kraj, miasto, region)
      - ZawÃ³d / branÅ¼a
      - Psychografia (postawy, zachowania, wartoÅ›ci)

      JSON OUTPUT:
      {
        "audience_defined": true/false,
        "target_audience": {
          "age_range": "25-40",
          "gender": "all"|"male"|"female"|"other",
          "location": "Polska, miasta >100k",
          "occupation": "IT professionals",
          "psychographics": "Early adopters, tech-savvy"
        },
        "follow_up_question": null,
        "assistant_message": "..."
      }
```

**3. generate_plan.yaml** - Generacja kompletnego planu
```yaml
id: study_designer.generate_plan
version: "1.0.0"
messages:
  - role: system
    content: |
      Wygeneruj kompletny plan badania w formacie Markdown.

      INPUT CONTEXT:
      - Cel: ${study_goal}
      - Grupa docelowa: ${target_audience}
      - Metoda: ${research_method}
      - Konfiguracja: ${configuration}

      JSON OUTPUT:
      {
        "markdown_summary": "# Plan Badania\n\n## Cel\n...",
        "estimated_time_seconds": 1200,  // Total execution time
        "estimated_cost_usd": 8.50,
        "execution_steps": [
          {"type": "personas_generation", "config": {...}},
          {"type": "focus_group_discussion", "config": {...}}
        ]
      }

      PLAN STRUCTURE (Markdown):
      # Plan Badania UX

      ## 1. Cel Badania
      ${study_goal}

      ## 2. Grupa Docelowa
      (demografia)

      ## 3. Metoda Badawcza
      ${research_method} - wyjaÅ›nienie dlaczego

      ## 4. SzczegÃ³Å‚y Wykonania
      - Liczba person/uczestnikÃ³w
      - Liczba pytaÅ„/zadaÅ„
      - Timeline

      ## 5. Oczekiwane Wnioski
      Co dowiemy siÄ™ z tego badania?

      ## 6. Next Steps
      Jak wykorzystaÄ‡ wyniki?
```

### JSON Parsing with Fallbacks

**Robust parsing strategy** (3-level fallback):

```python
def parse_llm_json_response(content: str) -> dict:
    """Parsuje JSON z LLM response z multiple fallback strategies."""

    # Strategy 1: Direct JSON parse
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Strategy 2: Extract from markdown code blocks
    match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass

    # Strategy 3: Find first {...} block
    match = re.search(r'\{.*\}', content, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass

    # Fallback: Return error structure
    return {
        "error": "Failed to parse JSON",
        "raw_content": content
    }
```

**Dlaczego potrzebne?**
- LLM czasami opakowuje JSON w markdown (` ```json ... ``` `)
- LLM dodaje dodatkowy tekst przed/po JSON
- Fallback zapewnia graceful degradation

### Conditional Routing Implementation

```python
class ConversationStateMachine:
    def __init__(self):
        workflow = StateGraph(ConversationState)

        # Add all nodes
        workflow.add_node("welcome", welcome_node)
        workflow.add_node("gather_goal", gather_goal_node)
        workflow.add_node("define_audience", define_audience_node)
        # ... etc

        # Static edges (always proceed)
        workflow.add_edge("welcome", "gather_goal")

        # Conditional edges (routing based on state)
        workflow.add_conditional_edges(
            "gather_goal",
            self._route_from_gather_goal,
            {
                "define_audience": "define_audience",  # Success path
                "gather_goal": "gather_goal"  # Loop-back path
            }
        )

        workflow.set_entry_point("welcome")
        self.graph = workflow.compile()

    def _route_from_gather_goal(self, state: ConversationState) -> str:
        """Routing logic: check current_stage set by node."""
        if state["current_stage"] == "define_audience":
            return "define_audience"  # Goal extracted - proceed
        return "gather_goal"  # Goal unclear - ask again
```

**Key Pattern:**
- Node ustawia `current_stage` w state
- Routing function sprawdza `current_stage` i zwraca nazwÄ™ nastÄ™pnego node
- LangGraph automatycznie wywoÅ‚uje wskazany node

### State Persistence Strategy

**Database:** `study_designer_sessions` table (PostgreSQL)

```sql
CREATE TABLE study_designer_sessions (
    id UUID PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES users(id),
    conversation_state JSONB NOT NULL,  -- Complete LangGraph state
    status VARCHAR(50) DEFAULT 'active',
    current_stage VARCHAR(50) DEFAULT 'welcome',
    generated_plan JSONB,  -- Cached from state for indexing
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Index for user queries
CREATE INDEX idx_sessions_user_status ON study_designer_sessions(user_id, status);

-- GIN index for JSONB queries (optional)
CREATE INDEX idx_sessions_state ON study_designer_sessions USING GIN(conversation_state);
```

**Serialization Functions:**

```python
def serialize_state(state: ConversationState) -> dict:
    """Convert TypedDict to JSON-serializable dict."""
    return {
        "session_id": state["session_id"],
        "user_id": state["user_id"],
        "messages": state["messages"],
        "current_stage": state["current_stage"],
        **{k: v for k, v in state.items() if k not in ["session_id", "user_id", "messages", "current_stage"]}
    }

def deserialize_state(data: dict) -> ConversationState:
    """Convert DB JSON back to TypedDict."""
    return ConversationState(**data)
```

### Performance Metrics

| Metryka | Target | Actual | Optimization |
|---------|--------|--------|--------------|
| Session init | < 2s | ~1.5s | Static welcome message |
| Message processing (LLM) | < 5s | ~3-4s | Gemini Flash (fast model) |
| Plan generation | < 8s | ~6s | Concurrent LLM calls |
| State save to DB | < 100ms | ~50ms | JSONB native format |
| State load from DB | < 100ms | ~40ms | Indexed query |

**Token Usage per Stage:**

| Stage | Avg Input Tokens | Avg Output Tokens | Cost per Stage |
|-------|------------------|-------------------|----------------|
| gather_goal | 150 | 100 | $0.04 |
| define_audience | 200 | 120 | $0.05 |
| select_method | 250 | 180 | $0.06 |
| configure_details | 300 | 150 | $0.07 |
| generate_plan | 500 | 800 | $0.28 |
| **Total per session** | **~1400** | **~1350** | **~$0.50** |

### Error Handling & Resilience

**LLM Failures:**
```python
try:
    response = await llm.ainvoke(prompt)
except Exception as e:
    logger.error(f"LLM call failed: {e}", extra={"session_id": session_id})

    # Fallback: Add error message to conversation
    state["messages"].append({
        "role": "assistant",
        "content": "Przepraszam, wystÄ…piÅ‚ problem. SprÃ³buj ponownie za moment."
    })

    # Don't change current_stage - allow retry
    return state
```

**State Corruption Recovery:**
```python
def validate_state(state: ConversationState) -> bool:
    """Validate state consistency."""
    required_fields = ["session_id", "user_id", "messages", "current_stage"]
    if not all(field in state for field in required_fields):
        return False

    if state["current_stage"] not in VALID_STAGES:
        return False

    # Check stage progression logic
    if state["current_stage"] == "define_audience" and not state.get("study_goal"):
        return False  # Can't be in define_audience without goal

    return True
```

### Future Enhancements

**Priority 1:**
- [ ] **Modify plan flow** - PozwÃ³l uÅ¼ytkownikowi wrÃ³ciÄ‡ do dowolnego stage'u
- [ ] **Multi-turn clarification** - GÅ‚Ä™bsza konwersacja w ramach jednego node
- [ ] **Session templates** - Zapisuj i ponownie uÅ¼ywaj udanych konfiguracji

**Priority 2:**
- [ ] **Voice input integration** - Web Speech API dla mÃ³wionego inputu
- [ ] **Real-time typing indicators** - Pokazuj gdy LLM "pisze"
- [ ] **Suggested responses** - Quick reply buttons na podstawie kontekstu

**Priority 3:**
- [ ] **Multi-language support** - TÅ‚umaczenie promptÃ³w (English, German)
- [ ] **Collaborative sessions** - Wielu uÅ¼ytkownikÃ³w w jednej sesji
- [ ] **A/B testing prompts** - Testuj rÃ³Å¼ne wersje promptÃ³w

---

## RAG System Architecture

### Dual-Source Retrieval Strategy

System RAG Å‚Ä…czy dwa komplementarne ÅºrÃ³dÅ‚a kontekstu:

1. **Hybrid Search** (~500ms) - Szybkie wyszukiwanie chunkÃ³w tekstowych
   - Vector search: Embeddingi Gemini (768 dim) + cosine similarity
   - Keyword search: Neo4j fulltext index (Lucene)
   - RRF fusion: Reciprocal rank fusion dla balansowania wynikÃ³w

2. **Graph RAG** (~1500ms) - Strukturalna wiedza z grafu
   - LLM-powered Cypher query generation
   - 4 typy wÄ™zÅ‚Ã³w: WskaÅºnik, Obserwacja, Trend, Demografia
   - Bogate metadane: streszczenie, kluczowe_fakty, pewnoÅ›Ä‡, okres_czasu

```
Query: "kobieta, 25-34, wyÅ¼sze, Warszawa"
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   PARALLEL RETRIEVAL (~2s)     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Hybrid Search  â”‚  Graph RAG   â”‚
    â”‚  8 chunks       â”‚  Graph nodes â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ UNIFIED CONTEXT       â”‚
     â”‚ 8000 chars max        â”‚
     â”‚ Graph + Enriched      â”‚
     â”‚ chunks                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid Search Implementation

**Lokalizacja:** `app/services/rag/rag_hybrid_search_service.py`

#### 1. Vector Search (Semantic)
- **Embeddings:** Google Gemini `models/gemini-embedding-001` (768 dimensions)
- **Index:** Neo4j Vector Index (HNSW algorithm)
- **Distance:** Cosine similarity
- **Performance:** ~200ms for top-8

#### 2. Keyword Search (Lexical)
- **Index:** Neo4j Fulltext (Lucene-based)
- **Fields:** `text` content w wÄ™zÅ‚ach `RAGChunk`
- **Performance:** ~100ms for top-8

#### 3. RRF Fusion
**Formula:** `score = 1 / (k + rank + 1)` dla kaÅ¼dego ÅºrÃ³dÅ‚a (k=60)

**KorzyÅ›ci:** Balansuje semantic recall i lexical precision

```python
def rrf_fusion(vector_results, keyword_results, k=60):
    scores = {}
    # Vector contribution
    for rank, (doc, _) in enumerate(vector_results):
        scores[hash(doc)] = 1.0 / (k + rank + 1)
    # Keyword contribution
    for rank, (doc, _) in enumerate(keyword_results):
        scores[hash(doc)] = scores.get(hash(doc), 0) + 1.0 / (k + rank + 1)
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)
```

#### 4. Cross-Encoder Reranking (Opcjonalny)
- **Model:** `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **Performance:** ~100-150ms dla 10 candidates
- **Configuration:** `rag.retrieval.reranking.enabled: true`

### Graph RAG Implementation

**Lokalizacja:** `app/services/rag/rag_graph_service.py`

#### Node Types
- **WskaÅºnik:** Metryki, statystyki z wielkoÅ›ciÄ… i pewnoÅ›ciÄ…
- **Obserwacja:** Fakty, przyczyny, skutki
- **Trend:** Zmiany w czasie z okresem czasu
- **Demografia:** Grupy demograficzne z charakterystykÄ…

#### Node Properties
- `streszczenie` - Jednozdaniowe podsumowanie (WYMAGANE)
- `kluczowe_fakty` - Max 3 kluczowe fakty
- `skala` - WartoÅ›Ä‡ z jednostkÄ… (tylko WskaÅºnik)
- `pewnosc` - wysoka / Å›rednia / niska
- `okres_czasu` - Zakres czasowy (np. "2019-2023")
- `doc_id`, `chunk_index` - Metadane ÅºrÃ³dÅ‚a

#### Graph Query Strategy

**Cypher Query Pattern (Neo4j 5.x+ CALL subqueries):**
```cypher
// WskaÅºniki (top 3, preferuj wysokÄ… pewnoÅ›Ä‡)
CALL () {
    MATCH (ind:Wskaznik)
    WHERE ANY(term IN $search_terms WHERE
        toLower(ind.streszczenie) CONTAINS toLower(term)
    )
    RETURN ind
    ORDER BY
        CASE ind.pewnosc
            WHEN 'wysoka' THEN 0
            WHEN 'srednia' THEN 1
            ELSE 2
        END
    LIMIT 3
}
// Analogicznie: Obserwacje (3), Trendy (2), Demografia (2)
RETURN indicators + observations + trends + demographics
```

**Performance:** <5s per query (z TEXT indexes), timeout 10s

#### Graph Context Formatting

**PrzykÅ‚ad:**
```
=== STRUKTURALNA WIEDZA Z GRAFU WIEDZY ===

ğŸ“Š WSKAÅ¹NIKI DEMOGRAFICZNE:
â€¢ 78.4% zatrudnienia w grupie 25-34 lata (2023)
  WielkoÅ›Ä‡: 78.4%
  PewnoÅ›Ä‡: wysoka
  Kluczowe fakty: NajwyÅ¼sza stopa wÅ›rÃ³d grup wiekowych

ğŸ“ˆ TRENDY:
â€¢ Wzrost zatrudnienia kobiet w IT o 23% (2019-2023)
  Okres: 2019-2023
  Kluczowe fakty: SzczegÃ³lnie w Warszawie i Krakowie
```

### Unified Context Assembly

**Strategia:** Chunk enrichment

1. **Format graph context** - Graph nodes â†’ czytelny tekst
2. **Find related nodes** - Matching: doc_id, keywords
3. **Enrich chunks** - Max 2 wskaÅºniki, 2 obserwacje, 1 trend per chunk
4. **Assemble** - Graph context na poczÄ…tku, enriched chunks poniÅ¼ej
5. **Truncate** - Limit do max_context_chars (8000)

**Metryki:**
- Enriched chunks: ~40-60% chunkÃ³w ma powiÄ…zane graph nodes
- Context size: ~6000-8000 chars
- Improvement: +15% persona realism score (user eval)

### Redis Caching Strategy

#### Hybrid Search Cache
- **Key:** `hybrid_search:{query_hash}:{top_k}`
- **TTL:** 7 dni
- **Hit rate:** 70-90%
- **Performance:** Cache hit <50ms vs miss ~500ms

#### Graph RAG Cache
- **Key:** `graph_context:{age}:{edu}:{loc}:{gender}`
- **TTL:** 7 dni
- **Hit rate:** 80-95%
- **Performance:** Cache hit <50ms vs miss ~1500ms

**PrzykÅ‚ad:**
```python
cache_key = f"graph_context:{age}:{edu}:{loc}:{gender}"
cached = await redis.get(cache_key)
if cached:
    return json.loads(cached)

# Cache miss - execute query
nodes = await graph_store.query(cypher_query)
await redis.setex(cache_key, 604800, json.dumps(nodes))
```

---

## Prompt Engineering

### Centralized Prompt Management

**Lokalizacja:** `config/prompts/`

**Struktura:**
```
config/prompts/
â”œâ”€â”€ personas/         # 7 promptÃ³w
â”œâ”€â”€ focus_groups/     # 2 prompty
â”œâ”€â”€ surveys/          # 4 prompty
â”œâ”€â”€ rag/              # 2 prompty
â””â”€â”€ system/           # 10 promptÃ³w systemowych
```

**ÅÄ…cznie:** 25 promptÃ³w

### Prompt Template Format

```yaml
id: "personas.generation"
version: "1.0.0"
description: "Generacja syntetycznej persony"
model: "gemini-2.5-flash"
temperature: 0.9
messages:
  - role: system
    content: |
      JesteÅ› ekspertem generacji person dla polskiego rynku...
  - role: user
    content: |
      Wygeneruj personÄ™: ${age}, ${gender}, ${education}
```

**Jinja2 Delimiters:** `${variable}` (kompatybilnoÅ›Ä‡ z Cypher queries)

### Key Prompt Patterns

#### 1. Persona Generation (Flash, temp=0.9)

**Challenge:** Generuj UNIKALNE, RÃ“Å»NORODNE persony w ramach segmentu

**Solution:**
- Persona seed: Losowy seed per persona (`#${seed}`)
- Explicit diversity: "KaÅ¼da persona MUSI mieÄ‡ RÃ“Å»NÄ„ historiÄ™"
- Few-shot example: Bogaty przykÅ‚ad (400-600 sÅ‚Ã³w background_story)
- RAG integration: Kontekst jako TÅO (nie cytuj statystyk!)

#### 2. Orchestration (Pro, temp=0.3)

**Challenge:** Analityczna segmentacja + dÅ‚ugie edukacyjne briefe

**Solution:**
- System prompt: Polish society expert
- Graph context: Insights z Neo4j jako faktyczne dane
- DÅ‚ugie briefe: 900-1200 znakÃ³w per segment
- JSON output: Pydantic validation

**Output:**
```json
{
  "total_personas": 20,
  "groups": [
    {
      "segment_name": "MÅ‚odzi Prekariusze",
      "allocation": 6,
      "segment_brief": "Kim sÄ… mÅ‚odzi prekariusze? [900-1200 znakÃ³w]",
      "reasoning": "Dlaczego 6 person..."
    }
  ]
}
```

#### 3. Focus Group Response (Flash, temp=0.8)

**Challenge:** Naturalne odpowiedzi 2-4 zdania

**Solution:**
- PeÅ‚ny persona context: Demografia, wartoÅ›ci, background_story
- Conversation history: Top 3 previous responses (RAG z event sourcing)
- Natural language: "Respond naturally as this person would"

#### 4. JTBD Analysis (Pro, temp=0.25)

**Challenge:** Deterministyczna ekstrakcja Jobs-to-be-Done

**Solution:**
- Very low temperature (0.25)
- Few-shot examples: 2 kompletne przykÅ‚ady
- Structured output: Pydantic (job, desired_outcome, pains)
- RAG integration: Polish market context

**Output:**
```json
{
  "jobs_to_be_done": [
    {
      "job": "ZnaleÅºÄ‡ stabilnÄ… pracÄ™ z moÅ¼liwoÅ›ciÄ… rozwoju",
      "job_type": "functional",
      "frequency": "ongoing"
    }
  ],
  "desired_outcomes": [...],
  "pains": [...]
}
```

### Prompt Validation

**Script:** `scripts/config_validate.py`

```bash
# Waliduj wszystkie prompty
python scripts/config_validate.py

# SprawdÅº placeholdery
python scripts/config_validate.py --check-placeholders

# Auto-bump wersji
python scripts/config_validate.py --auto-bump
```

**Versioning:** Semantic versioning (major.minor.patch)

---

## Performance Optimizations

### 1. Parallel LLM Calls

**Problem:** Sequential calls = 20 person Ã— 3s = 60s

**Solution:** Asyncio.gather

```python
# âŒ Wolne (sequential)
personas = []
for demographic in demographics:
    persona = await generate_persona(demographic)
    personas.append(persona)

# âœ… Szybkie (parallel)
tasks = [generate_persona(d) for d in demographics]
personas = await asyncio.gather(*tasks)
# Total: ~5s (limited by Gemini API rate)
```

**Rate Limiting:** Semaphore

```python
semaphore = asyncio.Semaphore(5)  # Max 5 concurrent

async def generate_with_limit(demographic):
    async with semaphore:
        return await generate_persona(demographic)
```

### 2. Segment Caching (Redis)

**Problem:** Identyczne segment briefe generowane wielokrotnie

**Solution:** Redis cache z 7-day TTL

**Metryki:**
- Hit rate: ~85%
- Speedup: 3x faster (cache hit <50ms vs generation ~1500ms)
- Token savings: 60% redukcja input tokens

### 3. Prompt Compression

**Przed:**
```python
prompt = f"""
You are a persona generation expert specializing in creating
realistic, statistically representative personas...
"""
# ~500 tokens
```

**Po:**
```python
prompt = f"""
Expert: Syntetyczne persony dla polskiego rynku.
PROFIL: Wiek: {age} | PÅ‚eÄ‡: {gender}
"""
# ~200 tokens (60% redukcja)
```

**Savings:** 300 tokens Ã— 20 person = 6000 tokens = $0.00045 saved per batch

### 4. Batch Processing

**Embeddings:**
```python
# âŒ Sequential
embeddings = []
for chunk in chunks:
    emb = await embeddings_model.aembed_query(chunk.text)
    embeddings.append(emb)

# âœ… Batch
texts = [chunk.text for chunk in chunks]
embeddings = await embeddings_model.aembed_documents(texts)
# Gemini: batch size 100 (5-10x faster)
```

---

## Token Usage & Cost Management

### Token Tracking Architecture

**Lokalizacja:** `app/services/dashboard/usage_logging.py`

**Flow:**
```
LLM Call â†’ Extract usage_metadata â†’ Log to DB (async) â†’ Dashboard
```

**Extraction:**
```python
response = await llm.ainvoke(messages)
usage = response.response_metadata.get("usage_metadata")
# Gemini format:
# {
#   "prompt_token_count": 1234,
#   "candidates_token_count": 567,
#   "total_token_count": 1801
# }

input_tokens = usage.get("prompt_token_count")
output_tokens = usage.get("candidates_token_count")
```

**Async Logging (Non-blocking):**
```python
asyncio.create_task(
    log_usage(
        user_id=user_id,
        operation="persona_generation",
        model="gemini-2.5-flash",
        input_tokens=1234,
        output_tokens=567
    )
)
```

### Cost Calculation

**Pricing:** `config/pricing.yaml`

```yaml
gemini-2.5-flash:
  input_price_per_million: 0.075
  output_price_per_million: 0.30

gemini-2.5-pro:
  input_price_per_million: 1.25
  output_price_per_million: 5.00
```

**Formula:**
```python
input_cost = (input_tokens / 1_000_000) * input_price
output_cost = (output_tokens / 1_000_000) * output_price
total_cost = input_cost + output_cost
```

**PrzykÅ‚ad:** 20 person
```
Input: 40,000 tokens
Output: 30,000 tokens

Gemini Flash:
  $0.003 (input) + $0.009 (output) = $0.012

Gemini Pro:
  $0.05 (input) + $0.15 (output) = $0.20 (17x droÅ¼ej!)
```

### Cost Optimization Strategies

#### 1. Model Selection
- **Rule:** Zawsze Flash chyba Å¼e Pro absolutnie konieczny
- **Savings:** 17x cheaper
- **Flash:** 90% operacji
- **Pro:** 10% operacji (orchestration, JTBD, summaries)

#### 2. Caching
- **Redis cache:** Segment briefe, graph context
- **Hit rate:** 70-90%
- **Savings:** $0.002 per cached query

#### 3. Token Budgeting
```python
budget_service = BudgetService(db)
remaining = await budget_service.get_remaining_budget(user_id)

if remaining < estimated_cost:
    raise BudgetExceededError(
        f"Insufficient budget. Remaining: ${remaining:.2f}"
    )
```

---

## Monitoring & Observability

### Target Performance Metrics (SLA)

| Operation | Target P95 | Current Avg | Status |
|-----------|-----------|-------------|--------|
| **Persona Generation** (20 personas) | <60s | ~45s | âœ… Met |
| **Focus Group** (20 Ã— 4 questions) | <3min | ~2min | âœ… Met |
| **Hybrid Search** (vector + keyword + RRF) | <350ms | ~280ms | âœ… Met |
| **Graph RAG Query** | <5s | ~3s | âœ… Met |
| **API Response** (P90) | <500ms | ~420ms | âœ… Met |

### Key Metrics to Track

**LLM Performance:**
- Tokens per operation (input/output)
- Cost per operation ($USD)
- Latency (p50, p90, p95, p99)
- Error rate (% failed calls)
- Retry rate (% requiring retry)

**RAG Performance:**
- Cache hit rate (hybrid, graph)
- Retrieval latency (vector, keyword, graph)
- Context size (chars, tokens)
- Relevance score (user feedback)

**Quality Metrics:**
- Persona quality score (0-100)
- Demographic accuracy (chi-square p-value)
- Consistency score (% passing checks)
- Hallucination rate (% with uncited facts)

### Structured Logging

```python
import logging

logger = logging.getLogger(__name__)

# Log LLM call
logger.info(
    "LLM generation completed",
    extra={
        "operation": "persona_generation",
        "model": "gemini-2.5-flash",
        "input_tokens": 1234,
        "output_tokens": 567,
        "latency_ms": 2800,
        "cost_usd": 0.00026,
        "user_id": str(user_id),
        "project_id": str(project_id)
    }
)
```

---

## Appendix: Quick Reference

### Kluczowe Pliki

**Backend:**
- `app/services/shared/clients.py` - LLM abstraction layer
- `app/services/rag/rag_hybrid_search_service.py` - Hybrid search
- `app/services/rag/rag_graph_service.py` - Graph RAG
- `app/services/personas/persona_generator_langchain.py` - Generacja person
- `app/services/dashboard/usage_logging.py` - Token tracking

**Configuration:**
- `config/models.yaml` - Rejestr modeli LLM
- `config/prompts/` - 25 promptÃ³w YAML
- `config/rag/retrieval.yaml` - Konfiguracja RAG
- `config/pricing.yaml` - Ceny modeli

**Dokumentacja:**
- `config/README.md` - Przewodnik po systemie konfiguracji
- `config/PROMPTS_INDEX.md` - Katalog wszystkich promptÃ³w
- `docs/RAG.md` - Architektura RAG (szczegÃ³Å‚y)
- `docs/TESTING.md` - Organizacja testÃ³w

### Detailed Documentation Reference

**SzczegÃ³Å‚owe dokumenty dostÄ™pne w `docs/architecture/`:**
- `ai_ml.md` - PeÅ‚na wersja z dodatkowymi szczegÃ³Å‚ami (1370 linii)
  - SzczegÃ³Å‚owe benchmarki performance
  - Rozszerzona sekcja Persona Details View
  - DÅ‚ugie przykÅ‚ady kodu i promptÃ³w
  - Roadmap Q1-Q4 2025

---

**Autorzy:** AI/ML Engineering Team
**Kontakt:** Slack #ai-ml-engineering
**Ostatnia aktualizacja:** 2025-11-03
