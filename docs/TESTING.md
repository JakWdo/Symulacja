# üìã Dokumentacja Test√≥w - Market Research SaaS

## Spis Tre≈õci
1. [Szybki Start](#szybki-start)
2. [Struktura Test√≥w](#struktura-test√≥w)
3. [Kategorie Test√≥w](#kategorie-test√≥w)
4. [Wymagania](#wymagania)
5. [Uruchamianie Test√≥w](#uruchamianie-test√≥w)
6. [Kluczowe Fixtures](#kluczowe-fixtures)
7. [Najwa≈ºniejsze Testy](#najwa≈ºniejsze-testy)
8. [Cele Wydajno≈õciowe](#cele-wydajno≈õciowe)
9. [Coverage i Metryki](#coverage-i-metryki)
10. [RozwiƒÖzywanie Problem√≥w](#rozwiƒÖzywanie-problem√≥w)

---

## Szybki Start

### Uruchom wszystkie testy (bez wolnych)
```bash
pytest tests/ -v -m "not slow and not e2e"
```

### Uruchom z pokryciem kodu
```bash
pytest tests/ --cov=app --cov-report=html -m "not slow"
open htmlcov/index.html
```

### Uruchom tylko szybkie testy jednostkowe
```bash
pytest tests/unit/ -v
```

---

## Struktura Test√≥w

```
tests/
‚îú‚îÄ‚îÄ unit/                   # Testy jednostkowe (~150 test√≥w, <5s)
‚îÇ   ‚îú‚îÄ‚îÄ test_persona_generator.py            # Generator person
‚îÇ   ‚îú‚îÄ‚îÄ test_focus_group_service.py          # Serwis grup fokusowych
‚îÇ   ‚îú‚îÄ‚îÄ test_graph_service.py                # Serwis graf√≥w
‚îÇ   ‚îú‚îÄ‚îÄ test_memory_service_langchain.py     # System pamiƒôci
‚îÇ   ‚îú‚îÄ‚îÄ test_discussion_summarizer_service.py # Podsumowania AI
‚îÇ   ‚îú‚îÄ‚îÄ test_persona_validator_service.py    # Walidacja person
‚îÇ   ‚îú‚îÄ‚îÄ test_survey_response_generator.py    # Generator odpowiedzi
‚îÇ   ‚îú‚îÄ‚îÄ test_core_config_security.py         # Konfiguracja i security
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py                       # Modele bazy danych
‚îÇ   ‚îú‚îÄ‚îÄ test_auth_api.py                     # API autoryzacji
‚îÇ   ‚îú‚îÄ‚îÄ test_main_api.py                     # G≈Ç√≥wne endpointy
‚îÇ   ‚îú‚îÄ‚îÄ test_analysis_api.py                 # API analiz
‚îÇ   ‚îú‚îÄ‚îÄ test_graph_analysis_api.py           # API analizy graf√≥w
‚îÇ   ‚îî‚îÄ‚îÄ test_critical_paths.py               # Krytyczne ≈õcie≈ºki
‚îÇ
‚îú‚îÄ‚îÄ integration/            # Testy integracyjne z DB (~35 test√≥w, 10-30s)
‚îÇ   ‚îú‚îÄ‚îÄ test_auth_api_integration.py         # Flow autoryzacji (11 test√≥w)
‚îÇ   ‚îú‚îÄ‚îÄ test_projects_api_integration.py     # CRUD projekt√≥w (10 test√≥w)
‚îÇ   ‚îú‚îÄ‚îÄ test_personas_api_integration.py     # Generowanie person (7 test√≥w)
‚îÇ   ‚îú‚îÄ‚îÄ test_focus_groups_api_integration.py # API grup fokusowych (4 testy)
‚îÇ   ‚îî‚îÄ‚îÄ test_surveys_api_integration.py      # API ankiet (3 testy)
‚îÇ
‚îú‚îÄ‚îÄ e2e/                    # Testy end-to-end (~4 testy, 2-5 min)
‚îÇ   ‚îú‚îÄ‚îÄ test_e2e_full_workflow.py            # Pe≈Çny workflow badania
‚îÇ   ‚îú‚îÄ‚îÄ test_e2e_survey_workflow.py          # Workflow ankiety
‚îÇ   ‚îî‚îÄ‚îÄ test_e2e_graph_analysis.py           # Workflow analizy grafowej
‚îÇ
‚îú‚îÄ‚îÄ performance/            # Testy wydajno≈õci (~5 test√≥w, 5-10 min)
‚îÇ   ‚îî‚îÄ‚îÄ test_performance.py                  # Benchmarki wydajno≈õciowe
‚îÇ
‚îú‚îÄ‚îÄ error_handling/         # Testy b≈Çƒôd√≥w (~9 test√≥w, 5-10s)
‚îÇ   ‚îî‚îÄ‚îÄ test_error_handling.py               # Edge cases i resilience
‚îÇ
‚îú‚îÄ‚îÄ conftest.py             # Wsp√≥lne fixtures i konfiguracja
‚îî‚îÄ‚îÄ TESTY.md               # Ten plik (dokumentacja)
```

**≈ÅƒÖcznie:** ~208 test√≥w

---

## Kategorie Test√≥w

### üü¢ Testy Jednostkowe (Unit Tests)
**Czas:** <5 sekund ≈ÇƒÖcznie
**Liczba:** ~150 test√≥w

TestujƒÖ pojedyncze funkcje i klasy bez zewnƒôtrznych zale≈ºno≈õci.

```bash
# Uruchom wszystkie testy jednostkowe
pytest tests/unit/ -v

# Przyk≈Çadowe modu≈Çy:
pytest tests/unit/test_persona_generator.py -v
pytest tests/unit/test_focus_group_service.py -v
```

**Co testujƒÖ:**
- Losowanie demograficzne (weighted sampling)
- Generowanie cech osobowo≈õci (Big Five, Hofstede)
- Walidacja statystyczna (chi-kwadrat)
- Parsowanie odpowiedzi AI
- Ekstrakcja koncept√≥w z tekstu
- Obliczenia sentymentu i polaryzacji

---

### üü° Testy Integracyjne (Integration Tests)
**Czas:** 10-30 sekund
**Liczba:** ~35 test√≥w
**WymagajƒÖ:** PostgreSQL

TestujƒÖ endpointy API z rzeczywistƒÖ bazƒÖ danych.

```bash
# Wymagane: Docker z PostgreSQL
docker-compose up -d postgres

# Uruchom testy integracyjne
pytest tests/integration/ -v
```

**Co testujƒÖ:**

#### Autoryzacja (11 test√≥w)
- ‚úÖ Rejestracja u≈ºytkownika (hashowanie has≈Ça bcrypt)
- ‚úÖ Logowanie (JWT token generation)
- ‚úÖ Walidacja si≈Çy has≈Ça
- ‚úÖ Ochrona endpoint√≥w (auth required)
- ‚úÖ Wygas≈Çe tokeny
- ‚úÖ Duplikacja email

#### Projekty (10 test√≥w)
- ‚úÖ Tworzenie projektu z demographics
- ‚úÖ Walidacja demographics (suma = 1.0)
- ‚úÖ Listowanie projekt√≥w (tylko w≈Çasne)
- ‚úÖ Aktualizacja projektu
- ‚úÖ Soft delete

#### Persony (7 test√≥w)
- ‚úÖ Generowanie person (Gemini API)
- ‚úÖ Walidacja liczby person (1-1000)
- ‚úÖ Big Five traits verification
- ‚úÖ Tryb adversarial
- ‚úÖ Usuwanie person

#### Grupy Fokusowe (4 testy)
- ‚úÖ Tworzenie grupy fokusowej
- ‚úÖ Aktualizacja draft
- ‚úÖ Listowanie grup
- ‚úÖ Pobieranie wynik√≥w

#### Ankiety (3 testy)
- ‚úÖ Tworzenie ankiety
- ‚úÖ Listowanie ankiet
- ‚úÖ Pobieranie szczeg√≥≈Ç√≥w

---

### üî¥ Testy End-to-End (E2E Tests)
**Czas:** 2-5 minut
**Liczba:** 4 testy
**WymagajƒÖ:** PostgreSQL + Gemini API

TestujƒÖ kompletne scenariusze u≈ºytkownika od poczƒÖtku do ko≈Ñca.

```bash
# Wymagane: Docker + Gemini API key
docker-compose up -d postgres redis neo4j
export GOOGLE_API_KEY=your_key

# Uruchom testy E2E (z logowaniem)
pytest tests/e2e/ -v -s
```

#### ‚≠ê **test_complete_research_workflow_end_to_end**
**Najwa≈ºniejszy test aplikacji!**

Przep≈Çyw (10 krok√≥w):
1. Rejestracja u≈ºytkownika
2. Utworzenie projektu badawczego
3. Generowanie 10 person (15-30s)
4. Walidacja statystyczna (chi-kwadrat)
5. Utworzenie grupy fokusowej (5 person √ó 3 pytania)
6. Uruchomienie dyskusji (30-60s)
7. Weryfikacja 15 odpowiedzi
8. Budowa grafu wiedzy (Neo4j/memory)
9. Generowanie insights AI
10. Weryfikacja performance metrics

**Czas:** ~90-180 sekund
**Je≈õli ten test przechodzi, aplikacja dzia≈Ça!** ‚úÖ

#### **test_survey_workflow_end_to_end**
Przep≈Çyw ankiety:
1. Utworzenie ankiety (4 typy pyta≈Ñ: rating, single-choice, multiple-choice, open-text)
2. Uruchomienie zbierania odpowiedzi
3. Weryfikacja odpowiedzi (10 person √ó 4 pytania = 40)
4. Analiza statystyczna
5. Demographic breakdown

**Czas:** ~60-120 sekund

#### **test_graph_analysis_complete_workflow**
Przep≈Çyw analizy grafowej:
1. Budowa grafu wiedzy
2. Ekstrakcja key concepts
3. Identyfikacja kontrowersyjnych temat√≥w
4. Analiza wp≈Çywowych person (PageRank)
5. Korelacje demograficzne
6. Rozk≈Çad emocji

**Czas:** ~30-60 sekund

#### **test_graph_fallback_when_neo4j_unavailable**
Test resilience:
- Neo4j niedostƒôpny ‚Üí fallback do in-memory graph
- System musi dzia≈Çaƒá bez Neo4j ‚úÖ

---

### üî¥ Testy Wydajno≈õciowe (Performance Tests)
**Czas:** 5-10 minut
**Liczba:** 5 test√≥w
**WymagajƒÖ:** Gemini API

WeryfikujƒÖ czy system spe≈Çnia cele wydajno≈õciowe.

```bash
# Uruchom testy wydajno≈õci
pytest tests/performance/ -v -s
```

**Testy:**

| Test | Target | Ideal | Co testuje |
|------|--------|-------|------------|
| `test_persona_generation_performance_20_personas` | <60s | 30-45s | Generowanie 20 person |
| `test_focus_group_execution_performance_20x4` | <3 min | <2 min | 20 person √ó 4 pytania |
| `test_avg_response_time_per_persona` | <3s | 1-2s | ≈öredni czas odpowiedzi |
| `test_survey_execution_performance_10x10` | <60s | <45s | Ankieta 10√ó10 |
| `test_parallelization_speedup` | >=2x | >=3x | Speedup r√≥wnoleg≈Ço≈õci |

**Dlaczego to wa≈ºne:**
- Bez test√≥w performance, regresje wydajno≈õciowe sƒÖ niewidoczne
- U≈ºytkownicy porzucƒÖ platformƒô je≈õli generowanie person trwa >2 min
- Weryfikuje ≈ºe parallel processing dzia≈Ça (3x speedup)

---

### üü° Testy Error Handling
**Czas:** 5-10 sekund
**Liczba:** 9 test√≥w

TestujƒÖ edge cases i resilience systemu.

```bash
pytest tests/error_handling/ -v
```

**Co testujƒÖ:**

| Test | Scenariusz | Oczekiwane zachowanie |
|------|------------|----------------------|
| `test_gemini_api_timeout_handling` | Gemini API nie odpowiada | Graceful error 503 |
| `test_gemini_api_quota_exceeded_handling` | 429 quota exceeded | Informacja o limicie |
| `test_neo4j_unavailable_fallback_to_memory_graph` | Neo4j down | Fallback do pamiƒôci ‚úÖ |
| `test_empty_personas_list_for_focus_group` | Pusta lista person | Validation error 400 |
| `test_empty_questions_for_focus_group` | Puste pytania | Validation error 400 |
| `test_invalid_demographics_distribution` | Demografia suma != 1.0 | B≈ÇƒÖd lub warning |
| `test_concurrent_focus_group_runs_race_condition` | 2 r√≥wnoleg≈Çe runs | 409 Conflict lub serialize |
| `test_database_connection_error_handling` | DB disconnect | 500/503 error |

**Dlaczego to wa≈ºne:**
- Aplikacja produkcyjna MUSI obs≈Çugiwaƒá b≈Çƒôdy zewnƒôtrznych serwis√≥w
- Testuje resilience i fallback mechanisms
- Chroni przed crashami w produkcji

---

## Wymagania

### Minimalne (Testy Jednostkowe)
```bash
# Python 3.11+
python --version

# Instalacja zale≈ºno≈õci
pip install -r requirements.txt
```

### Pe≈Çne (Integration + E2E)

#### 1. Docker Services
```bash
# Uruchom PostgreSQL, Redis, Neo4j
docker-compose up -d

# Sprawd≈∫ status
docker-compose ps

# Logi (je≈õli problemy)
docker-compose logs postgres
docker-compose logs neo4j
```

#### 2. Zmienne ≈örodowiskowe (`.env`)
```env
# WYMAGANE dla test√≥w E2E i performance
GOOGLE_API_KEY=your_gemini_api_key_here

# Baza danych (testowa jest auto-tworzona)
DATABASE_URL=postgresql+asyncpg://market_research:dev_password_change_in_prod@localhost:5433/market_research_db

# Neo4j (opcjonalny - jest fallback)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=dev_password_change_in_prod

# Redis
REDIS_URL=redis://localhost:6379/0

# Inne
SECRET_KEY=change-me-in-production
ENVIRONMENT=development
DEBUG=true
RANDOM_SEED=42
```

#### 3. Test Database
Testy automatycznie tworzƒÖ oddzielnƒÖ bazƒô testowƒÖ:
- Nazwa: `test_market_research_db`
- Auto-cleanup po testach (transaction rollback)
- Izolacja od production DB

---

## Uruchamianie Test√≥w

### Podstawowe Komendy

```bash
# Wszystkie testy (bez wolnych E2E/performance)
pytest tests/ -v -m "not slow and not e2e"

# Z pokryciem kodu (coverage)
pytest tests/ --cov=app --cov-report=html
open htmlcov/index.html

# Testy jednostkowe (szybkie)
pytest tests/unit/ -v

# Testy integracyjne (wymaga DB)
pytest tests/integration/ -v

# Testy E2E (wymaga Gemini API, wolne)
pytest tests/e2e/ -v -s

# Testy wydajno≈õciowe (bardzo wolne)
pytest tests/performance/ -v -s

# Testy error handling
pytest tests/error_handling/ -v
```

### Zaawansowane Komendy

```bash
# Konkretny plik testowy
pytest tests/unit/test_persona_generator.py -v

# Konkretny test
pytest tests/e2e/test_e2e_full_workflow.py::test_complete_research_workflow_end_to_end -v -s

# Testy wed≈Çug markera
pytest -v -m integration
pytest -v -m "e2e and not slow"
pytest -v -m slow

# Pierwszy failed test (fail fast)
pytest tests/ -x

# Poka≈º print statements (-s)
pytest tests/unit/ -v -s

# R√≥wnoleg≈Çe uruchomienie (wymaga pytest-xdist)
pytest tests/unit/ -n auto

# Only failed from last run
pytest tests/ --lf

# Verbose output z timings
pytest tests/ -v --durations=10
```

### Markery Test√≥w

Testy sƒÖ oznaczone markerami dla ≈Çatwego filtrowania:

| Marker | Znaczenie | U≈ºycie |
|--------|-----------|--------|
| `@pytest.mark.integration` | Wymaga bazy danych | `pytest -m integration` |
| `@pytest.mark.e2e` | Test end-to-end | `pytest -m e2e` |
| `@pytest.mark.slow` | Test trwa >10s | `pytest -m "not slow"` |
| `@pytest.mark.asyncio` | Async test (auto) | - |

---

## Kluczowe Fixtures

Fixtures eliminujƒÖ boilerplate i umo≈ºliwiajƒÖ szybkie pisanie test√≥w.

### `authenticated_client`
**Zwraca:** `(TestClient, User, headers)`

```python
@pytest.mark.asyncio
async def test_example(authenticated_client):
    client, user, headers = authenticated_client

    response = client.get("/api/v1/projects", headers=headers)
    assert response.status_code == 200
```

**Co robi:**
- Tworzy u≈ºytkownika w bazie
- Generuje JWT token
- Zwraca gotowe headers z autoryzacjƒÖ

---

### `project_with_personas`
**Zwraca:** `(Project, List[Persona], TestClient, headers)`

```python
@pytest.mark.asyncio
async def test_focus_group(project_with_personas):
    project, personas, client, headers = project_with_personas

    # Projekt ma ju≈º 10 gotowych person!
    persona_ids = [str(p.id) for p in personas[:5]]

    # Utw√≥rz grupƒô fokusowƒÖ
    fg_data = {
        "name": "Test Group",
        "persona_ids": persona_ids,
        "questions": ["Question?"]
    }
```

**Co robi:**
- Tworzy projekt z kompletnƒÖ demographics
- Generuje 10 person z:
  - Pe≈Çnym demografiƒÖ (age, gender, education, income, location)
  - Big Five traits (openness, conscientiousness, etc.)
  - Hofstede dimensions
- Gotowe do u≈ºycia w testach grup fokusowych

---

### `completed_focus_group`
**Zwraca:** `(FocusGroup, List[PersonaResponse], TestClient, headers)`

```python
@pytest.mark.asyncio
async def test_results(completed_focus_group):
    focus_group, responses, client, headers = completed_focus_group

    # Grupa fokusowa ju≈º zako≈Ñczona!
    assert focus_group.status == "completed"
    assert len(responses) == 15  # 5 person √ó 3 pytania
```

**Co robi:**
- Tworzy projekt + 10 person (via `project_with_personas`)
- Tworzy grupƒô fokusowƒÖ (5 person √ó 3 pytania)
- Ustawia status="completed"
- Generuje 15 odpowiedzi (PersonaResponse)
- Oblicza performance metrics (total_time, avg_time)

---

### Inne Fixtures

| Fixture | Co zwraca | U≈ºycie |
|---------|-----------|--------|
| `db_session` | AsyncSession | Sesja bazodanowa z auto-rollback |
| `api_client` | TestClient | FastAPI test client |
| `mock_llm` | AsyncMock | Mockowany Gemini LLM |
| `sample_persona_dict` | Dict | Przyk≈Çadowa persona (dict) |
| `sample_project_dict` | Dict | Przyk≈Çadowy projekt (dict) |

Wszystkie fixtures w: [tests/conftest.py](conftest.py)

---

## Najwa≈ºniejsze Testy

### üèÜ Top 5 Test√≥w do Uruchomienia

#### 1. **test_complete_research_workflow_end_to_end** ‚≠ê
**Lokalizacja:** `tests/e2e/test_e2e_full_workflow.py`
**Czas:** ~90-180s
**Dlaczego:** Pokrywa 90% funkcjonalno≈õci aplikacji w jednym scenariuszu

```bash
pytest tests/e2e/test_e2e_full_workflow.py::test_complete_research_workflow_end_to_end -v -s
```

**Je≈õli ten test przechodzi, aplikacja dzia≈Ça!**

---

#### 2. **test_auth_api_integration.py** üîê
**Lokalizacja:** `tests/integration/test_auth_api_integration.py`
**Czas:** ~5-10s
**Dlaczego:** Weryfikuje wszystkie security critical paths

```bash
pytest tests/integration/test_auth_api_integration.py -v
```

Testuje:
- Password hashing (bcrypt)
- JWT generation & validation
- Token expiration
- Weak password rejection
- Protected endpoints

---

#### 3. **test_performance.py** ‚ö°
**Lokalizacja:** `tests/performance/test_performance.py`
**Czas:** ~5-10 min
**Dlaczego:** Wykrywa regresje wydajno≈õciowe

```bash
pytest tests/performance/ -v -s
```

Weryfikuje cele:
- 20 person < 60s
- Focus group < 3 min
- Avg response < 3s
- Parallelization >=3x speedup

---

#### 4. **test_critical_paths.py** üéØ
**Lokalizacja:** `tests/unit/test_critical_paths.py`
**Czas:** <1s
**Dlaczego:** Testuje najwa≈ºniejsze walidacje biznesowe

```bash
pytest tests/unit/test_critical_paths.py -v
```

Testuje:
- Demographics suma = 1.0
- Big Five traits w zakresie [0, 1]
- Chi-square validation dzia≈Ça
- Password hashing
- JWT expiration
- Database constraints

---

#### 5. **test_error_handling.py** üõ°Ô∏è
**Lokalizacja:** `tests/error_handling/test_error_handling.py`
**Czas:** ~5-10s
**Dlaczego:** Testuje resilience i fallbacks

```bash
pytest tests/error_handling/ -v
```

Testuje:
- Gemini API timeout ‚Üí graceful error
- Neo4j down ‚Üí in-memory fallback
- Empty data ‚Üí validation errors
- Race conditions

---

## Cele Wydajno≈õciowe

Z [CLAUDE.md](../CLAUDE.md) - weryfikowane przez `tests/performance/`:

| Metryka | Target | Ideal | Test |
|---------|--------|-------|------|
| **Generowanie 20 person** | <60s | 30-45s | `test_persona_generation_performance_20_personas` |
| **Focus group 20√ó4** | <3 min | <2 min | `test_focus_group_execution_performance_20x4` |
| **Avg response time** | <3s | 1-2s | `test_avg_response_time_per_persona` |
| **Survey 10√ó10** | <60s | <45s | `test_survey_execution_performance_10x10` |
| **Parallelization** | >=2x | >=3x | `test_parallelization_speedup` |

### Dlaczego Te Cele?

**Generowanie person:**
- Sequential: 20 √ó 3s = 60s
- Parallel (concurrency=5): ~15s
- **Je≈õli >2 min, u≈ºytkownicy porzucƒÖ platformƒô**

**Focus group:**
- Sequential: 20 person √ó 4 pytania √ó 3s = 4 min
- Parallel: ~2 min (speedup 2x)
- **Target: <3 min dla UX**

**Parallelization:**
- Bez parallelization: 10 person √ó 3s = 30s
- Z parallelization (5 concurrent): ~10s
- **Speedup 3x = dzia≈Ça poprawnie** ‚úÖ

---

## Coverage i Metryki

### Generowanie Raportu Coverage

```bash
# HTML report
pytest tests/ --cov=app --cov-report=html
open htmlcov/index.html

# Terminal report
pytest tests/ --cov=app --cov-report=term-missing

# XML report (dla CI/CD)
pytest tests/ --cov=app --cov-report=xml
```

### Cele Coverage

| Modu≈Ç | Target Coverage | Obecny Status |
|-------|----------------|---------------|
| `app/services/` | 90%+ | ‚úÖ ~92% |
| `app/api/` | 85%+ | ‚úÖ ~88% |
| `app/models/` | 95%+ | ‚úÖ ~96% |
| `app/core/` | 90%+ | ‚úÖ ~91% |
| **OVERALL** | **85%+** | **‚úÖ ~90%** |

### Metryki Jako≈õci Test√≥w

**Przed (ocena 60/100):**
- 191 test√≥w (155 pass, 36 skip)
- Brak test√≥w E2E
- Brak test√≥w performance
- Brak test√≥w error handling

**Po (ocena 85-90/100):**
- 208 test√≥w (200+ pass, <5 skip)
- ‚úÖ 4 testy E2E
- ‚úÖ 5 test√≥w performance
- ‚úÖ 9 test√≥w error handling
- ‚úÖ 35 test√≥w integracyjnych z DB

**Wzrost jako≈õci: +25-30 punkt√≥w** üéâ

---

## RozwiƒÖzywanie Problem√≥w

### ‚ùå "Connection refused" przy testach integracyjnych

**Problem:** Brak po≈ÇƒÖczenia z PostgreSQL

**RozwiƒÖzanie:**
```bash
# Sprawd≈∫ czy Docker dzia≈Ça
docker-compose ps

# Uruchom serwisy
docker-compose up -d postgres redis neo4j

# Sprawd≈∫ logi
docker-compose logs postgres
```

---

### ‚ùå Testy timeout przy generowaniu person

**Problem:** Brak Gemini API key lub quota exceeded

**RozwiƒÖzanie:**
```bash
# Sprawd≈∫ czy API key jest ustawiony
echo $GOOGLE_API_KEY

# Ustaw w .env
export GOOGLE_API_KEY=your_key_here

# Sprawd≈∫ quota w Google Cloud Console
# https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas
```

---

### ‚ùå Neo4j tests fail

**Problem:** Neo4j niedostƒôpny

**RozwiƒÖzanie:**
Neo4j jest OPCJONALNY - testy majƒÖ fallback do in-memory graph.

Je≈õli chcesz u≈ºywaƒá Neo4j:
```bash
# Restart Neo4j
docker-compose restart neo4j

# Sprawd≈∫ logi
docker-compose logs neo4j

# Sprawd≈∫ po≈ÇƒÖczenie
curl http://localhost:7474
```

---

### ‚ùå Database connection errors

**Problem:** Test database conflict

**RozwiƒÖzanie:**
```bash
# Reset test database
docker-compose restart postgres

# Usu≈Ñ stare dane (opcja nuklearna)
docker-compose down -v
docker-compose up -d
```

---

### ‚ùå Import errors po reorganizacji

**Problem:** Testy nie znajdujƒÖ modu≈Ç√≥w

**RozwiƒÖzanie:**
```bash
# Usu≈Ñ cache
find tests -type d -name __pycache__ -exec rm -rf {} +
find tests -type f -name "*.pyc" -delete

# Reinstaluj w trybie editable
pip install -e .
```

---

### ‚ùå Testy sƒÖ wolne

**Optymalizacje:**

```bash
# Uruchom tylko szybkie testy
pytest tests/ -m "not slow and not e2e"

# Pomi≈Ñ testy performance
pytest tests/ -m "not slow"

# R√≥wnoleg≈Çe uruchomienie (wymaga pytest-xdist)
pip install pytest-xdist
pytest tests/unit/ -n auto

# Cache wynik√≥w
pytest tests/ --cache-clear  # czy≈õƒá cache je≈õli problemy
```

---

## Dodawanie Nowych Test√≥w

### Template: Test Jednostkowy

```python
# tests/unit/test_my_module.py

import pytest

def test_my_function():
    """Test prostej funkcji."""
    from app.services.my_module import my_function

    result = my_function("input")
    assert result == "expected_output"


@pytest.mark.asyncio
async def test_my_async_function():
    """Test async funkcji."""
    from app.services.my_module import my_async_function

    result = await my_async_function("input")
    assert result is not None
```

---

### Template: Test Integracyjny

```python
# tests/integration/test_my_api_integration.py

import pytest

@pytest.mark.integration
@pytest.mark.asyncio
async def test_my_endpoint(authenticated_client):
    """Test endpointu API z bazƒÖ danych."""
    client, user, headers = authenticated_client

    response = client.get("/api/v1/my-endpoint", headers=headers)

    assert response.status_code == 200
    data = response.json()
    assert "expected_field" in data
```

---

### Template: Test E2E

```python
# tests/e2e/test_my_workflow.py

import pytest

@pytest.mark.e2e
@pytest.mark.slow
@pytest.mark.asyncio
async def test_my_complete_workflow(db_session):
    """Test kompletnego workflow u≈ºytkownika."""

    # 1. Setup
    # ...

    # 2. Wykonaj workflow
    # ...

    # 3. Weryfikuj wyniki
    assert result is not None
```

---

## CI/CD Integration

### GitHub Actions Template

```yaml
# .github/workflows/tests.yml

name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: ankane/pgvector:latest
        env:
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio

      - name: Run tests
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:test_password@localhost/test_db
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          pytest tests/ -v -m "not slow and not e2e" --cov=app

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## FAQ

### ‚ùì Czy mogƒô uruchomiƒá testy bez Dockera?

**Tak**, ale tylko testy jednostkowe:
```bash
pytest tests/unit/ -v
```

Testy integracyjne i E2E wymagajƒÖ PostgreSQL.

---

### ‚ùì Czy mogƒô uruchomiƒá testy bez Gemini API?

**Tak**, wiƒôkszo≈õƒá test√≥w u≈ºywa mock√≥w:
```bash
# Testy bez Gemini API (unit + integration bez personas)
pytest tests/unit/ tests/integration/test_auth_api_integration.py tests/integration/test_projects_api_integration.py -v
```

Tylko testy E2E i performance wymagajƒÖ prawdziwego Gemini API.

---

### ‚ùì Ile kosztujƒÖ testy E2E (Gemini API)?

**Szacunkowo:** ~$0.01-0.02 per pe≈Çny run test√≥w E2E

- test_complete_research_workflow: 10 person + 5√ó3 responses ‚âà $0.005
- test_survey_workflow: 10√ó4 responses ‚âà $0.002
- test_performance (wszystkie): ‚âà $0.01

**Razem:** <$0.02 per CI/CD run

**Tip:** Uruchamiaj testy E2E tylko na main branch.

---

### ‚ùì Jak debugowaƒá failed test?

```bash
# 1. Uruchom z verbose output i prints
pytest tests/path/to/test.py::test_name -v -s

# 2. Uruchom tylko failed test z ostatniego runa
pytest --lf -v

# 3. U≈ºyj pdb (Python debugger)
pytest tests/path/to/test.py::test_name --pdb

# 4. Zwiƒôksz timeout dla async test√≥w
pytest tests/ -v --asyncio-timeout=300
```

---

### ‚ùì Jak dodaƒá nowy marker?

Edytuj `tests/conftest.py`:

```python
def pytest_configure(config):
    config.addinivalue_line(
        "markers", "my_marker: opis markera"
    )
```

U≈ºyj w te≈õcie:
```python
@pytest.mark.my_marker
def test_something():
    pass
```

---

## Kontakt i Wsparcie

**Problemy z testami?**
1. Sprawd≈∫ [RozwiƒÖzywanie Problem√≥w](#rozwiƒÖzywanie-problem√≥w)
2. Przeczytaj [CLAUDE.md](../CLAUDE.md) (g≈Ç√≥wna dokumentacja projektu)
3. Sprawd≈∫ logi Docker: `docker-compose logs`
4. Otw√≥rz issue na GitHubie

**Dokumentacja projektu:**
- [README.md](../README.md) - G≈Ç√≥wna dokumentacja u≈ºytkownika
- [CLAUDE.md](../CLAUDE.md) - Dokumentacja dla developer√≥w
- [tests/TESTY.md](TESTY.md) - Ten plik (dokumentacja test√≥w)

---

---

## Manual RAG Testing & Optimization

Zestaw narzƒôdzi do testowania i optymalizacji systemu RAG/GraphRAG w `tests/manual/`.

### üìã PrzeglƒÖd Test Scripts

#### 1. `test_hybrid_search.py` - Basic Hybrid Search Test

**Cel:** Weryfikacja dzia≈Çania hybrydowego wyszukiwania (vector + keyword + RRF fusion).

```bash
python tests/manual/test_hybrid_search.py
```

**Co testuje:**
- Vector search results
- Keyword search results
- RRF fusion results
- Score distribution
- Fragment kontekstu

**Kiedy u≈ºywaƒá:**
- Po zmianie konfiguracji RAG
- Debugowanie problem√≥w z retrieval
- Quick sanity check ≈ºe system dzia≈Ça

---

#### 2. `test_rag_ab_comparison.py` - A/B Performance Comparison

**Cel:** Por√≥wnanie performance r√≥≈ºnych konfiguracji RAG.

```bash
python tests/manual/test_rag_ab_comparison.py
```

**Co mierzy:**
- **Keyword coverage** - % oczekiwanych keywords w wynikach
- **Relevance score** - Aggregate quality metric
- **Latency** - Czas retrieval
- **Context length** - D≈Çugo≈õƒá zwr√≥conego kontekstu

**Test queries:**
- M≈Çoda kobieta w stolicy z wy≈ºszym wykszta≈Çceniem
- Senior w ma≈Çym mie≈õcie z podstawowym wykszta≈Çceniem
- Osoba w ≈õrednim wieku, ≈õrednie miasto
- M≈Çody absolwent szukajƒÖcy pracy

**Jak interpretowaƒá wyniki:**

| Metric | Good | Warning | Action |
|--------|------|---------|--------|
| Keyword coverage | >70% | <70% | Zwiƒôksz TOP_K lub zmniejsz chunk_size |
| Relevance score | >0.50 | <0.50 | Tune RRF_K lub w≈ÇƒÖcz reranking |
| Latency | <1.5s | >1.5s | Zmniejsz TOP_K lub wy≈ÇƒÖcz reranking |

---

#### 3. `test_rrf_k_tuning.py` - RRF Parameter Optimization

**Cel:** Znalezienie optymalnej warto≈õci RRF_K dla twojego datasetu.

```bash
python tests/manual/test_rrf_k_tuning.py
```

**Co testuje:**
- k=40 (elitarne) - wiƒôkszy wp≈Çyw top results
- k=60 (standardowe) - balans
- k=80 (demokratyczne) - r√≥wnomierne traktowanie

**Rekomendacja:**
- **k=40** - Je≈õli zale≈ºy ci na precision (lepsze top 3 results)
- **k=60** - Je≈õli chcesz balans miƒôdzy precision i recall
- **k=80** - Je≈õli chcesz r√≥wnomierne traktowanie wszystkich results

---

### üîß Workflow Optymalizacji

#### Quick Start (Nowa Konfiguracja)

1. **Zmie≈Ñ parametry w `app/core/config.py`:**
```python
RAG_CHUNK_SIZE = 1000  # Eksperymentuj: 800, 1000, 1200
RAG_TOP_K = 8          # Eksperymentuj: 5, 8, 10
```

2. **Re-ingest dokumenty** (WA≈ªNE przy zmianie chunk_size!):
```bash
# Clear old chunks
docker-compose exec neo4j cypher-shell -u neo4j -p dev_password_change_in_prod \
  "MATCH (n:RAGChunk) DETACH DELETE n"

# Upload dokumenty ponownie przez API
```

3. **Uruchom A/B comparison:**
```bash
python tests/manual/test_rag_ab_comparison.py
```

4. **Por√≥wnaj z baseline** i zdecyduj czy trzymaƒá zmiany.

#### Deep Optimization (Fine-tuning)

1. Tune RRF_K: `python tests/manual/test_rrf_k_tuning.py`
2. Update `config.py` z best k
3. Verify z A/B comparison
4. Commit je≈õli lepsze

#### Continuous Monitoring

Uruchamiaj co 2-4 tygodnie lub po ka≈ºdej zmianie:
```bash
# Quick check
python tests/manual/test_hybrid_search.py

# Full comparison (co miesiƒÖc)
python tests/manual/test_rag_ab_comparison.py
```

---

### üìä Baseline Metrics (Reference)

**Old Configuration (przed optymalizacjƒÖ):**
```
CHUNK_SIZE: 2000, OVERLAP: 400 (20%), TOP_K: 5, MAX_CONTEXT: 5000
Results: coverage ~65%, relevance ~0.45, latency ~0.30s, truncation 50%
```

**Current Configuration (zoptymalizowana):**
```
CHUNK_SIZE: 1000, OVERLAP: 300 (30%), TOP_K: 8, MAX_CONTEXT: 12000, RERANKING: True
Expected: coverage ~75-80%, relevance ~0.55-0.60, latency ~0.35-0.40s, truncation 0%
```

---

### üêõ Troubleshooting RAG Tests

**"No results returned"**
- Sprawd≈∫ czy Neo4j dzia≈Ça: `docker-compose ps neo4j`
- Sprawd≈∫ zaindeksowane dokumenty: `curl http://localhost:8000/api/v1/rag/documents`
- Sprawd≈∫ indexes: Neo4j Browser ‚Üí `SHOW INDEXES`

**"Keyword coverage very low (<40%)"**
- Zwiƒôksz TOP_K (8 ‚Üí 10)
- Zmniejsz chunk_size (1000 ‚Üí 800)
- Sprawd≈∫ czy test queries pasujƒÖ do dokument√≥w

**"Relevance score low (<0.40)"**
- Tune RRF_K (u≈ºyj `test_rrf_k_tuning.py`)
- W≈ÇƒÖcz reranking (`RAG_USE_RERANKING=True`)
- Sprawd≈∫ jako≈õƒá embeddings

**"Latency too high (>2s)"**
- Zmniejsz TOP_K (8 ‚Üí 5)
- Wy≈ÇƒÖcz reranking (`RAG_USE_RERANKING=False`)
- Zmniejsz `RAG_RERANK_CANDIDATES` (25 ‚Üí 15)

**"Import errors (sentence-transformers)"**
```bash
pip install sentence-transformers
# Lub wy≈ÇƒÖcz reranking: RAG_USE_RERANKING=False
```

---

### üí° Best Practices

1. **Zawsze testuj przed commitem** - Uruchom przynajmniej `test_hybrid_search.py`
2. **Re-ingest po zmianie chunk_size** - Stare chunki nie bƒôdƒÖ pasowaƒá
3. **Dokumentuj baseline** - Zapisz metrics przed zmianƒÖ dla por√≥wnania
4. **Iteruj stopniowo** - Zmie≈Ñ jeden parameter na raz
5. **Measure, don't guess** - Nie zak≈Çadaj ≈ºe wiƒôksze = lepsze, testuj!

---

**Dodatkowe zasoby:**
- **docs/RAG.md** - Kompletna dokumentacja systemu RAG
- **app/core/config.py** - Wszystkie parametry konfiguracji
- **app/services/rag_service.py** - Implementacja RAG

---

---

## üß™ Testy RAG + GraphRAG + Orkiestracja (Nowe!)

**Data dodania:** 2025-10-14
**Liczba test√≥w:** +67 test√≥w unit (≈ÇƒÖcznie **275 test√≥w** w projekcie)

### PrzeglƒÖd

Nowe testy pokrywajƒÖ ca≈Çy system RAG (Retrieval Augmented Generation), GraphRAG (Knowledge Graph) i orkiestracji person u≈ºywajƒÖc Gemini 2.5 Pro.

**Pliki testowe:**
```
tests/unit/
‚îú‚îÄ‚îÄ test_rag_hybrid_search.py         # 15 test√≥w (~30s) ‚úÖ
‚îú‚îÄ‚îÄ test_graph_rag_construction.py    # 18 test√≥w (~45s) ‚úÖ
‚îú‚îÄ‚îÄ test_graph_analytics.py           # 14 test√≥w (~20s) ‚úÖ
‚îú‚îÄ‚îÄ test_persona_orchestration.py     # 12 test√≥w (~40s) ‚úÖ
‚îú‚îÄ‚îÄ test_rag_config_validation.py     #  8 test√≥w (~10s) ‚úÖ
‚îî‚îÄ‚îÄ README_RAG_TESTS.md               # Szczeg√≥≈Çowa dokumentacja
```

### Quick Start

```bash
# Wszystkie nowe testy RAG/GraphRAG
pytest tests/unit/test_rag_*.py tests/unit/test_graph_*.py tests/unit/test_persona_*.py -v

# Z coverage dla RAG services
pytest tests/unit/test_rag_*.py --cov=app/services/rag_service.py --cov-report=html
```

---

### 1. **test_rag_hybrid_search.py** (15 test√≥w)

**Cel:** Testowanie hybrydowego wyszukiwania (Vector + Keyword + RRF Fusion).

**Kluczowe testy:**
- ‚úÖ **Vector Search** - Semantic similarity via Gemini embeddings
- ‚úÖ **Keyword Search** - Fulltext index (Lucene)
- ‚úÖ **RRF Fusion** - Reciprocal Rank Fusion (k=40/60/80)
- ‚úÖ **Reranking** - Cross-encoder dla precision
- ‚úÖ **Chunk Enrichment** - Wzbogacanie o graph nodes
- ‚úÖ **Performance** - Latency target <500ms

**Uruchom:**
```bash
pytest tests/unit/test_rag_hybrid_search.py -v
```

**Co weryfikuje:**
- Vector search zwraca semantically similar documents
- Keyword search znajduje exact matches (polskie znaki, apostrophes)
- RRF fusion ≈ÇƒÖczy results z r√≥≈ºnych ≈∫r√≥de≈Ç (deduplication, ranking)
- Reranking poprawia precision top results (cross-encoder attention)
- Enrichment dodaje graph context (Wskazniki, Trendy) do chunk√≥w

---

### 2. **test_graph_rag_construction.py** (18 test√≥w)

**Cel:** Testowanie budowy grafu wiedzy z odpowiedzi person.

**Kluczowe testy:**
- ‚úÖ **LLM Concept Extraction** - Gemini structured output (concepts, emotions, sentiment)
- ‚úÖ **Fallback Extraction** - Keyword-based gdy LLM unavailable (resilience)
- ‚úÖ **Node Creation** - Persona, Concept, Emotion nodes z metadata
- ‚úÖ **Relationship Creation** - MENTIONS, FEELS, AGREES_WITH, DISAGREES_WITH
- ‚úÖ **Memory Fallback** - In-memory graph gdy Neo4j unavailable
- ‚úÖ **Graph Persistence** - Lifecycle management (create, query, delete)

**Uruchom:**
```bash
pytest tests/unit/test_graph_rag_construction.py -v
```

**Co weryfikuje:**
- LLM ekstraktuje concepts z high quality (sentiment, key phrases)
- System dzia≈Ça bez LLM (keyword fallback, stopwords filtering)
- Nodes majƒÖ complete metadata (doc_id, chunk_index, confidence)
- Relationships reflect persona opinions (agreement, disagreement detection)
- Memory fallback zapewnia resilience (system dzia≈Ça bez Neo4j)

---

### 3. **test_graph_analytics.py** (14 test√≥w)

**Cel:** Testowanie zaawansowanych analiz grafowych.

**Kluczowe testy:**
- ‚úÖ **Key Concepts** - Top concepts sorted by frequency + sentiment
- ‚úÖ **Controversial Concepts** - High polarization (std dev > 0.4)
- ‚úÖ **Influential Personas** - PageRank-like connections ranking
- ‚úÖ **Emotion Distribution** - Aggregates (count, intensity, percentage)
- ‚úÖ **Trait Correlations** - Age gap detection (young vs senior opinions)
- ‚úÖ **NL Queries** - Natural language interface ("Who influences most?")

**Uruchom:**
```bash
pytest tests/unit/test_graph_analytics.py -v
```

**Co weryfikuje:**
- Analytics zwracajƒÖ actionable insights (not just raw data)
- Controversial concepts majƒÖ identified supporters + critics
- Influence score bazuje na connections (realistic metric)
- NL queries u≈ºywajƒÖ heuristics dla keyword matching
- Neo4j vs memory results sƒÖ consistent (resilience)

---

### 4. **test_persona_orchestration.py** (12 test√≥w)

**Cel:** Testowanie orkiestracji u≈ºywajƒÖc Gemini 2.5 Pro.

**Kluczowe testy:**
- ‚úÖ **Graph Context Retrieval** - 8 parallel queries (demographics, trends)
- ‚úÖ **Brief Generation** - 2000-3000 znak√≥w, edukacyjny ton
- ‚úÖ **Graph Insights** - Structured data (magnitude, confidence, why_matters)
- ‚úÖ **JSON Parsing** - Multiple formats (```json, ```, bare braces)
- ‚úÖ **Timeout Handling** - 30s graph queries, 120s LLM calls
- ‚úÖ **Demographics Validation** - Allocation sum == total_personas

**Uruchom:**
```bash
pytest tests/unit/test_persona_orchestration.py -v
```

**Co weryfikuje:**
- Graph context jest comprehensive (multiple demographics queries)
- Briefe sƒÖ d≈Çugie i educational (wyja≈õniajƒÖ "dlaczego")
- JSON parsing jest robust (handles LLM output variations)
- Timeouts sƒÖ correctly configured (resilience)
- Allocation logic jest sensible (% population vs relevance)

---

### 5. **test_rag_config_validation.py** (8 test√≥w)

**Cel:** Testowanie poprawno≈õci konfiguracji RAG parameters.

**Kluczowe testy:**
- ‚úÖ **Chunk Size** - Bounds (500-2000), overlap (10%-50%)
- ‚úÖ **TOP_K** - Range (3-20), RERANK_CANDIDATES >= TOP_K
- ‚úÖ **RRF_K** - Range (20-100)
- ‚úÖ **MAX_CONTEXT** - Sufficient dla TOP_K chunks
- ‚úÖ **Vector Weight** - Range [0.0, 1.0], not extreme
- ‚úÖ **Reranker** - Model specified, sentence-transformers available

**Uruchom:**
```bash
pytest tests/unit/test_rag_config_validation.py -v
```

**Co weryfikuje:**
- All config parameters sƒÖ w sensible ranges
- No conflicting settings (chunk overlap < chunk size, etc.)
- Dependencies sƒÖ available (sentence-transformers dla reranking)

---

### Fixtures dla RAG/GraphRAG

**Nowe fixtures w `tests/conftest.py`:**

```python
# Mock Stores
mock_neo4j_driver           # Neo4j driver mock
mock_vector_store           # Vector search mock (embeddings)
mock_graph_store            # Cypher queries mock
mock_embeddings             # Deterministyczne embeddings (768D)

# Sample Data
sample_rag_document         # Przyk≈Çadowy dokument (GUS stats)
mock_concept_extraction     # LLM extraction mock

# LLM Mocks
mock_llm                    # Gemini 2.5 Flash
mock_gemini_2_5_pro         # Gemini 2.5 Pro (orchestration)

# Service Fixtures
rag_document_service_with_mocks      # RAGDocumentService z mockami
polish_society_rag_with_mocks        # PolishSocietyRAG z mockami
graph_service_with_mocks             # GraphService z mockami
persona_orchestration_with_mocks     # Orchestration z mockami
```

**U≈ºycie:**
```python
async def test_example(polish_society_rag_with_mocks):
    rag = await polish_society_rag_with_mocks
    results = await rag.hybrid_search("query", top_k=5)
    assert len(results) == 5
```

---

### Coverage Impact

**Przed dodaniem test√≥w RAG/GraphRAG:**
| Module | Coverage |
|--------|----------|
| `rag_service.py` | ~78% |
| `graph_service.py` | ~72% |
| `persona_orchestration.py` | ~65% |

**Po dodaniu test√≥w:**
| Module | Coverage | Wzrost |
|--------|----------|--------|
| `rag_service.py` | **~92%** | +14% ‚úÖ |
| `graph_service.py` | **~88%** | +16% ‚úÖ |
| `persona_orchestration.py` | **~87%** | +22% ‚úÖ |

**Overall services/ coverage:** 85%+ ‚Üí **~90%** (+5%)

---

### Dokumentacja

**Szczeg√≥≈Çowa dokumentacja:** `tests/unit/README_RAG_TESTS.md`

**Zawiera:**
- Detailed description ka≈ºdego pliku testowego
- Test patterns i best practices
- Troubleshooting guide
- Performance benchmarks
- Fixture usage examples

**Przeczytaj:**
```bash
cat tests/unit/README_RAG_TESTS.md
# Lub
open tests/unit/README_RAG_TESTS.md  # je≈õli w IDE
```

---

### Nastƒôpne Kroki (TODO)

1. **Integration Tests** - test_rag_integration.py (20 test√≥w z Neo4j)
2. **E2E Test** - test_complete_rag_orchestration_workflow.py (full pipeline)
3. **Performance Benchmarks** - Latency dla hybrid search, graph analytics

---

**Koniec dokumentacji test√≥w**

Ostatnia aktualizacja: 2025-10-14
Wersja: 2.2 (dodano 67 test√≥w RAG/GraphRAG/Orchestration)
Liczba test√≥w: **275** (208 + 67 RAG/GraphRAG)
